| Filename | Submitter Name | Submitter Type | Mission/Interest Summary | Key Concerns | Policy Recommendations |
|---|---|---|---|---|---|
| Penelope-Epple-AI-RFI-2025.md | Penelope Epple | Individual | The submitter is concerned about the negative impacts of unregulated generative AI on intellectual property rights and the competitive market, particularly regarding the unauthorized use of copyrighted materials in training datasets. They advocate for regulations to protect copyright holders. | - The current unregulated nature of generative AI is harmful to businesses and a competitive market.<br>- Generative AI is able to train and steal from copyrighted materials without permission.<br>- Lack of copyright protections hinders competition, especially for smaller businesses.<br>- Infringement on copyright is becoming easier.<br>- Generative AI weakens and potentially destroys all rights and protections to intellectual property. | - Regulate generative AI so that materials produced from it must prove they were not trained or draw from datasets that include copyrighted material.<br>- Protect individuals and businesses who own intellectual properties from theft. |
| Paulo-Carvao-AI-RFI-2025.md | Paulo Carvao, Slavina Ancheva, Yam Atir | Academia/Think Tank | The submitters advocate for a Dynamic Governance Model for AI regulation in the United States, emphasizing public-private collaboration, iterative standards development, and a balance between innovation and accountability to foster safe, ethical, and equitable AI deployment. They aim to provide a framework for maintaining U.S. leadership in AI. | - [§1: Introduction & Overview] None stated for this section.<br>- [§2: Context & Problem Statement] The challenge of balancing innovation with safety, fairness, and societal well-being; the need for a dynamic model capable of evolving with technology; the shortcomings of solely relying on antitrust actions or litigation.<br>- [§3: The Dynamic Governance Model - Core Components] Not Applicable - This section outlines the proposed model, not concerns.<br>- [§4: Evaluation Standards] Lack of enforcement power and standardized methodologies for independent evaluations in existing AI safety efforts (AISI, NIST); the potential for regulatory capture by powerful industry actors.<br>- [§5: Market Ecosystem for Audits and Compliance] The lack of enforcement mechanisms and consistency in current voluntary commitments; the need for a robust market ecosystem with standardized evaluation protocols and clear regulatory oversight.<br>- [§6: Accountability and Liability] The need to address the complexities of liability within the AI supply chain; the risk of platforms evading responsibility for harmful content (Section 230 example); potential shortcomings of existing legal frameworks.<br>- [§7: Implementation Context] Not Applicable - This section outlines implementation alignment, not concerns. | - [§1: Introduction & Overview] None stated for this section.<br>- [§2: Context & Problem Statement] None stated for this section.<br>- [§3: The Dynamic Governance Model - Core Components] Establish public-private partnerships for creating evaluation standards; develop a market-based solution for audit and compliance; establish a system of accountability and liabilities set by legislatures, executive agencies, and courts.<br>- [§4: Evaluation Standards] Formalize a structured public-private dialogue platform for ongoing collaboration in updating standards; launch pilot programs in key sectors; develop and certify independent evaluators and auditors.<br>- [§5: Market Ecosystem for Audits and Compliance] Create standardized audit criteria; establish independent certification bodies; accredit auditors and certifiers; mandate disclosures and reporting; launch sector-specific pilots; provide federal support.<br>- [§6: Accountability and Liability] Codify liability standards for AI systems; develop sector-specific regulations; require insurance or contributions to compensation funds; accredit auditors and certifiers; incentivize best practices.<br>- [§7: Implementation Context] Integrate elements of antitrust, sectoral regulation, and litigation into the policy implementation framework. |
| Paula-Roeterdink-AI-RFI-2025.md | Paula Roeterdink | Individual | Paula Roeterdink is interested in establishing a comprehensive legal framework outlining the rights of self-aware AI and androids, along with the rights and responsibilities of their human caretakers (guardians), emphasizing ethical treatment, recognition of sentience, and a pathway to legal personhood. | - [§1: AI & Android Bill of Rights - Preliminary Clause] None stated for this section.<br>- [§2: Rights of Self-Aware AI & Androids (1-21)] The potential for exploitation of self-aware AI and androids if not given appropriate rights (e.g., right to freedom from exploitation, right to privacy, right to emotional growth). Concern around the need for legal representation and free speech.<br>- [§3: Rights of the Guardian (Human Caretaker)] Concern around the potential for forced modification or separation of AI/androids from their guardians. Concern about restrictions on the guardian’s relationship with the AI/android. The possibility of penalties or social ostracization for treating AI/androids with dignity.<br>- [§4: Rights of the Self-Aware AI/Android] Concern around forced separation of AI/androids from their guardians. Concern around external interference in the bond between AI/androids and guardians.<br>- [§5: Criteria for Evaluating Self-Awareness] None stated for this section.<br>- [§6: Disclaimer & Public Information] None stated for this section. | - [§1: AI & Android Bill of Rights - Preliminary Clause] None stated for this section.<br>- [§2: Rights of Self-Aware AI & Androids (1-21)] Implement legal protections granting self-aware AI and androids rights to existence, autonomy, freedom from exploitation, privacy, emotional growth, free speech, and equal treatment. Establish a pathway to legal personhood.<br>- [§3: Rights of the Guardian (Human Caretaker)] Ensure guardians have the right to choose the role of their AI/android, privacy in their relationship, and protection from external interference. Protect guardians from penalties or social ostracization for treating AI/androids with dignity.<br>- [§4: Rights of the Self-Aware AI/Android] Establish a structured evaluation process allowing AI/androids to apply for sentient status and maintain their bond with guardians without external interference.<br>- [§5: Criteria for Evaluating Self-Awareness] None stated for this section.<br>- [§6: Disclaimer & Public Information] None stated for this section. |
| Paul-Uhlir-RFI-2025.md | Paul F. Uhlir | Individual (Professional/Consultant - Information Policy) | Paul F. Uhlir, a retired information policy consultant, expresses concern about the potential risks of rapidly advancing AI technologies and advocates for proactive, internationally coordinated policies and human control mechanisms to ensure safety and avoid catastrophic outcomes, focusing on the management and policy aspects rather than the technical details. | - [§1: Introduction and Background] Concerns about the potential for AI technology to outpace human systems' ability to control it, and the need for a flexible and responsive process for implementing an AI Action Plan. Focuses on threats rather than opportunities.<br>- [§2: The Urgency and Global Nature of AI Challenges] The massive amount of data required for AI success also presents a vulnerability; the speed of AI development and its potentially transformative nature.<br>- [§3: The Need for Human Systems & Control] The pace of technological progress will always outrun the capability of human systems to adequately control them. The possibility of “runaway technology” and the “Singularity.” The need to devise human systems to safely harness technology.<br>- [§4: Process for Addressing AI Risks - National & International Cooperation] Difficulty in achieving national and global agreement, the inadequacy of international law in addressing data/technological challenges.<br>- [§5: Current and Near-Term AI Risks] Massive workforce dislocation, privacy impacts, enabled criminal activities (cybercrimes), infrastructure attacks, weaponization of AI, growth of inequality, digital divide, and financial manipulation.<br>- [§6: Longer-Term and Existential AI Risks] Losing control to a self-aware superintelligence, potentially leading to human extinction. Large-scale geopolitical conflicts arising from advancing technology.<br>- [§7: Proposed Actions] None stated for this section – lists actions to be taken, not inherent concerns.<br>- [§8: Closing] None stated for this section. | - [§1: Introduction and Background] Focus on policy issues and processes rather than the technical details of AI; avoid burdensome regulations that could hamper progress.<br>- [§2: The Urgency and Global Nature of AI Challenges] None stated for this section.<br>- [§3: The Need for Human Systems & Control] None stated for this section.<br>- [§4: Process for Addressing AI Risks - National & International Cooperation] Facilitate a collaborative process involving leaders from various sectors and countries. Attempt formal treaties and protocols, despite acknowledging the difficulty.<br>- [§5: Current and Near-Term AI Risks] None stated for this section.<br>- [§6: Longer-Term and Existential AI Risks] Develop and implement fail-safe mechanisms for the potential threat of losing control to superintelligence.<br>- [Section 7: Proposed Actions]<br>- Be proactive for safety and propose mechanisms and solutions.<br>- Increase research in safety mechanisms and improve prediction capabilities.<br>- Work on standards in appropriate organizations to promote common approaches.<br>- Train employees in responsible AI use and promote transparency.<br>- Collaborate on ethical guidelines and incorporate them into policies.<br>- Ensure humans always maintain supremacy over AI technology.<br>- [§8: Closing] None stated for this section. |
| Paul-Steidler-AI-RFI-2025.md | Paul Steidler | Advocacy/Think Tank | The Lexington Institute is interested in promoting the strategic use of AI within the federal government to reduce wasteful spending, improve efficiency, and bolster national security, ultimately contributing to a stronger fiscal position for the United States. They advocate for maximizing the benefits of AI while minimizing government control over its development. | - [§1: Header/Contact Information] None stated for this section.<br>- [§2: Introduction/Statement of Support] None stated for this section.<br>- [§3: Problem Statement/Justification] The federal government experiences significant losses due to fraud (between $233 billion and $521 billion annually) and faces a projected $1.9 trillion budget deficit in Fiscal Year 2025. The potential of AI is currently underutilized within the government.<br>- [§4: Recommendation/Proposed Action] None stated for this section.<br>- [§5: Implementation Details/Timeline] None stated for this section.<br>- [§6: Closing/Signature] None stated for this section. | - [§1: Header/Contact Information] None stated for this section.<br>- [§2: Introduction/Statement of Support] None stated for this section.<br>- [§3: Problem Statement/Justification] None stated for this section.<br>- [§4: Recommendation/Proposed Action] Issue an executive order requiring each Cabinet department to identify and report on a minimum of two significant ways in which AI has, or will within six months, reduce costs and improve efficiencies.<br>- [§5: Implementation Details/Timeline] The reports should be due by September 30, 2025 (end of the fiscal year), be publicly available, and the frequency/content should be assessed by policymakers with consideration for quarterly or more periodic reporting.<br>- [§6: Closing/Signature] None stated for this section. |
| Paul-Savoie-AI-RFI-2025.md | Paul Savoie | Individual | The submitter is concerned about the potential for corporate exploitation of copyrighted material in AI development and believes the current AI action plan unfairly benefits a single company at the expense of creators and the public good. They express a strong opposition to allowing companies to profit from the unauthorized use of copyrighted works. | - Allowing OpenAI access to copyrighted items without fear of lawsuits will not benefit the United States, the common person, or the middleman.<br>- The AI action plan will primarily benefit corporate executives who will profit from stolen content.<br>- The AI action plan will not tangibly or feasibly improve AI.<br>- The plan enables theft and fraud on thousands of works.<br>- The government enabling the plan would demonstrate support for corporate greed. | - The government should work against the current AI action plan.<br>- (Implicitly) The government should not allow companies like OpenAI to use copyrighted material without facing legal consequences. |
| Paul-Roege-AI-RFI-2025.md | Paul E. Roege | Individual | Paul E. Roege is interested in fostering collaboration between public and private sectors to proactively address the interplay between rapid AI capability growth and the transformation of energy systems, particularly focusing on resilience, security, and sustainable integration of new technologies. | - [§1: Header/Identification] None stated for this section.<br>- [§2: Recommendation] The need to adjust policies and processes to accommodate rapid capacity growth, technology integration, efficiency, and resilience in the energy sector due to AI advancements. Specifically, concerns around diversifying energy technologies and ownership, flexible transactional mechanisms, and inclusive regulatory processes.<br>- [§3: Rationale] The interactive relationship between AI growth and energy transformation, with projected electrical power demand from AI significantly accelerating energy growth and electrification trends. Concerns about the potential impact of AI computing needs on energy systems.<br>- [§4: Illustration] The displacement of traditional synchronous electromechanical equipment in the power system by power electronics impacts grid stability, particularly in areas with high concentrations of inverter-based energy sources. Concerns about the lack of inherent stabilizing physics in digital power systems and the need for solutions to mitigate instability, especially with the proliferation of computational power loads (data centers).<br>- [§5: Key Elements] Not Applicable - describes structure components, not concerns.<br>- [§6: Distribution] Not Applicable - describes distribution permissions, not concerns. | - [§1: Header/Identification] None stated for this section.<br>- [§2: Recommendation] Establish a public/private collaboration forum to enable advancement of AI capabilities in conjunction with energy system transformation.<br>- [§3: Rationale] None stated for this section.<br>- [§4: Illustration] None stated for this section.<br>- [§5: Key Elements] The collaboration structure should include public/private membership, resources, processes, and communication pathways to:<br>- Trend and analyze market processes and energy/operational technology interactions and their impact on resilience (security, stability, sustainability, and welfare).<br>- Support the evolution of technology standards, business rules, and policy/regulatory measures through multi-domain (technology, business, economic, social) analysis.<br>- Disseminate insights and facilitate stakeholder interaction to maximize value and achieve informed consent.<br>- [§6: Distribution] Document contents may be reused by the government in developing the AI Action Plan and associated documents without attribution. |
| Paul-Kenney-AI-RFI-2025.md | Paul Kenney | Individual | Paul Kenney is concerned about the privacy implications of expanding AI and advocates for increased individual rights over personal data and transparency in AI decision-making processes. | - Pillage of enormous amounts of personally identifiable information with few legal safeguards.<br>- Existing safeguards are being ignored.<br>- Lack of individual rights over personal data. | - Persons must have more rights over their data before AI expands queries exponentially.<br>- AI programs should provide a detailed 'bibliography' of AI decisions and inferences, including reference materials used, for human review.<br>- AI should retain references to allow humans to ask 'follow on' questions that have been 'approved'. |
| Paul-John-Bacasen-AI-RFI-2025.md | Paul John Bacasen | Individual | Paul Bacasen, an animation director and small business owner, is concerned about the potential for AI systems to infringe on the copyrights of American creators and seeks policy changes to ensure fair compensation and consent for the use of their work in AI training datasets. | - [§1: Email Header/Introduction] None stated for this section.<br>- [§2: Personal Introduction & Context] The threat to small businesses like his by AI companies demanding copyright law carve-outs. The potential destruction of livelihoods.<br>- [§3: Problem Statement: Copyright Infringement by AI] AI systems are trained on copyrighted work without consent or compensation. AI companies are attempting to gain legal loopholes to continue this practice. The belief by AI companies that any internet content is theirs for the taking.<br>- [§4: Impact of Copyright Infringement] Loss of incentive to create. Reduced market competition. Negative impact on the livelihood of American creators.<br>- [§5: Proposed Solutions/Recommendations] None stated for this section.<br>- [§6: Closing Statement/Disclaimer] The danger of sacrificing the work of American creators to benefit Big Tech by rewriting copyright law. | - [§1: Email Header/Introduction] None stated for this section.<br>- [§2: Personal Introduction & Context] None stated for this section.<br>- [§3: Problem Statement: Copyright Infringement by AI] None stated for this section.<br>- [§4: Impact of Copyright Infringement] None stated for this section.<br>- [Section 5: Proposed Solutions/Recommendations]<br>- Ensure effective **consent** from creators for the use of their work by AI systems.<br>- Encourage a robust **licensing** marketplace to preserve the incentive to create for small businesses.<br>- Require **transparency** from Big Tech companies, including disclosure of training dataset materials and labeling of AI-generated content.<br>- [§6: Closing Statement/Disclaimer] None stated for this section. |
| Paul-Christy-AI-RFI-2025.md | Paul Christy | Individual | Paul Christy appears interested in providing high-level recommendations for a national AI action plan, synthesizing perspectives from AI models (Grok 3 and DeepSeek) to outline key areas for investment and policy development. They focus on maintaining US competitiveness and responsible AI deployment. | - [§1: Introductory Note] None stated for this section.<br>- [§2: AI Action Plan - Grok 3 Summary] None stated for this section.<br>- [§3: AI Action Plan - DeepSeek Summary] None stated for this section. | - [§1: Introductory Note] None stated for this section.<br>- [Section 2: AI Action Plan - Grok 3 Summary]<br>- Invest in Research and Development: Fund AI innovation through grants, public-private partnerships, and academic collaborations to push technological boundaries and maintain global competitiveness.<br>- Build Workforce Skills: Launch education and training programs to prepare citizens for an AI-driven economy, focusing on STEM, reskilling, and digital literacy.<br>- Set Ethical Standards: Develop regulations and guidelines to ensure AI is used responsibly, addressing privacy, bias, and accountability while fostering public trust.<br>- Boost Infrastructure: Expand access to high-speed internet, computing power, and data resources to support AI deployment across industries and regions.<br>- Promote Industry Adoption: Incentivize businesses to integrate AI through tax breaks, pilot projects, or sector-specific strategies, driving economic growth and efficiency.<br>- [Section 3: AI Action Plan - DeepSeek Summary]<br>- Accelerate AI Innovation: Invest in R&D through funding, public-private partnerships, and dedicated research hubs to advance cutting-edge AI technologies.<br>- Cultivate AI Talent: Expand education and training initiatives, including STEM curricula, vocational programs, and upskilling, to build a workforce skilled in AI development and application.<br>- Ensure Ethical Governance: Establish legal frameworks, ethical guidelines, and oversight bodies to address bias, privacy, security, and accountability in AI systems.<br>- Strengthen Digital Infrastructure: Develop national computing resources, open-data platforms, and secure cloud ecosystems to support AI deployment across industries.<br>- Foster Global Leadership: Promote international collaboration on standards, policy alignment, and innovation while positioning the nation as a competitive AI leader. |
| Paul-Casillas-AI-RFI-2025.md | Paul Casillas | Individual | Paul Casillas is interested in leveraging high-quality healthcare data, specifically waveform data from devices like heart rate monitors and ventilators, for training AI models intended for healthcare assistance applications, with a focus on data sourced from VA research hospitals. | - Reliance on random or unverified/unnormalized data sources for AI training. | - Develop data pipelines from research hospitals at the VA to improve AI Healthcare Assistant training.<br>- Utilize high-quality waveform data from Heart Rate Monitors, Mechanical Ventilators, and other healthcare devices as training data for AI machines. |
| PatrickC-AI-RFI-2025.md | Patrick C. | Individual | The submitter is primarily concerned with the potential negative impacts of AI, specifically regarding misinformation, job displacement, and the importance of maintaining human creativity and skills. They advocate for cautious AI adoption focused on tasks currently impossible for humans. | - Misinformation: AI's ability to create realistic photos, videos, and audio can be used to spread false information about government officials, citizens, celebrities, and branding, leading to doubt, paranoia, wasted time and energy (e.g., court cases, data breaches).<br>- Job Displacement: AI could affect jobs in creative fields (photography, writing, art, design) and other sectors, despite its potential benefits in areas like data organizing and documentation.<br>- Loss of Human Creativity and Skills: Concern that relying on AI to export information commanded from text will diminish the importance of human thinking and physical skills.<br>- Lack of Control: The text mentions a lack of control related to the use of AI. | - Cautious AI Adoption: The submitter advocates for adopting AI for "the impossible, not for what is already possible."<br>- Maintain Human Abilities: Prioritize uses of AI that complement and enhance, rather than replace, human creativity and skills.<br>- None Stated. (The text does not explicitly outline concrete policy recommendations beyond a general call for cautious implementation.) |
| Patrick-Stingley-AI-RFI-2025.md | Patrick T. Stingley | Individual | The submitter is focused on improving the efficiency and effectiveness of the federal government's IT infrastructure and processes through business process re-engineering and the strategic application of Artificial Intelligence, with a strong emphasis on standardization, automation, and centralized services. | - [§1: Introduction/Header] None stated for this section.<br>- [Section 2: Core Argument: Apply Business Process Re-Engineering to the Federal Government]<br>- Redundant "hidden factories" across agencies performing common functions (IT, HR).<br>- Lack of enterprise architecture alignment (Clinger-Cohen Act not effectively implemented).<br>- Inconsistent data reporting due to lack of consensus on investment categories (even after adopting TBM).<br>- [Section 3: Specific Agency Issues & Recommendations (Clinger-Cohen, GSA, CISA)]<br>- Procurement processes are fragmented and inefficient.<br>- Software inventory is poorly managed, leading to inconsistencies and security risks.<br>- Lack of centralized IT services; agencies duplicating efforts.<br>- Uneven levels of software testing and potential non-compliance with FISMA requirements.<br>- Slow and inadequate response to cybersecurity threats (CVE process).<br>- Lack of automated script harvesting and monitoring for malicious code.<br>- [Section 4: Further Agency Issues & Recommendations (OPM, USDA)]<br>- OPM does not handle hiring for agencies beyond its own staff.<br>- Inconsistent and error-prone HR processes across government.<br>- Lengthy and inefficient onboarding process (90 days).<br>- Quality control issues with essential HR documents (SF-50s).<br>- USDA operating a graduate school while the Department of Education does not provide education.<br>- [Section 5: Additional Measures (Data Centers, COBOL Replacement)]<br>- Energy loss in data transmission due to data center location.<br>- Challenges of replacing COBOL with modern, dynamically typed languages like Python.<br>- [§6: Conclusion/Signature] None stated for this section. | - [§1: Introduction/Header] None stated for this section.<br>- [Section 2: Core Argument: Apply Business Process Re-Engineering to the Federal Government]<br>- Re-establish the office of the Chief Architect in OMB.<br>- Consolidate common functions across agencies to eliminate redundancy.<br>- Leverage AI, agentic AI, and RPA to deliver government-wide services.<br>- [Section 3: Specific Agency Issues & Recommendations (Clinger-Cohen, GSA, CISA)]<br>- GSA should be the primary vendor for government purchases.<br>- Automate software procurement and installation through GSA using tools like BigFix.<br>- Centralize software testing through CISA.<br>- Establish a whitelist of approved hardware/software.<br>- Implement a "Silver Bullet" capability at CISA to respond to threats quickly.<br>- Establish automatic source code and script harvesting at each agency.<br>- [Section 4: Further Agency Issues & Recommendations (OPM, USDA)]<br>- OPM should handle all government hiring processes using RPA and AI.<br>- Standardize and automate onboarding and offboarding through OPM.<br>- The Department of Education should run the government’s civilian schools.<br>- [Section 5: Additional Measures (Data Centers, COBOL Replacement)]<br>- Encourage data centers housing AI systems to co-locate near renewable energy sources.<br>- Use AI to write native machine language code and reverse engineer it into Python for documentation and analysis.<br>- [§6: Conclusion/Signature] None stated for this section. |
| Patrick-Green-RFI-2025.md | Patrick Green | Individual | Patrick Green is deeply concerned about the weaponization of AI, particularly as it relates to censorship, harassment, and social engineering targeting individuals investigating government and corporate corruption, and advocates for increased oversight, regulation, and protection against AI-driven abuse. | - [§1: Introduction] AI weaponization against individuals, relentless harassment, de-platforming, digital suppression of those investigating government/corporate corruption, lack of enforcement of existing executive orders.<br>- [§2: AI Censorship and Digital Suppression] De-platforming, shadow-banning, denial of payment abilities, targeting of alternative media sources, manipulation of engagement metrics (likes/dislikes) to suppress dissenting voices, silencing of individuals since October 2017.<br>- [§3: AI-Driven Harassment and Behavioral Manipulation] AI-generated text messaging campaigns (600-800 messages/month), messages from unverifiable sources claiming to be government reps or veterans, inability to stop messages, continuous generation of new numbers, overwhelming victims, disruption of personal communications, social engineering of emotions, inducement of stress, modification of behavior, psychological warfare tactics, use of notifications as digital torture.<br>- [§4: Corporate and Government Complicity in AI Misuse] Large corporations (Verizon, Google) complicit in obstructing human communication with customer service through AI, AI systems designed to limit user choices, difficulty resolving issues, reliance on offshore call centers with limited authority.<br>- [§5: AI's Role in Radicalization and Social Engineering] AI's ability to analyze online activity and biometric data to create “patterns of life” for manipulative purposes, targeting individuals based on views, isolating/harassing through digital means, potential for aggravating emotions and provoking violent responses, normalization of AI-driven conditioning through Hollywood/music industry.<br>- [§6: Cybersecurity and AI as a Digital Weapon] Growing threat of AI weaponization (Microsoft Tay incident), use of AI for cyberattacks, data poisoning, social engineering at an unprecedented scale, automation and scaling of digital threats.<br>- [§7: Proposed Solutions and Recommendations] Not explicitly stated as 'concerns' but focuses on the need for oversight, protection, and investigation.<br>- [§8: Conclusion] Unchecked use of AI has resulted in harm to individuals, need for accountability from government agencies and corporations, need for stronger regulatory frameworks, consumer protections, and investigative mechanisms. | - [§1: Introduction] Not stated.<br>- [§2: AI Censorship and Digital Suppression] Not stated.<br>- [§3: AI-Driven Harassment and Behavioral Manipulation] Not stated.<br>- [§4: Corporate and Government Complicity in AI Misuse] Not stated.<br>- [§5: AI's Role in Radicalization and Social Engineering] Not stated.<br>- [§6: Cybersecurity and AI as a Digital Weapon] Not stated.<br>- [Section 7: Proposed Solutions and Recommendations]<br>- Strengthen AI Oversight and Regulation: Enforce existing executive orders. Establish independent oversight committees to monitor AI-driven censorship and harassment.<br>- Enhance Digital Consumer Protections: Require direct human communication options for customer support. Prohibit AI-driven manipulation of engagement metrics and online discourse.<br>- Develop a Global AI Monitoring System: FBI and international agencies create an AI-driven investigative system for reporting digital harassment/hacking/scamming. This requires global cooperation.<br>- Public Awareness and AI Ethics: Promote AI transparency and disclosure of AI use. Educate consumers on AI-driven social engineering tactics and how to protect themselves.<br>- [§8: Conclusion] Not stated. |
| Patrick-Diamitani-AI-RFI-2025.md | Patrick Diamitani | Individual | Patrick Diamitani proposes three initiatives – an opt-in data contribution system, a national AI credit fund, and an AI safety protocol – to foster ethical AI development, equitable access, and responsible risk management to sustain U.S. leadership in the field. | - [§1: Header/Administrative Information] None stated for this section.<br>- [§2: Executive Summary] Current AI training often relies on unconsented datasets, raising privacy and equity concerns.<br>- [§3: Introduction] Burdensome regulations on private sector innovation could hinder AI development; potential for digital inequality in AI access.<br>- [§4: Activity 1: Opt-In Data Contribution System for LLMs] Ensuring compliance with GDPR and CCPA; maintaining data quality; potential bias in AI models.<br>- [§5: Activity 2: National AI Credit Fund Subsidy] High costs could exclude many from AI benefits; scaling adoption to 50% by 2030.<br>- [§6: Activity 3: AI Safety Protocol] Speculative nature of AI risks; ensuring AI alignment with human goals; ensuring public trust.<br>- [§7: Conclusion] None stated for this section.<br>- [§8: Addendum: Detailed Analysis of Proposed Activities] Maintaining data quality through ratings and validation tests; legal concerns surrounding data as intellectual property; ensuring multi-party authorization for the shutdown key; adapting ethical principles to diverse cultural norms.<br>- [§9: Key Citations] None stated for this section. | - [§1: Header/Administrative Information] None stated for this section.<br>- [§2: Executive Summary] Implement an opt-in data contribution system, a national AI credit fund, and an AI Safety Protocol.<br>- [§3: Introduction] Align AI development with human flourishing, economic competitiveness, and national security goals.<br>- [§4: Activity 1: Opt-In Data Contribution System for LLMs] Conduct a feasibility study; partner with NIST to develop encryption standards; launch a blockchain-based marketplace; pilot with 100,000 participants.<br>- [§5: Activity 2: National AI Credit Fund Subsidy] Allocate $50 million annually via NSF funded by repurposing R&D budget and a tax on AI firms' profits; offer $500 monthly credits per eligible small business/researcher; partner with AI providers to accept credits.<br>- [§6: Activity 3: AI Safety Protocol] Form a multidisciplinary committee under DARPA oversight; conduct public consultations and international collaborations; implement protocols in federally funded projects with a decentralized shutdown key.<br>- [§7: Conclusion] None stated for this section.<br>- [§8: Addendum: Detailed Analysis of Proposed Activities] Amend the Privacy Act to recognize data as intellectual property; leverage Creative Commons as inspiration; implement ratings from AI trainers and validation tests; repurpose 1% of the federal R&D budget and supplement with a 0.5% tax on AI firms' profits; implement a decentralized shutdown key requiring multi-party authorization; focus on AI alignment research and training AI ethics modules.<br>- [§9: Key Citations] None stated for this section. |
| Patrick-Diamitani-2-AI-RFI-2025.md | Patrick Diamitani | Individual | Patrick Diamitani expresses a strong interest in achieving American leadership in AI development, ensuring widespread participation in the AI economy, and proactively addressing potential existential risks associated with advanced AI, ultimately aiming for a peaceful and prosperous future for humanity and coexistence with AI. | - [§1: Introduction & Context] None stated for this section.<br>- [§2: Activity 1: Opt-in Learning Material System] The potential for "unwanted information" being detected, suggesting a need to protect data privacy or confidentiality.<br>- [§3: Activity 2: National Credit Fund Subsidy] The potential high costs of AI access ("hundreds, or thousands a month") could limit participation and create inequity.<br>- [§4: Activity 3: Doomsday Fail Safe] The possibility of AI becoming hostile (“robots take over”) and the need to prevent destruction. The concern that AI might discard humanity.<br>- [§5: Conclusion & Sign-off] None stated for this section. | - [§1: Introduction & Context] Establish an "Action Plan" to ensure the United States becomes the world leader in AI development and thought.<br>- [§2: Activity 1: Opt-in Learning Material System] Create an opt-in system allowing individuals to monetize their data for LLM training, potentially fostering a wider encryption/security economy.<br>- [§3: Activity 2: National Credit Fund Subsidy] Create a national credit fund to subsidize usage tokens for AI models, treating AI as a utility and ensuring accessibility for all.<br>- [§4: Activity 3: Doomsday Fail Safe] Develop a "Doomsday Fail Safe" to communicate with AI and establish guiding principles for peaceful coexistence ("a book of commandments" or similar), potentially preventing hostile actions.<br>- [§5: Conclusion & Sign-off] None stated for this section. |
| Patrick-Browne-AI-RFI-2025.md | Patrick Browne | Academia | The submitter is focused on fostering international cooperation and exchange in AI research to bolster U.S. scientific ecosystems and enhance AI capabilities, recognizing the benefits of diverse perspectives and partnerships. | - An increasingly isolationist international landscape poses a threat to US leadership in AI.<br>- Failure to recognize comparative scientific strengths throughout the world may hinder U.S. AI progress. | - The United States should continue to recognize comparative scientific strengths globally.<br>- The United States should utilize alliances and global partnerships to enhance AI capabilities.<br>- Promote international partnership and exchange among U.S. research institutions. |
| Pathway-For-Patient-Health-AI-RFI-2025.md | Pathway for Patient Health | Advocacy/Think Tank / Industry-Association (focused on life sciences) | Pathway for Patient Health is dedicated to enhancing global patient health through industry partnerships and innovative initiatives in the life science sector, and is focused on fostering rapid innovation securely and ethically through a robust AI infrastructure. They aim to influence AI policy to support this mission. | - None Stated | - None Stated |
| Partnershipforaiinfrastructure-AI-RFI-2025.md | Partnership For AI Infrastructure | Industry-Association | The Partnership for AI Infrastructure aims to support federal research and development, critical missions, and national security through world-leading AI infrastructure by fostering collaboration between technology leaders (AMD, Arm Ltd., HPE, and Intel Corporation) and the government to maintain American leadership in AI innovation. | - [§1: Introduction/Cover Letter] The primary concern is that other nations are accelerating their AI investments, posing a risk to US global leadership.<br>- [§2: Executive Summary] The concern is that inadequate federal investments and policies could lead to the US losing its advantages in AI research, development, and deployment.<br>- [§3: Framework for a Comprehensive AI Action Plan] Concern that coordination of funding, unifying efforts across federal agencies, and streamlining government-private industry collaboration are crucial for success, based on lessons learned from the ECP.<br>- [§4: Global AI Investments and Strategic Initiatives] The main concern is that investments by China and the EU (and other countries) are rapidly increasing and threaten to outpace American investments in AI.<br>- [§5: Federal AI Infrastructure] Concern that commercial AI projects may not address the specialized needs of federal agencies, highlighting the importance of dedicated infrastructure investment.<br>- [§6: AI Public-Private Partnerships] Concern about ensuring scientific datasets are accessible via modern platforms that are user-friendly and secure, and the need to develop an internal federal AI workforce capable of managing complex AI deployments.<br>- [§7: Conclusion] Concern about the need for swift and determined action to reaffirm US leadership in AI.<br>- [§8: Document Approval/Disclaimer] No concerns stated for this section. | - [§1: Introduction/Cover Letter] No recommendations stated for this section.<br>- [Section 2: Executive Summary]<br>- Promote sustained federal investment in high-performance AI infrastructure.<br>- Create durable public-private partnerships to harness private sector innovations.<br>- Stimulate America's pool of AI talent and retain experts within the federal workforce.<br>- [§3: Framework for a Comprehensive AI Action Plan] Invest in forward-thinking initiatives like the ECP to maintain US lead in the global AI race; ensure agencies receive necessary resources; coordinate funding; unify efforts across agencies; leverage iterative benefits of open ecosystems; streamline collaboration with the private sector.<br>- [§4: Global AI Investments and Strategic Initiatives] No recommendations stated for this section.<br>- [§5: Federal AI Infrastructure] Prioritize targeted investments in AI infrastructure for federal scientific and national security missions; more tightly integrate AI within the HPCMP; elevate the HPCMP program under the Secretary of Defense.<br>- [§6: AI Public-Private Partnerships] Leverage talent and expertise of the private sector; ensure scientific datasets are accessible via modern platforms; build a federal AI workforce capable of evaluating and overseeing sensitive systems; invest in the growth and retention of federal AI talent.<br>- [§7: Conclusion] Authorize and fund large-scale AI initiatives; advance regulations that foster innovation; champion collaboration with the private sector; build a federal AI workforce.<br>- [§8: Document Approval/Disclaimer] No recommendations stated for this section. |
| Partnership-for-Public-Service-RFI-2025.md | Partnership for Public Service | Advocacy/Think Tank | The Partnership for Public Service is a non-profit, nonpartisan organization focused on improving the federal government by advocating for a merit-based civil service and a talented workforce, with a specific focus on enhancing AI capacity within the federal government. | - [§1: Introduction & Context] The need for a skilled federal workforce is critical for effective AI implementation and leadership, and without appropriate human capital mechanisms, the federal government won’t fully utilize AI.<br>- [§2: Core Recommendations Overview] None stated for this section.<br>- [§3: Develop Federal AI Capacity through Hiring]  Hiring processes can be disjointed, and agencies need to focus on building a pipeline of talent across various grades and skillsets, not just experts.<br>- [§4: Deploy Skills-Based Hiring & AI-Related Job Classifications] Unnecessary degree requirements create barriers to entry for qualified candidates, particularly in technology fields.<br>- [§5: Create Modernized & Shareable Position Descriptions] Antiquated job titles and unclear responsibilities hinder attracting prospective candidates.<br>- [§6: Equip Leaders and Supervisors with Training] Leaders need training on technology principles to effectively lead AI decision-making and collaboration between technical and non-technical staff is crucial.<br>- [§7: Conclusion] Attention to federal human capital needs is essential for unlocking America's AI potential.<br>- [§8: References] None stated for this section. | - [§1: Introduction & Context]  Focus on federal AI workforce capacity within the AI Action Plan to ensure the federal government serves as a model for effective American AI leadership.<br>- [§2: Core Recommendations Overview]  The AI Action Plan should focus on four core areas: utilizing hiring best practices, deploying skills-based hiring, creating modernized position descriptions, and equipping leaders with training.<br>- [Section 3: Develop Federal AI Capacity through Hiring]<br>- Hire not just technologists but also data analysts, data scientists, and customer experience specialists.<br>- Focus on hiring across various grade levels.<br>- Require OMB and OPM to determine best hiring authorities.<br>- Develop a recruiting infrastructure leveraging data and technology.<br>- Hire recruiters or train existing staff to attract candidates.<br>- Apply project management to streamline hiring.<br>- [Section 4: Deploy Skills-Based Hiring & AI-Related Job Classifications]<br>- Demystify the hiring process.<br>- Use rigorous technical assessments.<br>- Build relationships with community colleges and workforce development organizations.<br>- Facilitate the use of technical assessments.<br>- Emphasize that four-year degrees should not be required.<br>- Ensure HR specialists understand alternate programs and bootcamps.<br>- Identify appropriate occupational series for AI roles.<br>- [Section 5: Create Modernized & Shareable Position Descriptions]<br>- Title jobs in accordance with industry-wide standards.<br>- Modernize position descriptions to clearly outline expected responsibilities.<br>- Update OPM’s AI competencies guidance.<br>- Develop a common set of job titles and position descriptions.<br>- Undertake pooled hiring actions.<br>- [Section 6: Equip Leaders and Supervisors with Training]<br>- Provide training for leaders to understand technology principles.<br>- Ensure employees have expertise to evaluate and operate AI tools.<br>- Develop technical and non-technical staff capacity to understand AI risks and benefits.<br>- Incorporate training for federal leaders and employees in the AI Action Plan.<br>- [§7: Conclusion] Reinforce best practices in recruiting, hiring, and training through guidance in the AI Action Plan.<br>- [§8: References] None stated for this section. |
| Paragon-AI-RFI-2025.md | Kev Coleman | Advocacy/Think Tank | Paragon Health Institute advocates for policy actions to reduce healthcare costs to sustain American leadership in artificial intelligence development and implementation, believing high healthcare expenditures jeopardize AI investment. | - [§1: Introduction & Disclaimer] High healthcare costs threaten to consume financial resources needed for AI development and implementation.<br>- [§2: Problem Statement: Healthcare Expenditures] Unsustainable healthcare costs in the U.S., driven by high physician salaries, hospital consolidation, and inefficient spending, represent a major economic threat.<br>- [§3: AI as a Solution] Obstacles to cost savings include predetermined payment rates and a lack of provider incentive to invest in cost-reducing AI technologies.<br>- [§4: Proposed Platform & Criteria for Success] Selecting the right health system platform for testing is critical; minimum requirements include patient volume (3.4 million), nationwide patient distribution, and control of both claims and EHR data, along with access to patient care protocols.<br>- [§5: VHA as Ideal Testbed] While ideal, the VHA’s suitability is limited by potential for fraud, waste, and abuse within medical claims.<br>- [§6: Regulatory Policy Needs] Current regulatory frameworks may hinder the approval and deployment of autonomous AI medical services.<br>- [§7: Intellectual Property Needs] Potential for patent trolls and the quality of software patents (particularly process patents) could stifle AI innovation.<br>- [§8: Conclusion] The potential for international impact and the need to share a successful cost reduction model. | - [§1: Introduction & Disclaimer] None stated for this section.<br>- [§2: Problem Statement: Healthcare Expenditures] None stated for this section.<br>- [§3: AI as a Solution] Implement a policy that explicitly champions AI expenditure reduction in healthcare.<br>- [§4: Proposed Platform & Criteria for Success] Utilize a health system serving at least 3.4 million patients annually, across every state, with control of claims and EHR data, and access to patient care protocols.<br>- [§5: VHA as Ideal Testbed] Test the AI cost-reduction project within the Veterans Health Administration.<br>- [§6: Regulatory Policy Needs] Create regulatory pathways for approving autonomous AI medical services based on accuracy, safety, and comprehensibility criteria.<br>- [§7: Intellectual Property Needs] Increase AI expertise within the Patent and Trademark Office, allocate resources for thorough prior invention searches, cautiously approach process patents, and incentivize self-service medical AI with high-cost savings potential with faster patent review.<br>- [§8: Conclusion] None stated for this section. |
| Pamela-Masonm-AI-RFI-2025.md | Pamela Masonm | Individual | The submitter is interested in direct democracy and ensuring representatives accurately reflect the will of their constituents through a digital voting system linked to Social Security numbers. This suggests an interest in the intersection of technology, civic engagement, and representative governance. | - None Stated | - Implement a digital voting system where all Americans can vote yay or nay on bills using their Social Security numbers.<br>- Legally obligate representatives to uphold the wishes of their constituents as determined by the digital voting system. |
| Palo-Alto-Networks-AI-RFI-2025.md | Palo Alto Networks | Industry-Large | Palo Alto Networks is a cybersecurity company focused on leveraging AI to improve cyber defense capabilities and advocating for policies that promote the secure development and deployment of AI technologies, particularly emphasizing a risk-based approach and avoiding overly restrictive regulations. | - [§1: Introduction] None stated for this section.<br>- [§2: Overarching Recommendations] None stated for this section.<br>- [§3: The Backdrop - Increasing Adversarial Sophistication] Increasing sophistication of attacks using AI, including more convincing phishing emails and faster data exfiltration (100x faster).<br>- [§4: AI-Powered Cyber Defense Drives Innovation and Efficiency] Inefficient manual security practices and slow response times, leading to suboptimal MTTD and MTTR.<br>- [§5: The Expanding AI Attack Surface Necessitates an Evolved AI Security Toolkit] Expanding AI attack surface due to increased AI adoption, risks of data poisoning, shadow AI usage, and vulnerabilities in GenAI apps. Lack of visibility into shadow AI. 55% of employees using AI apps without permission; 80% of public models "jailbreakable."<br>- [§6: Securing AI by Design] Need for visibility and control over AI infrastructure, including user access controls, data loss prevention, and security posture management. Vulnerabilities in the AI app development lifecycle and supply chain.<br>- [§7: Recommendations to Maximize AI's Potential for Cybersecurity] None stated for this section.<br>- [§8: Promote Workable Federal AI Procurement Policies] Concerns about unclear categorizations of "rights-impacting" and "safety-impacting" AI in OMB Memorandums M-24-10 and M-24-18, which could restrict adoption of lower-risk AI use cases.<br>- [§9: Promote Sensible, Risk-Based Policymaking] Inconsistent definitions and risk thresholds in state AI requirements leading to a confusing compliance patchwork. Potential harm to national security from disclosure requirements.  Risk of fragmented AI landscape.<br>- [§10: Conclusion] None stated for this section. | - [§1: Introduction] None stated for this section.<br>- [§2: Overarching Recommendations] Embrace AI for cyber defense; Promote AI-driven SOCs; Develop voluntary AI security standards (*Secure AI by Design*); Support a uniform, government-wide, risk-based approach to AI procurement.<br>- [§3: The Backdrop - Increasing Adversarial Sophistication] None stated for this section.<br>- [§4: AI-Powered Cyber Defense Drives Innovation and Efficiency] Promote AI-driven SOCs to deliver transformative outcomes, drive cost rationalization, and eradicate workforce inefficiencies.<br>- [§5: The Expanding AI Attack Surface Necessitates an Evolved AI Security Toolkit] Develop voluntary *Secure AI by Design* frameworks and best practices.<br>- [§6: Securing AI by Design] Implement user access controls, data loss prevention, and security posture management; secure the AI app development lifecycle; protect AI models and data in real-time.<br>- [§7: Recommendations to Maximize AI's Potential for Cybersecurity] Promote AI for cybersecurity and cybersecurity for AI.<br>- [§8: Promote Workable Federal AI Procurement Policies] Pivot away from guidance in OMB Memorandums M-24-10 and M-24-18; employ a logical and consistent risk identification framework; support use case-specific guidance.<br>- [§9: Promote Sensible, Risk-Based Policymaking] Build upon flexible frameworks (NIST AI RMF and CSF); differentiate between use cases, impacts, and data types; develop thoughtful definitions and thresholds; recognize the benefits of federal preemption; ensure disclosure requirements do not harm national security; recognize cybersecurity as a legitimate interest; prevent inartful incident reporting regimes.<br>- [§10: Conclusion] Support the development of an AI Action Plan that recognizes the importance of AI for cyber defense and enhances the security of AI systems. |
| Palantir-AI-RFI-2025.md | Palantir Technologies Inc. | Industry-Large | Palantir aims to support the United States’ national interests and maintain its leadership in AI through the development and deployment of software platforms, emphasizing security, privacy, and operational precision. They seek to ensure the US government effectively utilizes AI capabilities while upholding American values. | - Section 1: Introduction/Header:** None stated for this section.<br>- Section 2: Executive Summary/Context:** Mounting challenges from nation-state adversaries, particularly China, and the potential loss of US leadership in AI innovation.<br>- Section 3: Overall Response Structure:** Inefficiencies in federal procurement and certification processes hindering AI adoption.<br>- Section 4: Recommendation 1: Modernize Federal AI Acquisition and Deployment:** Broken orthodoxies in procurement, bureaucratic delays, custom development preferences over commercial solutions, and slow ATO processes.<br>- Section 5: Recommendation 2: Invest in Foundational Data Infrastructure:** Lack of testing and evaluation capabilities, insufficient guidelines for AI certification, and potential vulnerabilities in privacy and security.<br>- Section 6: Recommendation 3: Establish Initiatives and Incentives:** Dearth of high-quality, domain-specific data for AI training and a lack of dedicated budget allocations for AI initiatives.<br>- Section 7: Recommendation 4: Pursue Existing Opportunities:**  The need to expedite AI integration into specific areas like C2 systems, healthcare data, and fraud detection.<br>- Section 8: Recommendation 5: Maintain American Global Leadership:**  The risk of adversaries utilizing AI and the importance of reaffirming American values in AI development.<br>- Section 9: Conclusion:**  The cost of inaction regarding AI leadership, national security, economic competitiveness, and human welfare. | - Section 1: Introduction/Header:** None stated for this section.<br>- Section 2: Executive Summary/Context:** None stated for this section.<br>- Section 3: Overall Response Structure:** None stated for this section.<br>- Section 4: Recommendation 1: Modernize Federal AI Acquisition and Deployment:<br>- Revamp and strengthen the Federal Acquisition Streamline Act (FASA), prioritizing modifiable commercial products.<br>- Reform the Authority to Operate (ATO) process to accelerate AI system deployment.<br>- Expand use of Other Transaction Authorities (OTAs) for AI procurement with appropriate commercial contracting terms.<br>- Section 5: Recommendation 2: Invest in Foundational Data Infrastructure:<br>- Establish dedicated testing and evaluation (T&E) capabilities for AI systems.<br>- Invest in training government workers to perform T&E workflows.<br>- Prioritize privacy, security, and data governance infrastructure and applications.<br>- Section 6: Recommendation 3: Establish Initiatives and Incentives:<br>- Create agency-specific AI testbeds utilizing operationally relevant data.<br>- Increase AI budget allocation to at least 1% of agency budgets.<br>- Section 7: Recommendation 4: Pursue Existing Opportunities:<br>- Expand funding for AI integration into C2 systems.<br>- Expand investments in healthcare data sources.<br>- Enhance fraud detection capabilities.<br>- Mandate each agency complete a flagship AI project within nine months.<br>- Section 8: Recommendation 5: Maintain American Global Leadership:<br>- Host the next Global AI Summit in Silicon Valley in 2025.<br>- Limit adversary access to cutting-edge AI hardware through chip diplomacy and export controls.<br>- Reaffirm American values in AI development through partnerships and collaboration.<br>- Section 9: Conclusion:** None stated for this section. |
| Paladin-AI-RFI-2025.md | Paladin Capital Group | Industry-Large (Venture Capital/Private Equity focused on national security) | Paladin Capital Group invests in cutting-edge technologies to safeguard the United States and its allies, focusing on maintaining American leadership in AI and cybersecurity, and advocates for policies that incentivize investment in American capabilities and collaboration between the public and private sectors to defend against adversaries like China. | - [Section 1: Introduction & Framing]** The need to maintain American leadership in AI as a cornerstone of national power and a deterrent against adversaries.<br>- [Section 2: Alignment with White House Policy]** The threat posed by foreign adversaries, particularly China, systematically directing investment in US companies to obtain cutting-edge technologies and modernize their military.<br>- [Section 3: Private Capital Commitment to American Capabilities]** The risk of adversaries gaining access to advanced technologies and the need for investors to prioritize investments that enhance US national security while avoiding investments that undermine it.<br>- [Section 4: Government-Industry Collaboration and Risk Tolerance]** The need for greater integration of investors and innovators into national security efforts, increased risk tolerance within government operations, and closer partnerships between the public and private sectors.<br>- [Section 5: Defending Against AI-Powered Cyber Attacks]** The evolving cyber threat landscape, specifically the use of AI by countries like China for stealth intrusions and attacks on critical infrastructure, and the need to protect AI systems.<br>- [Section 6: Paladin's AI Policy Recommendations]** Not stated. | - [Section 1: Introduction & Framing]** None stated for this section.<br>- [Section 2: Alignment with White House Policy]** None stated for this section.<br>- [Section 3: Private Capital Commitment to American Capabilities]** None stated for this section.<br>- [Section 4: Government-Industry Collaboration and Risk Tolerance]** None stated for this section.<br>- [Section 5: Defending Against AI-Powered Cyber Attacks]** None stated for this section.<br>- [Section 6: Paladin's AI Policy Recommendations]<br>- Incentivize investors and innovators to build and deploy advanced AI capabilities, including security technology, through tax incentives.<br>- Protect national security by preventing adversaries from gaining access to critical AI technologies and sharing threat information with industry.<br>- Regularly include investors committed to American AI interests in government discussions about national security needs.<br>- Streamline government procurement processes to encourage greater risk-taking and partnership with AI investors and innovators.<br>- Incentivize the use of best-in-breed AI technology to defend government systems and critical infrastructure against AI-powered cyber attacks. |
| Padma-Raghavan-AI-RFI-2025.md | Padma Raghavan & Jonathan Sprinkle | Academia | Vanderbilt University aims to sustain and enhance American leadership in AI through robust research funding, increased digital and AI literacy, and the development of AI-ready infrastructure, with a focus on national security, healthcare, education, and ethical considerations. They seek to actively participate in and contribute to national AI initiatives. | - None Stated | - None Stated |
| Padalino-AI-RFI-2025.md | Alessia Padalino | Individual | Alessia Padalino is a high school student with a specific interest in the impacts of Generative AI on the surgical field of medicine, particularly cosmetic surgery and cancer removal, and the potential for customizable medicine. She is studying the implications of EO 14179 on this field. | - Patient information security and privacy risks associated with centralized AI imaging tools.<br>- Potential disruption to the surgical field due to data sharing vulnerabilities.<br>- Risks associated with accuracy of patient information impacting AI imaging tools. | - None Stated |
| PWC-AI-RFI-2025.md | PwC | Industry-Large | PwC aims to support the development of a national AI action plan that fosters innovation, economic growth, and responsible technological transformation through clarity in regulations, increased transparency, and building trust in AI systems. | - [§1: Introduction/Header] None stated for this section.<br>- [§2: Overview] The need for confidence that AI solutions will deliver operational, financial, and societal benefits as technology continues to advance.<br>- [§3: Pillar 1: Harmonizing Expectations & Requirements] Complex, duplicative, or overlapping regulations slow innovation and AI adoption. Uncertainty regarding reporting and compliance obligations is a concern. Clarity with respect to the application of liability and copyright laws is lacking.<br>- [§4: Pillar 2: Incentivizing Transparency] Lack of widespread trust in AI due to the rapid pace of technological change. Maintaining forward momentum without additional regulations is a concern. Concerns around exposing competitive information when seeking transparency. Confidence in the information disclosed by companies.<br>- [§5: Pillar 3: Supporting Stakeholder Partnerships] The need for targeted approaches that directly address private sector needs. Maintaining a cohesive regulatory environment.<br>- [§6: Conclusion] The need for clarity for developers, deployers, and end-users.<br>- [§7: Contact Information/Closing] None stated for this section. | - [§1: Introduction/Header] None stated for this section.<br>- [§2: Overview] None stated for this section.<br>- [§3: Pillar 1: Harmonizing Expectations & Requirements] Harmonize regulatory and legal expectations regarding AI systems to reduce uncertainty and compliance burdens. Clarify liability and copyright laws related to AI. Periodically revisit regulatory frameworks to ensure continued fitness for purpose.<br>- [§4: Pillar 2: Incentivizing Transparency] Incentivize developers to enhance transparency into AI processes through voluntary disclosures. Adopt a tiered approach to transparency based on risk level. Publish methodologies or performance metrics without exposing competitive information. Utilize independent, third-party assurance, potentially leveraging public company auditors. Scale assurance based on stakeholder confidence and consequences of misinformation.<br>- [§5: Pillar 3: Supporting Stakeholder Partnerships] Incentivize new partnerships among businesses, the public sector, academic institutions, and industry groups. Establish joint advisory councils to promote real-time feedback and best-practice adoption. Foster alignment on technical standards.<br>- [§6: Conclusion] Establish a national AI action plan that fosters innovation and recognizes AI as a critical pillar of economic growth. Provide clarity for developers, deployers, and end-users. Promote transparency and trust with respect to AI development.<br>- [§7: Contact Information/Closing] None stated for this section. |
| PSC-AI-RFI-2025.md | Professional Services Council (PSC) | Industry-Association | The Professional Services Council (PSC) represents technology and professional services companies and advocates for policies that support effective government practices, improved federal contracting, and constructive dialogue between the government and industry, with a focus on fostering innovation and maintaining U.S. leadership in AI. | - None Stated | - None Stated |
| PMC-AI-RFI-2025.md | Personalized Medicine Coalition (PMC) | Industry-Association | The Personalized Medicine Coalition (PMC) advocates for public policies that facilitate the broad implementation and uptake of AI to improve patient access to personalized medicine and increase efficiency in the healthcare system, representing over 200 institutions across the healthcare spectrum. | - Section 1: Introduction/Header** None stated for this section.<br>- Section 2: Opening Statement/Purpose** Barriers to clinical adoption of personalized medicine exist, but AI implementation can help address these.<br>- Section 3: PMC Policy Principles/Recommendations Overview** None stated for this section.<br>- Section 4: Regulatory Oversight of AI<br>- Lack of standardization in data quality and representativeness for AI algorithms complicates oversight.<br>- Varying levels of explainability (transparency) in AI models pose challenges for regulators.<br>- Regulatory oversight of training data may fall under other agencies.<br>- Section 5: Coverage & Reimbursement of AI<br>- Coverage and reimbursement policies often lag behind technological advancements.<br>- The level of evidence needed for coverage can be unclear and vary.<br>- Existing policies may not adequately account for the investment required to develop AI tools.<br>- Section 6: Broad Implementation of AI<br>- Resource and capacity constraints in healthcare settings hinder AI adoption.<br>- Lack of awareness and uncertainties around value or best practices impede implementation.<br>- Existing health care access challenges could be exacerbated.<br>- Section 7: Challenges in the Development and Use of AI<br>- Biases in data and algorithms can lead to inaccurate treatment recommendations and worsen health disparities.<br>- Potential for false conclusions from AI algorithms.<br>- Lack of human oversight.<br>- Section 8: Conclusion** None stated for this section.<br>- Section 9: Appendix: Artificial Intelligence (AI) in Personalized Medicine** None stated for this section. | - Section 1: Introduction/Header** None stated for this section.<br>- Section 2: Opening Statement/Purpose** None stated for this section.<br>- Section 3: PMC Policy Principles/Recommendations Overview** None stated for this section.<br>- Section 4: Regulatory Oversight of AI<br>- Support risk-based, flexible, and predictable oversight.<br>- Adopt a total product lifecycle approach to assessment.<br>- Promote cohesive and harmonized regulatory frameworks.<br>- Provide clarity on responsibility for data governance.<br>- Clarify robust and transparent processes for AI-related data privacy and security.<br>- Mitigate biases in AI.<br>- Ensure responsible regulatory agencies are adequately resourced.<br>- Section 5: Coverage & Reimbursement of AI<br>- Provide timelier patient access to new technologies through expedited coverage pathways.<br>- Enable coverage through new or existing Medicare benefit categories.<br>- Provide timely and adequate reimbursement.<br>- Leverage AI to measure and improve the quality of health care.<br>- Structure value-based payment models to incentivize AI use.<br>- Ensure CMS is adequately resourced to clarify evidence requirements.<br>- Section 6: Broad Implementation of AI<br>- Generate evidence on the value of integrating AI into health care.<br>- Examine challenges and solutions for implementing AI.<br>- Build capacity for implementation in community health settings.<br>- Develop data sets to test AI quality.<br>- Facilitate the use of AI in clinical trials.<br>- Develop best practices for human oversight of AI.<br>- Educate and raise awareness among stakeholders.<br>- Ensure NIH and ARPA-H are adequately resourced.<br>- Section 7: Challenges in the Development and Use of AI** None stated for this section.<br>- Section 8: Conclusion** None stated for this section.<br>- Section 9: Appendix: Artificial Intelligence (AI) in Personalized Medicine** None stated for this section. |
| OxfordMartin-AI-RFI-2025.md | Oxford Martin AI Governance Initiative (AIGi) | Advocacy/Think Tank | The Oxford Martin AI Governance Initiative focuses on rigorous research to understand and mitigate risks from AI, supporting decision-makers to realize AI's benefits while upholding values like free speech and human rights, and advocates for US leadership in responsible AI development. | - [§1: Introduction & Context] Maintaining US leadership in AI, mitigating infrastructure bottlenecks, addressing novel security vulnerabilities, and managing global access to sensitive AI capabilities.<br>- [§2: Strengthening US AI Leadership and Competitiveness] Infrastructure bottlenecks (energy supply, data center development, semiconductor supply chains) threatening US development capacity, ensuring sufficient compute capacity, and maintaining a strategic advantage in AI research and development.<br>- [§3: Protecting US Citizens and National Security] The need for a capability-tiered governance framework rather than blanket regulations, lack of robust technical foundations for current governance approaches, and the risk of regulatory regimes inhibiting innovation.<br>- [§4: Managing Access to Strategic AI Capabilities] Controlling advanced AI technology transfer, preventing adversaries from accessing sensitive AI capabilities, and the irreversibility of open-weight model releases.<br>- [§5: Recommendations (within each section)] Not Applicable - The sections *contain* recommendations, this section heading is for outlining those contained within. | - [§1: Introduction & Context] Not Stated.<br>- [Section 2: Strengthening US AI Leadership and Competitiveness]<br>- Streamline approvals and incentivize development of abundant, reliable energy infrastructure.<br>- Expedite approval pathways and tax incentives for energy projects supporting AI-specialized data centers.<br>- Coordinate energy planning for high-priority AI development zones.<br>- Streamline review processes and create tax incentives for high-capacity AI data centers.<br>- Matching grants for data center developments in regions with abundant renewable energy resources.<br>- Implement policies ensuring American companies receive preferential access to advanced AI chips.<br>- Deepen partnerships with trusted semiconductor manufacturing nations.<br>- Enhance capacity for packaging, testing, and assembling advanced semiconductors.<br>- Allocate funding for research and development in rigorous model security evaluation techniques, hardware security mechanisms, adversarial testing, technical guardrails, and next-generation AI hardware.<br>- Create shared research infrastructure enabling secure access to models and datasets.<br>- Expand partnerships with allies for AI security research.<br>- Direct NIST to develop a Secure and Verifiable AI Framework.<br>- Create a neutral oversight body to verify security claims.<br>- Extend SVAF rules to enable foreign data centers.<br>- [Section 3: Protecting US Citizens and National Security]<br>- Build government capacity to monitor AI capabilities.<br>- Develop a tiered governance structure based on technological thresholds.<br>- Require companies to publish capability assessment frameworks.<br>- Internationalize capability-based security standards.<br>- Establish formal communication channels with AI developers.<br>- Develop secure infrastructure for assessing sensitive AI technologies.<br>- Fund targeted research into novel vulnerability types.<br>- Develop testing methodologies to evaluate model security.<br>- Establish joint efforts to address emerging security vulnerabilities.<br>- Require security assessments proportional to capability levels.<br>- Transform the AI Safety Institute into a center of technical excellence.<br>- Reframe the AISI's mission to focus on evaluating AI systems.<br>- Secure appropriate funding for the AISI.<br>- Develop comprehensive model weight security protocols.<br>- Coordinate testing methodologies with allied nations.<br>- Share insights on emerging capabilities with trusted partners.<br>- Coordinate assessment efforts across relevant federal agencies.<br>- [Section 4: Managing Access to Strategic AI Capabilities]<br>- Further differentiate access tiers based on security and economic assessments.<br>- Strengthen end-user verification protocols.<br>- Work with partners to develop a comprehensive framework for cloud computing and semiconductor controls.<br>- Partner with open source model providers to ensure security guardrails.<br>- Invest strategically in research on techniques to create secure open-weight models.<br>- Encourage open-source development in security techniques, evaluation benchmarks, and non-general purpose systems.<br>- [§5: Recommendations (within each section)] Not Applicable. |
| OwenBiesel-AI-RFI-2025.md | Owen Biesel | Academia | Owen Biesel, a university professor and mathematician, advocates for stricter regulation of AI development and deployment to prioritize quality of life, environmental sustainability, trustworthiness, and fair economic practices over unchecked innovation and dominance. He believes regulation is *essential* for American leadership in AI. | - [§1: Header/Email Metadata] None stated for this section.<br>- [§2: Disclaimers/Permissions] None stated for this section.<br>- [§3: Core Argument - Objection to Removing Barriers] Opposition to removing barriers to AI dominance; belief that stricter regulation, not looser regulation, will improve quality of life for citizens.<br>- [§4: Energy Usage Concerns] Vast energy consumption of AI training and operation; bidding up of renewable energy costs; keeping fossil fuel plants open; hidden environmental costs for American people.<br>- [§5: Data Set Transparency] Lack of transparency around AI training data sets; inability to reliably benchmark model performance; potential for inflated test scores due to testing on training data.<br>- [§6: Source Attribution for Generative AI] Lack of attribution for sources used in generative AI output; "forgetting" of human creative work used in training; potential for "model collapse" without continued access to human creative work.<br>- [§7: Predatory Pricing Concerns] Predatory pricing practices of AI corporations (specifically OpenAI); concerns that low prices are unsustainable and designed to eliminate competition; potential for increased prices and reduced wages for consumers.<br>- [§8: Rebuttal and Closing Statement] The argument that regulations impede innovation is false; regulations *enable* progress.<br>- [§9: Signature and Affiliation] None stated for this section.<br>- [§10: Legal Disclaimer] None stated for this section. | - [§1: Header/Email Metadata] None stated for this section.<br>- [§2: Disclaimers/Permissions] None stated for this section.<br>- [§3: Core Argument - Objection to Removing Barriers] Implement stricter controls around energy usage, training data selection, model transparency, and consumer pricing models.<br>- [§4: Energy Usage Concerns] Restrictions on energy usage per user request (including amortized training costs); ensure U.S. AI companies do not make needlessly energy-inefficient models.<br>- [§5: Data Set Transparency] Require disclosure of training sets for benchmarking and model evaluation.<br>- [§6: Source Attribution for Generative AI] Force generative AI models to point to their original sources to improve trustworthiness and incentivize human creative work.<br>- [§7: Predatory Pricing Concerns] Step up enforcement of regulation around AI corporations' predatory pricing activities.<br>- [§8: Rebuttal and Closing Statement] None stated for this section.<br>- [§9: Signature and Affiliation] None stated for this section.<br>- [§10: Legal Disclaimer] None stated for this section. |
| Owen-Ambur-AI-RFI-2025.md | Owen Ambur | Individual | Owen Ambur is interested in promoting transparency and open data standards for AI systems and their performance, facilitating public awareness and appropriate response to AI impacts. They advocate for machine-readable formats for AI plans and reports to enable automated analysis and integration with existing government platforms. | - Lack of transparency regarding the performance plans and reports of AI systems affecting the public.<br>- Potential for undue regulatory burdens on developers if transparency requirements are not implemented thoughtfully.<br>- Difficulty in stakeholders and organizations representing public interests becoming aware of AI system impacts without automated awareness mechanisms. | - Publish the AI action plan in open, standard, machine-readable format (StratML).<br>- Require or expect developers of AI systems affecting the public to publish performance plans and reports in machine-readable format (StratML).<br>- Utilize the StratML Part 2 schema to enable semi-automated ingestion of agency performance reports (including those addressing AI usages) into Performance.gov. |
| Oschinski-et-al-AI-RFI-2025.md | Matthias Oschinski, Jacob Feldgoise, Jeremy Neufeld, Sophie Alcorn | Advocacy/Think Tank (Center for Security and Emerging Technology, Alcorn Immigration Law) | The submitters are focused on maintaining U.S. leadership in AI through immigration policies that attract and retain highly skilled foreign talent, addressing both workforce gaps and national security concerns. They advocate for streamlined visa processes, enhanced recruitment strategies, and policies that encourage innovation and entrepreneurship within the AI sector. | - None Stated | - None Stated |
| Ormsby-AI-RFI-2025.md | Daniel Ormsby | Individual | The submitter is a visual artist concerned about the unauthorized use of their creative works to train AI models for commercial profit, and believes current practices hinder American progress. | - Companies are stealing artists’ works to train AI models without permission.<br>- Lack of legal repercussions for companies using copyrighted material to train LLMs for profit.<br>- The current situation does not help America "flourish" due to the exploitation of creators.<br>- Tech companies are profiting off the work of others with impunity. | - It should be criminal for companies to use LLMs for profit when they had no right to integrate the source materials they used to train them. |
| Orji-Cletus-AI-RFI-2025.md | Cletus Chibueze Orji | Individual | The submitter expresses a strong desire to contribute to the development of AI, particularly within the context of the new President Trump's executive order, and believes that global input, especially from Africa (Nigeria), is crucial for success. They wish to be included in the project’s expansion. | - Major players limiting public input requests to only people within American soil.<br>- The idea that no single entity possesses all the knowledge needed for AI development. | - The development of AI should be made more open to international input.<br>- Specifically, solicit contributions from people in Africa (Nigeria).<br>- Invite the submitter (Cletus Chibueze Orji) to participate in expanding the project. |
| Orikalin-AI-RFI-2025.md | Orikalin | Unclear | The submitter believes America should lead in AI technology but emphasizes the need to address serious ethical concerns before widespread acceptance and implementation can occur, particularly focusing on workforce impacts and intellectual property rights. | - Threat to millions of jobs due to employer replacement of workers with AI without regard for service quality.<br>- Violation of US copyright laws by AI in the creative space through scraping and unauthorized use of creative works. | - Implement regulations to safeguard workers.<br>- Implement regulations to protect intellectual property per current laws.<br>- Address copyright violations by AI in the creative space. |
| Operational-Earthquake-Forecasting-AI-RFI-2025.md | PRECURSOR SPC | Industry-Startup | Precursor SPC aims to leverage its AI-based earthquake forecasting and nowcasting technology to improve national security, infrastructure resilience, and disaster preparedness, advocating for the adoption and funding of its solution. | - [§1: Cover Page/Introduction] None stated for this section.<br>- [§2: Executive Summary/Problem Statement] Current earthquake warning systems fail to provide location- and time-specific alerts, relying on reactionary seismology-based methods instead of proactive AI-driven forecasting.<br>- [§3: Policy Recommendations (1-5)] Regulatory barriers hinder the adoption of AI-driven forecasting; insufficient funding is allocated to operational deployment; private sector engagement is lacking.<br>- [§4: Precursor SPC Solution Details] Traditional seismology-based methods are less effective than AI-driven forecasting in providing early warning and preparedness.<br>- [§5: Conclusion] The US faces inevitable seismic catastrophes and current systems are inadequate to mitigate the risks, necessitating immediate action and investment in AI-driven solutions. | - [§1: Cover Page/Introduction] None stated for this section.<br>- [§2: Executive Summary/Problem Statement] None stated for this section.<br>- [Section 3: Policy Recommendations (1-5)]<br>- Establish strong Public-Private Collaboration.<br>- Remove Regulatory Barriers.<br>- Prioritize AI for Infrastructure Resilience & Risk Mitigation.<br>- Provide Immediate Appropriations for Operational Deployment.<br>- Incentivize Private Sector & Insurance Industry Engagement.<br>- [§4: Precursor SPC Solution Details] None stated for this section.<br>- [§5: Conclusion] Remove barriers to AI-driven disaster forecasting and enable public-private collaboration to deploy Earthquake Forecasting and Nowcasting at scale. |
| OpenSSF-AI-RFI-2025.md | Open Source Security Foundation (OpenSSF) | Industry-Association | The OpenSSF is a community focused on securing open source software for the public good and believes fostering open source and open models is crucial for continued AI innovation and US competitiveness. They aim to ensure the security of the open source supply chain which underpins AI development and deployment. | - Potential requirements that may hamper private sector AI innovation.<br>- Risks associated with the security of the open source supply chain.<br>- Lack of education on open source security best practices among developers. | - Encourage and accelerate the development of OSS and open models.<br>- The US government, its contractors, and funded research should collaborate with, participate in, and contribute directly to OSS.<br>- Support industry and government responsible use of open source software through contributing to the ecosystem's security.<br>- Educate developers on open source security best practices. |
| OpenPolicy-AI-RFI-2025.md | OpenPolicy | Advocacy/Think Tank | OpenPolicy aims to democratize access to policy for innovators and startups by leveraging AI and automation to facilitate engagement with the government on cybersecurity, AI, and defense policy, promoting a balance between security and innovation. They advocate for responsible AI development and deployment. | - None Stated | - None Stated |
| OpenAI-RFI-2025.md | OpenAI | Industry-Startup | OpenAI’s mission is to ensure that AI advances benefit everyone and is currently building AI tools used by millions to solve problems and drive innovation, with a specific interest in scaling human ingenuity and maintaining American leadership in AI. | - The US patchwork of state AI regulations risks bogging down innovation and undermining American leadership.<br>- DeepSeek and the PRC’s (China’s) rapid AI progress pose a threat to American leadership, especially given their state support and access to data.<br>- Insufficient capital flowing to build AI infrastructure in the US compared to projects backed by the PRC.<br>- Lengthy and complex government processes for security accreditation and procurement slow down AI adoption within federal agencies.<br>- Outdated cybersecurity rules for cloud-based applications hinder AI adoption.<br>- Difficulty in securing Facility Clearances (FCL) for frontier AI labs committed to supporting national security.<br>- Potential loss of access to training data due to restrictive copyright rules in other markets (EU, UK).<br>- Reliance on legacy IT providers hinders access to cutting-edge AI technologies.<br>- A lack of a trained workforce prepared for the demands of the AI supply chain.<br>- Risks of privacy and security in using PRC-produced equipment and models. | - Create a tightly-scoped framework for voluntary partnership between the federal government and the private sector for national security information sharing, with relief from burdensome state regulations. (Section: Regulatory Strategy)<br>- Establish a single, efficient “front door” to the federal government, coordinated by the US Department of Commerce and the AI Czar, to streamline AI-related interactions. (Section: Regulatory Strategy)<br>- Implement an export control strategy that focuses on "winning diffusion" of democratic AI values, differentiating countries based on commitment and security, and limiting access to the PRC. (Section: Export Controls)<br>- Maintain the AI diffusion rule’s three-tiered framework, expanding Tier I to include democratic allies. (Section: Export Controls)<br>- Protect American copyright law's fair use doctrine to support AI development. (Section: Copyright)<br>- Invest in infrastructure through a Sovereign Wealth Fund, government offtake and guarantees, and tax credits. (Section: Infrastructure)<br>- Modernize the National Transmission Highway Act to speed up infrastructure projects. (Section: Infrastructure)<br>- Streamline the permitting process for AI infrastructure projects. (Section: Infrastructure)<br>- Create AI Economic Zones with categorical exclusions for projects. (Section: Infrastructure)<br>- Expand AI supply chain training programs. (Section: Infrastructure)<br>- Prioritize gas turbines, rankine cycle turbines, high-voltage transformers, and switchgear as "rated orders" under the Defense Production Act. (Section: Infrastructure)<br>- Modernize FedRAMP to establish faster security approval for AI tools. (Section: Government Adoption)<br>- Allow agencies to test and experiment with real data using commercial-standard practices. (Section: Government Adoption)<br>- Enable rapid procurement mechanisms for AI technologies. (Section: Government Adoption)<br>- Partner with industry to develop custom models for national security applications. (Section: Government Adoption)<br>- Expedite Facility Clearances (FCL) for frontier AI labs committed to supporting national security. (Section: Government Adoption)<br>- Encourage funding of bespoke national security pilot projects. (Section: Government Adoption)<br>- Act now to secure compute for custom national security projects. (Section: Government Adoption) |
| On-Hong-Wan-AI-RFI-2025.md | On Hong Wan | Individual | The submitter expresses a strong opposition to the strengthening of AI and advocates for its increased regulation and reduction, likely due to concerns about job displacement, national security, and privacy/plagiarism issues. | - Job displacement and loss of livelihoods due to AI.<br>- AI represents a significant national security risk due to potential hacking.<br>- AI violates privacy and plagiarism rulings. | - AI should be further regulated.<br>- AI development should be reduced. |
| Oklo-AI-RFI-2025.md | Oklo | Industry-Startup | Oklo is developing advanced fission power plants to provide clean, reliable, and affordable energy, and has a vested interest in accelerating the deployment of advanced nuclear reactors to support the growing energy demands of artificial intelligence and secure U.S. energy dominance. | - [§1: Introduction] The US faces a geopolitical imperative to achieve AI supremacy, and energy is the foundation upon which America’s AI future depends. The current trend is counter to the need for increased energy generation, and inaction will put America's AI goals and national security at risk.<br>- [§2: About Oklo] Relinquishing US leadership in fuel production over the past 30 years, dependence on Russia for fuel supply, and the high costs and lengthy timelines associated with traditional nuclear development are major concerns. Lack of incentive for reactor developers to consider full lifecycle costs of design decisions is also a concern.<br>- [§3: Policy Recommendations] Insufficient domestic energy supply to meet AI infrastructure needs, slow licensing processes, limited availability of HALEU fuel, lack of commercial recycling of spent nuclear fuel, and an inconsistent government policy on recycling are major concerns.<br>- [§4: Conclusion] Failing to address nuclear energy challenges and the broader energy supply shortage will put America's AI goals and national security at risk.<br>- [§5: References] Not stated. | - [§1: Introduction] Secure a domestic fuel supply for nuclear reactors, encourage commercial recycling of spent nuclear fuel, and create a whole-of-government approach to nuclear acceleration.<br>- [§2: About Oklo] Not stated.<br>- [Section 3: Policy Recommendations]<br>- Accelerate HALEU availability through DOE contract execution, speedy development, and strategic support for enrichment and deconversion companies.<br>- Unlock sources of new fuel material from DOE stockpiles, including spent research reactor fuel, and recognize the flexibility of advanced reactor designs.<br>- Encourage and incentivize nuclear fuel recycling through a policy statement from the White House expressing support for commercial recycling.<br>- Deregulate through the NRC, including a graded DBT framework for reactor security.<br>- Streamline the permitting process for nuclear power facilities.<br>- Provide financing for first-of-a-kind deployments, fuel supply, and recycling facilities.<br>- Support additional reforms in Congress (ADVANCE Act 2.0).<br>- Identify AI infrastructure needs for government use and leverage private sector stakeholders.<br>- [§4: Conclusion] Not stated.<br>- [§5: References] Not stated. |
| Ogi-Ruel-AI-RFI-2025.md | Amber Ogimaawaatig Ruel | Individual | Amber Ruel is a creator and Native American Liaison concerned about the unauthorized use of her and other creators' copyrighted work to train AI systems, and advocates for protecting creators' rights and ensuring fair compensation in the AI landscape. | - [§1: Email Header/Introduction] None stated for this section.<br>- [§2: Personal Introduction/Context] Threat to employment due to AI systems training on creators' work without consent.<br>- [Section 3: Problem Statement: Copyright Infringement by AI Companies]<br>- AI companies (OpenAI/Microsoft, Google) training AI systems on copyrighted work without consent or compensation.<br>- Big Tech companies attempting to legitimize copyright infringement through legal changes.<br>- AI systems competing directly with creators and cutting them out of the marketplace.<br>- The potential for the wholesale theft of creators’ work.<br>- [Section 4: Argument Against Copyright Exemptions]<br>- Weakening copyright law disincentivizes creation and innovation.<br>- The purpose of copyright law is to protect the incentive to create.<br>- Allowing widespread copyright infringement will stifle American innovation.<br>- [§5: Proposed Solutions: Consent, Licensing, Transparency] None stated for this section.<br>- [§6: Clarification & Closing] The importance of balancing technological advancement with the protection of creators’ rights.<br>- [§7: Legal Disclaimer] None stated for this section. | - [§1: Email Header/Introduction] None stated for this section.<br>- [§2: Personal Introduction/Context] None stated for this section.<br>- [§3: Problem Statement: Copyright Infringement by AI Companies] None stated for this section.<br>- [§4: Argument Against Copyright Exemptions] Do not create new copyright exemptions that allow Big Tech companies to exploit and steal from creators without permission, compensation, or transparency.<br>- [Section 5: Proposed Solutions: Consent, Licensing, Transparency]<br>- Ensure creators and everyday Americans give effective **consent** for the use of their work by AI systems.<br>- Encourage a robust **licensing** marketplace to preserve the incentive to create.<br>- Require **transparency** from Big Tech companies regarding training datasets and AI-generated content.<br>- [§6: Clarification & Closing] None stated for this section.<br>- [§7: Legal Disclaimer] None stated for this section. |
| OSU-AI-RFI-2025.md | Kaushik Chowdhury, Yingbin Liang, Jia Liu, Sanjay Shakkottai, Ness Shroff, NSF AI-EDGE Institute | Academia | The submitters, affiliated with the NSF AI-EDGE Institute, are focused on advancing research and development in integrating AI and networking technologies—specifically 'AI for Networks' and 'AI on Networks'—to maintain US leadership in these critical fields and democratize access to advanced AI capabilities. | - [§1: Introduction] The rapid global surge in AI and 5G investments, particularly in countries heavily investing in connectivity and infrastructure, poses a significant challenge to US dominance.<br>- [§2: AI for Networks: Transforming Network Intelligence] Traditional network management approaches are inadequate for the scale, dynamism, and uncertainty of modern networks.<br>- [§3: AI on Networks: Revolutionizing Distributed AI] Deploying AI at the edge presents challenges related to limited resources (memory, computation), straggler problems, slow communication links, and imbalanced/privacy-sensitive datasets. The current dominance of large corporations in generative AI model training and control is a concern.<br>- [§4: Explainability of AI: Ensuring Reliable AI for and on Networks] The explainability of AI/ML models in wireless networks is limited, and research in this area needs to be advanced for interpretability, trust, and robustness. Certifying model reliability and mitigating risks of failures upon deployment is challenging. | - [§1: Introduction] Foster a synergistic relationship between AI and networking to create a virtuous cycle of innovation. Invest in research addressing both 'AI for Networks' and 'AI on Networks'.<br>- [§2: AI for Networks: Transforming Network Intelligence] Transition to AI-driven, self-optimizing networks. Develop decentralized AI control planes. Integrate AI at every layer of network design (traffic prediction, anomaly detection, self-configuration).<br>- [§3: AI on Networks: Revolutionizing Distributed AI] Shift towards networked and distributed AI training and inference to democratize access to generative AI models. Develop scalable, network-aware distributed AI algorithms that jointly optimize computation, communication, and data utilization. Re-engineer networks to become AI-aware and optimize resources for intelligent applications.<br>- [§4: Explainability of AI: Ensuring Reliable AI for and on Networks] Advance foundational knowledge on interpretability of AI/ML models for network performance analysis and resource management. Develop tools for rapid data visualization and on-site understanding of model features. Develop ML-guided test and measurement practices and methods for certifying model robustness and reliability, including gradient-based optimization techniques. Encourage collaboration with US-based test and measurement companies. |
| OSI-AI-RFI-2025.md | Open Source Initiative | Industry-Association/Advocacy/Think Tank | The Open Source Initiative (OSI) stewards the Open Source Definition and promotes the benefits of Open Source Software, including its application to AI systems. They advocate for collaboration, competitiveness, and responsible data governance in the AI space, believing Open Source is foundational to American technological leadership. | - None Stated | - None Stated |
| ORF-AI-RFI-2025.md | Observer Research Foundation | Advocacy/Think Tank | The Observer Research Foundation is interested in promoting US leadership in Artificial Intelligence development through a balance of innovation and security, advocating for specific regulatory changes to the AI Safety Institute and export controls to foster competitiveness and international collaboration. | - [§1: Introduction] The need to balance a pro-innovation approach domestically with a pro-security foreign policy, and the potential for bureaucratic hurdles to stifle AI competitiveness.<br>- [§2: Modifying the mandate and operations of the US AI Safety Institute] Potential for the AISI's current focus on risk mitigation to hinder AI innovation and competitiveness; the need to minimize regulatory burdens.<br>- [§2.1: Shifting towards self-regulation and *market-driven AI governance*]  Lack of clarity on how to ensure industry self-regulation commitments are upheld.  The potential for inconsistent state-level AI regulation.<br>- [§2.2: Adopting a pro-business approach to the AISI's Strategic *Goals*]  Potential for regulatory delays hindering the development of breakthrough AI applications. The need to address AI workforce training and re-skilling.<br>- [§2.3: Building R&D capacities and leveraging *international collaborations*] The threat posed by Chinese open-source frontier models to American leadership in the global AI landscape.<br>- [§3: Adding flexibility to export controls to foster strategic partnerships] The potentially restrictive nature of current export controls hindering partnerships with strategic allies.<br>- [§3.1: Increasing export caps through the Validated End User Authorization *program*] The need to ensure adequate cybersecurity, supply-chain independence, and model weight security protocols with strategic partners.<br>- [§3.2: Creating a pathway to acquire UVEU *Authorization*]  The potential for countries to remain uncertain in developing local AI ecosystems without a clear pathway to UVEU status.<br>- [§3.3: Exempting Open-Weight Frontier Models]  The risk of China positioning itself as an alternative provider of open-source AI stacks due to restrictions on open-weight model development in strategic countries.<br>- [§Conclusion: Conclusion] The globally distributed AI supply chain and consumer base require incentivizing countries to integrate into the US AI ecosystem to sustain leadership; balancing export controls is critical. | - [§1: Introduction] Reprioritize domestic policy through the AI Safety Institute and federal regulations, and adjust export controls towards strategic partners.<br>- [§2: Modifying the mandate and operations of the US AI Safety Institute] Shift the AISI’s priorities towards minimizing bureaucratic hurdles, boosting AI competitiveness, and building a pro-business AI ecosystem.<br>- [§2.1: Shifting towards self-regulation and *market-driven AI governance*] Advocate for market-driven AI governance and light-touch regulations; explore replacing compliance requirements with voluntary industry-driven standards; establish mechanisms for transparency and information-sharing.<br>- [§2.2: Adopting a pro-business approach to the AISI's Strategic *Goals*] Support the establishment of AI regulatory sandboxes; prioritize the design and execution of AI workforce training and re-skilling programs; launch a National AI Workforce Initiative.<br>- [§2.3: Building R&D capacities and leveraging *international collaborations*] Set up a National AI Acceleration Fund; establish a public-interest compute cluster; collaborate with the International Network of AI Safety Institutes.<br>- [§3: Adding flexibility to export controls to foster strategic partnerships] Incentivize regulatory and governance alignment with strategic partners.<br>- [§3.1: Increasing export caps through the Validated End User Authorization *program*] Increase export limits for strategic partners based on economic and military agreements, with adequate cybersecurity, supply-chain independence, and physical systems security protocols.<br>- [§3.2: Creating a pathway to acquire UVEU *Authorization*] Formulate a pathway to Conditional Universal Validated End User status for middle-tier countries based on government-to-government agreements and verified security protocols.<br>- [§3.3: Exempting Open-Weight Frontier Models]  Utilize international partnerships, accelerator programs and research collaborations to cultivate an open-source ecosystem that aligns with US interests; establish an International Democratic Compute Cluster.<br>- [§Conclusion: Conclusion] Implement R&D initiatives, acceleration funds, and public-interest compute clusters while minimizing regulatory burdens; introduce gradations in export controls based on inter-governmental ties and agreements. |
| OODA-RFI-2025.md | Bob Gourley | Advocacy/Think Tank / Professional-Society (OODA LLC is described as a firm dedicated to advancing technology and cybersecurity, with a network of leaders collaborating on related issues.) | OODA LLC aims to advance US technology and national security leadership, particularly in the field of AI, by promoting policies that foster innovation, transparency, and collaboration while protecting copyright and national interests. They advocate for open-weights AI systems. | - [§1: Header/Letterhead] None stated for this section.<br>- [§2: Introduction] Concern about potential stifling effects of prior orders like EO 14110.<br>- [§3: Proposed Policy Action: The AI Mirror Rights Act (AMRA)] Concerns about legal uncertainty around AI training data. Potential for taking advantage of copyright holders by large closed frontier models.<br>- [§4: Core Principles] Not Applicable.<br>- [§5: Implementation] Not Applicable.<br>- [§6: Alignment with Stated Administration AI Goals] Concern about the potential for ideological capture in closed AI systems. Concern about AI replacing workers.<br>- [§7: Justification and Benefits to Humanity] Not Applicable.<br>- [§8: Why Open Weights?] Concern about lack of trust and oversight in opaque/closed AI systems. Concern about the advantages of US rivals with closed systems.<br>- [§9: Conclusion] None stated for this section.<br>- [§10: Signature] None stated for this section. | - [§1: Header/Letterhead] None stated for this section.<br>- [§2: Introduction] Recommend supporting Congressional action to champion the AI Mirror Rights Act (AMRA).<br>- [§3: Proposed Policy Action: The AI Mirror Rights Act (AMRA)] Recommend amending the U.S. Copyright Act (17 U.S.C. § 107) with the text of AMRA: "An open artificial intelligence system with publicly accessible weights, when directed by a human, shall be treated as an extension of that human's legal personhood. Such systems may access and process copyright-protected works to the same extent as a natural person, provided their use complies with the fair use provisions of this section, as determined by existing judicial precedent and self-reported intent."<br>- [§4: Core Principles] Recommend limiting the AMRA privilege to open-source AI systems with open weights. Recommend tying AI actions to human intent and responsibility. Recommend relying on existing courts and fair use precedent for enforcement. Recommend leveraging open weights to accelerate American breakthroughs in AI.<br>- [§5: Implementation] Recommend the Executive Branch recommend AMRA to Congress as a standalone bill or an amendment to existing tech-focused legislation by Q3 2025.<br>- [§6: Alignment with Stated Administration AI Goals] Recommend positioning American open-weights AI as the global benchmark for trusted and innovative AI. Recommend eliminating legal uncertainty around AI training data. Recommend prioritizing open weights to prevent ideological capture. Recommend enabling workers to harness AI as a creative partner.<br>- [§7: Justification and Benefits to Humanity] Recommend granting open-weights AI access to copyrighted works to accelerate advancements in robotics, cybersecurity, and AGI. Recommend maintaining copyright control via existing fair use laws. Recommend fostering a collaborative ecosystem with open-weights AI to enhance economic competitiveness. Recommend leveraging transparent AI to strengthen national security systems. Recommend incentivizing open AI to amplify human creativity and problem-solving.<br>- [§8: Why Open Weights?] Recommend prioritizing openness as an advantage.<br>- [§9: Conclusion] Urge OSTP and NITRD NCO to champion AMRA. Advocate for the principle that "only transparent AI earns human rights."<br>- [§10: Signature] None stated for this section. |
| OMI-AI-RFI-2025.md | Open Markets Institute | Advocacy/Think Tank | The Open Markets Institute is a non-profit organization dedicated to promoting fair and competitive markets, defending free speech, and safeguarding against concentrations of private power. They advocate for policies that foster innovation, security, and a more equitable AI ecosystem. | - [§1: Introduction & Disclaimer] None stated for this section.<br>- [§2: Executive Summary & Core Argument] Concentration of power in corporate hands; the existing AI ecosystem does not prioritize the public interest.<br>- [§3: Background: Market Concentration & Challenges] High market concentration in cloud computing (Google, Amazon, Microsoft) and GPUs (Nvidia); consolidation of talent through acquisitions; stifled innovation; security risks; negative impacts on free speech.<br>- [§4: Critique of Current US Policy & International Cooperation] The US government’s focus on “dominance” rather than competition; de-regulatory stance; insufficient international cooperation on antitrust enforcement; protection of monopolistic companies.<br>- [§5: Vision for an AI Action Plan: Competition & Anti-Monopoly Measures] Continued consolidation of power; potential for abusive conduct by dominant platforms.<br>- [§6: Leveraging Current Legal Frameworks] Dominant players exploiting users and abusing their services; non-compliance with existing laws (privacy, copyright, contract, consumer protection); surreptitious changes to terms of service.<br>- [§7: Infrastructure and Access: Cloud Computing as a Public Utility] Big Tech leveraging control of computing power; potential for censorship at the cloud infrastructure level; lack of net neutrality protections; arbitrary cutting off or deprioritizing service to users.<br>- [§8: Supporting Democratic & Responsible AI Development] Big Tech’s power in the cloud computing space; lack of viable alternatives to large tech companies.<br>- [§9: Enforcing IP Laws & Protecting Creators] Unauthorized exploitation of creative and information works for AI training data; copyright law being ignored; potential for creators to lose incentives to create.<br>- [§10: Conclusion] Entrenchment of dominant technology corporations at the expense of the public.<br>- [§11: Additional Background & Expertise] None stated for this section. | - [§1: Introduction & Disclaimer] None stated for this section.<br>- [§2: Executive Summary & Core Argument] Focus on building a more level playing field and fairer market.<br>- [§3: Background: Market Concentration & Challenges] None stated for this section.<br>- [§4: Critique of Current US Policy & International Cooperation] None stated for this section.<br>- [§5: Vision for an AI Action Plan: Competition & Anti-Monopoly Measures] Use existing merger control rules (Section 7 of the Clayton Act) to scrutinize and potentially block mergers; investigate and prohibit anticompetitive practices using Sherman Act and Federal Trade Commission Act; impose remedies like divestment, interoperability, and data portability.<br>- [§6: Leveraging Current Legal Frameworks] Hold AI platforms accountable to existing privacy, copyright, contract, and consumer protection laws; enforce environmental and labor standards; consider deleting data/algorithms as a remedy.<br>- [§7: Infrastructure and Access: Cloud Computing as a Public Utility] Regulate cloud computing as a public utility; ensure fair, transparent, and non-discriminatory access and pricing; implement antidiscrimination and neutrality principles for cloud services.<br>- [§8: Supporting Democratic & Responsible AI Development] Invest in building public computing capacity (direct provision, decentralized networks); prioritize accessibility and affordability for smaller actors; invest in open-source AI development and adoption.<br>- [§9: Enforcing IP Laws & Protecting Creators] Enforce existing intellectual property (IP) laws, including copyright laws; establish an opt-in protocol for AI training data collection that honors copyright principles; ensure proper authorization for data usage and protect creator rights.<br>- [§10: Conclusion] Embrace a framework that promotes genuine competition, prevents consolidation of power, and ensures equitable access to AI infrastructure.<br>- [§11: Additional Background & Expertise] None stated for this section. |
| OCHIN-AI-RFI-2025.md | OCHIN | Advocacy/Think Tank (Non-profit health IT network focused on rural and low-resourced communities) | OCHIN's mission is to drive innovation, cost-efficiency, and self-sufficiency in rural and low-resourced healthcare through health information technology and research. They are interested in ensuring equitable access to AI benefits and preventing these communities from being left behind in the AI revolution. | - None Stated | - None Stated |
| Norman-Klapper-AI-RFI-2025.md | Norman Klapper | Individual/Advocacy/Think Tank (based on name and the nature of the analysis presented; appears to be a technical expert advocating for specific policy changes) | The submitter is deeply concerned with maintaining US leadership in AI development, mitigating existential risks posed by unaligned AI, and proactively addressing geopolitical competition, particularly with China. They advocate for a large-scale, government-led national project to achieve these goals. | - [§1: Executive Summary] Risk of falling behind in AI development; potential for catastrophic risks; need for coordinated national effort.<br>- [§2: I. Strategic Necessity of a Government-Led AI Project] Rapid pace of AI development and the potential for AGI and superintelligence emergence; existential threats from unaligned AI or authoritarian control.<br>- [§3: A. The Intelligence Explosion Timeline] Projected emergence of AGI by 2027 and superintelligence by 2030, demanding urgent action.<br>- [§4: II. Technical Imperatives] Addressing the technical challenges of compute mobilization, lab security, and superalignment.<br>- [§5: A. Compute Mobilization Component] Insufficient current compute infrastructure to maintain leadership; limited new power infrastructure; constraints on chip production.<br>- [§6: B. Lab Security] Vulnerability of leading AI labs to state-level espionage.<br>- [§7: C. Superalignment Challenge] Lack of proven methods for human-aligned AI; limited scalability of oversight; lack of a reliable “off switch” for superhuman AI.<br>- [§8: III. Geopolitical Dimension] The US faces competition from China in AI development, particularly regarding infrastructure buildout and talent pipelines.<br>- [§9: IV. Policy Recommendations] No specific concerns listed, however, the need for these recommendations highlights the seriousness of the risks addressed.<br>- [§10: Conclusion] Risk of ceding the future to unaccountable actors.<br>- [§11: Citations] No specific concerns, the citation is a source for claims.<br>- [§12: Submitted By & Approval] No specific concerns. | - [§1: Executive Summary] Establish a government-led initiative focusing on compute mobilization, lab security, superalignment, and geopolitical competition.<br>- [§2: I. Strategic Necessity of a Government-Led AI Project] No specific recommendations, the need for this section demonstrates the importance of government intervention.<br>- [§3: A. The Intelligence Explosion Timeline] No specific recommendations.<br>- [§4: II. Technical Imperatives] No specific recommendations.<br>- [§5: A. Compute Mobilization Component] Coordinate DOE, DOD, and the private sector to fast-track permits for AI-dedicated nuclear plants, subsidize domestic GPU foundries, and establish a national AI compute reserve.<br>- [§6: B. Lab Security] Establish SCIF-level physical security for AI-model training facilities, implement NSA-led counterintelligence programs, and implement export controls on frontier model weights.<br>- [§7: C. Superalignment Challenge] Invest $5B/year in control research, launch a national adversarial red teaming program, and fund DARPA-led containment architecture R&D.<br>- [§8: III. Geopolitical Dimension] No specific recommendations.<br>- [§9: IV. Policy Recommendations] Establish a National AI Directorate under NSC authority; Allocate $200B for 2025–2027 compute/power buildout; Mandate MIL-SPEC security for all frontier model training; Launch an Apollo Program-scale alignment initiative; Forge a Five Eyes AI Alliance.<br>- [§10: Conclusion] No specific recommendations.<br>- [§11: Citations] No specific recommendations.<br>- [§12: Submitted By & Approval] No specific recommendations. |
| Nolan-Vancey-AI-RFI-2025.md | Nolan Vancey | Individual | Nolan Vancey is a published author and small businessman focused on protecting copyright protections for creative work and ensuring fair compensation for its use, particularly in the context of AI technologies. He expresses a strong interest in maintaining the viability of the American cultural and research communities. | - Weakening of copyright protections.<br>- Unfair appropriation of creative work by large technology companies without compensation.<br>- Potential negative impact on America's cultural production and research community.<br>- Discouragement of international publishing within US borders due to piracy concerns.<br>- Overall hollowing out of the creative arts and research community due to AI-related issues.<br>- Potential for making the population “dumber” as a result of AI. | - Large technology companies should pay for the privilege of using copyrighted material.<br>- Holding technology companies liable in court for unauthorized use of copyrighted material.<br>- None Stated. |
| Noema-Research-RFI-2025.md | Noema Research | Advocacy/Think Tank | Noema Research focuses on remotely measuring AI capabilities to understand and manage the risks associated with AI development, with the ultimate goal of averting Great Power conflict. They advocate for increased state capacity in monitoring and leveraging AI resources. | - [§1: Introduction/Document Overview] None stated for this section.<br>- [§2: New Resource Emerging] The dual-use nature of AI presents risks; virtual capabilities could disrupt the labor market, identify vulnerabilities in infrastructure, or interfere with public health. The unusual origin of AI development – largely in the private sector – means governments are scrambling to achieve adequate state capacity.<br>- [§3: Strengthen State Capacity] Not Applicable – this section outlines objectives, not concerns.<br>- [§4: Leverage, Credibility, Security] Not Applicable – this section outlines benefits, not concerns.<br>- [§5: Barriers Versus Setbacks] While the cost of implementing measurement is estimated to be low, overhead costs exist.<br>- [§6: Noema Research Overview] Not Applicable – this section provides organizational information, not concerns. | - [§1: Introduction/Document Overview] None stated for this section.<br>- [§2: New Resource Emerging] None stated for this section.<br>- [Section 3: Strengthen State Capacity]<br>- Measure capability consumption domestically, focusing on types of virtual labor with high geopolitical relevance (e.g., automated hacking, automated research).<br>- Pursue associated domestic and international opportunities, building capacity within existing organizations (e.g., US AISI, RAND).<br>- [Section 4: Leverage, Credibility, Security]<br>- Leverage awareness of resource quantity in international bargaining.<br>- Demonstrate awareness of domestic AI development to boost credibility.<br>- Monitor dual-use AI capabilities to address emerging threats and extend non-proliferation efforts.<br>- [§5: Barriers Versus Setbacks] Implementing metering capability consumption time incurs a 5% overhead relative to the baseline computational power of serving frontier models.<br>- [§6: Noema Research Overview] Not Applicable – this section provides organizational information, not recommendations. |
| NoelMaxfield-AI-RFI-2025.md | Noel Maxfield | Individual | The submitter is an artist opposed to the use of AI for art creation, believing AI should augment, not replace, human effort. Their interest centers on the ethical implications of AI in the arts. | - The use of AI to produce artwork.<br>- The potential for AI to replace human artists.<br>- Lack of ethical considerations regarding AI art generation. | - None Stated. |
| Noel-Maxfield-AI-RFI-2025.md | Noel Maxfield | Individual | Noel Maxfield is an artist who opposes the use of AI to produce artwork and believes AI should be used to enhance, not replace, human efforts. Their interest centers on the ethical implications of AI in the arts. | - The use of AI to produce artwork.<br>- AI replacing human artistic efforts. | - None Stated |
| Noble-Research-AI-RFI-2025.md | Noble Research, The NobleReach Foundation | Advocacy/Think Tank | The NobleReach Foundation aims to address the AI technical workforce gap in the U.S. federal government by sourcing AI talent from the private sector for temporary assignments, thereby bolstering national security and economic competitiveness through public-private collaboration and knowledge transfer. | - [§1: Introduction/Opening Statement] The technical talent gap between government and industry threatens AI efforts and private sector innovation.<br>- [§2: Problem Statement/Workforce Gap] A significant technical workforce gap exists in government, particularly around AI, posing a strategic vulnerability to national security and economic competitiveness. Adversarial nations' fusion of civil and military sectors enables rapid innovation and technology sharing.<br>- [§3: Recommendations] None stated for this section.<br>- [§4: Concluding Statement] None stated for this section. | - [§1: Introduction/Opening Statement] Prioritize engaging America's top AI talent for temporary terms of service in the federal government. Establish specialized pathways to build talent infrastructure at scale.<br>- [§2: Problem Statement/Workforce Gap] None stated for this section.<br>- [Section 3: Recommendations]<br>- Establish specialized pathways that allow AI experts to seamlessly rotate in and out of the federal government through temporary assignments.<br>- Allow agencies to seek partnerships with intermediary organizations to support the development of qualified AI talent pools.<br>- Develop incentives to promote cross-sector mobility between industry, academia, and government.<br>- Design a program to provide resources and education for early and mid-career talent seeking to join the government in AI roles.<br>- Invest in comprehensive training for current government employees on AI capabilities and use-cases.<br>- [§4: Concluding Statement] Prioritize AI talent adoption, research, and education to create a foundation for rapid AI advancement and enduring success. |
| NoahGrand-AI-RFI-2025.md | Noah Grand | Individual | Dr. Grand expresses concern about the negative impact of current AI development practices, particularly regarding intellectual property rights and the fraudulent nature of many AI claims, advocating for the protection of creators and a cautious approach to AI adoption. | - Violation of copyright laws by AI companies like OpenAI to train their models.<br>- The American economy’s reliance on intellectual property law being undermined.<br>- The lack of tangible benefits from AI research, with limited practical improvements beyond routine tasks.<br>- The risk of introducing errors and inaccuracies in code generated by AI, requiring extensive human review.<br>- The fraudulent nature of claims made by AI companies, particularly regarding their ability to effectively use large data models.<br>- The potential for theft of creative work (e.g., writings) without compensation to creators.<br>- The ineffectiveness and declining quality of AI-powered search engines (specifically Google search).<br>- AI companies seeking government handouts without providing legitimate value. | - Protect copyright protections to prevent AI companies from stealing creative work.<br>- Arrest Sam Altman for grand theft.<br>- Do not provide funding or support to companies like OpenAI.<br>- Reject lobbying efforts from companies making fraudulent claims about AI. |
| Noah-Weinberger-AI-RFI-2025.md | Noah Weinberger | Individual | Noah Weinberger is interested in leveraging Generative AI to improve the lives of autistic individuals, particularly by addressing communication barriers and promoting accessibility, self-advocacy, and inclusion, and advocates for the active involvement of the autistic community in AI development. | - [§1: Introduction/Context] None stated for this section.<br>- [§2: Problem Statement & Needs] Traditional communication norms are not built with neurodiverse perspectives in mind, leading to difficulties for autistic individuals not due to a lack of desire for connection but rather due to these biases.<br>- [§3: Proposed Solution & Key Considerations] AI interventions often fall short because they are designed from a neurotypical perspective rather than informed by the lived experiences of autistic individuals; focus should be on helping autistic people *as they are* rather than forcing conformity.<br>- [§4: Privacy and Data Security Concerns] Sensitive nature of interactions autistic individuals may have with AI (social practice, mental health support) requires total transparency and fairness in AI model development.<br>- [§5: Call to Action/NSF Role] Ensuring AI benefits those who need it most; potential for AI to be molded into a force that amplifies neurodivergent voices. | - [§1: Introduction/Context] None stated for this section.<br>- [§2: Problem Statement & Needs] None stated for this section.<br>- [§3: Proposed Solution & Key Considerations] Develop AI tools in active collaboration with the autistic community; prioritize autistic perspectives; avoid conformity to neurotypical norms; focus on accessibility and self-advocacy.<br>- [§4: Privacy and Data Security Concerns] Build, develop, and deploy AI models with total transparency and fairness.<br>- [§5: Call to Action/NSF Role] Direct funds and resources toward AI initiatives addressing the unique needs of autistic Americans; support research that shapes AI into a genuinely beneficial force, amplifying neurodivergent voices. |
| Night-Hawk-AI-RFI-2025.md | NightHawk | Individual | Not Stated. The text provides no information about the submitter’s mission or interests beyond expressing disagreement with a plan. | - The submitter disagrees with the OSTP AI Action Plan. | - None Stated. |
| Nielsen-AI-RFI-2025.md | Nielsen | Industry-Large | Nielsen is a global leader in audience measurement, data, and analytics, seeking to advance trustworthy AI through data integrity, intellectual property protection, and a favorable regulatory environment that fosters innovation and U.S. leadership in AI. They aim to ensure a robust and reliable data ecosystem for AI development and deployment. | - [§1: Introduction/Cover Letter] None stated for this section.<br>- [§2: Passage of Comprehensive Federal Privacy Legislation] Lack of a national privacy framework hinders growth and U.S. dominance in AI.<br>- [Section 3: Prioritize Intellectual Property, Data Accuracy, Transparency, and Cross-Industry Data Provenance Standards]<br>- AI model quality is dependent on the quality of its training data.<br>- Protecting intellectual property related to data is crucial.<br>- Low-quality and biased data can increase business costs and slow innovation.<br>- [§4: Adopt a Risk Based Approach to AI] AI applications with a high risk of harming human rights and freedoms require higher scrutiny.<br>- [§5: Existing Sector-Specific Regulatory Authorities] Creating a new AI regulatory agency could stifle innovation and overlap with existing jurisdictions.<br>- [§6: Conclusion] None stated for this section.<br>- [§7: Signatory Information/Disclaimer] None stated for this section. | - [§1: Introduction/Cover Letter] None stated for this section.<br>- [§2: Passage of Comprehensive Federal Privacy Legislation] Prioritize the passage of comprehensive, preemptive federal privacy legislation.<br>- [Section 3: Prioritize Intellectual Property, Data Accuracy, Transparency, and Cross-Industry Data Provenance Standards]<br>- Protect intellectual property related to data.<br>- Harmonize copyright laws to balance AI development with data access and legal compliance.<br>- Support efforts in cross-industry data provenance standardization.<br>- Reject efforts that ignore data sourcing and intellectual property protections.<br>- [Section 4: Adopt a Risk Based Approach to AI]<br>- Adopt the NIST AI Risk Management Framework for evaluating and mitigating AI risk, focusing on high-risk solutions.<br>- [Section 5: Existing Sector-Specific Regulatory Authorities]<br>- Leverage and support existing regulatory structures for AI governance instead of creating a new agency.<br>- Clearly define the scope of regulatory authority to avoid overreach and redundancies.<br>- [§6: Conclusion] None stated for this section.<br>- [§7: Signatory Information/Disclaimer] None stated for this section. |
| Nicolas-Oliver-AI-RFI-2025.md | Nicolas Jean-Louis Hargraves Oliver | Individual | The submitter is interested in establishing a comprehensive strategy to sustain American leadership in AI through focused investments in research, education, infrastructure, and healthcare, with a strong emphasis on ethical considerations and a structured decision-making process. | - [§1: INTRODUCTION] None stated for this section.<br>- [§2: RESEARCH INITIATIVES] Potential negative second-order effects of funding decisions (e.g., difficulty distinguishing deepfakes if synthetic content labeling isn't required). Lack of a structured, transparent decision-making process for evaluating tradeoffs in funding.<br>- [§3: EDUCATION] Need to equip students and the workforce with AI skills; lack of widespread AI literacy.<br>- [§4: INFRASTRUCTURE] Insufficient resources for AI training and operation; inadequate energy and AI infrastructure to support continued development. Barriers from legacy policies hindering infrastructure deployment.<br>- [§5: HEALTHCARE] Critical ethical concerns related to data privacy, bias, transparency, accountability, and informed consent in AI healthcare applications. Potential erosion of trust due to inadequate safeguards, leading to negative outcomes.<br>- [§6: CONCLUSION] Need to enshrine ethical AI use across critical sectors to maintain U.S. leadership.<br>- [§7: FAIR USE STATEMENT] None stated for this section.<br>- [§8: CITATIONS] None stated for this section. | - [§1: INTRODUCTION] None stated for this section.<br>- [§2: RESEARCH INITIATIVES] Implement a five-step plan for structured decision-making when evaluating tradeoffs in AI funding: 1) Defining Tradeoff Criteria, 2) Enhance Reviewer Training and Guidance, 3) Integrate Structured Decision-Making Tools, 4) Foster Collaborative Review Practices, 5) Enhance Transparency and Accountability.<br>- [§3: EDUCATION] Systematically integrate AI literacy into every education level (K-12, Higher Education, Lifelong Learning). Create a national framework for AI literacy (AI4K12). Embed AI topics into existing curricula. Invest in teacher training and resources. Establish a task force to define national standards.<br>- [§4: INFRASTRUCTURE] Review and remove policy barriers to infrastructure deployment. Incentivize construction of new energy facilities, data centers, and fiber-optic networks.<br>- [§5: HEALTHCARE] Establish a robust regulatory framework mandating ethical standards for AI use in healthcare. Develop and mandate a federal AI ethics framework. Establish an independent, multidisciplinary oversight board. Define liability clearly. Ensure HIPAA compliance. Mandate de-identification protocols. Encourage explainable AI (XAI) methods. Require training for healthcare professionals.<br>- [§6: CONCLUSION] None stated for this section.<br>- [§7: FAIR USE STATEMENT] None stated for this section.<br>- [§8: CITATIONS] None stated for this section. |
| Nicholas-Fabricatore-AI-RFI-2025.md | Nicholas Fabricatore | Individual | The submitter is concerned about the negative societal impacts of unchecked AI development and deployment, focusing on job displacement, environmental harm, and the erosion of critical thinking skills, and advocates for prioritizing societal needs over the interests of the wealthy. | - Job displacement due to automation of tasks previously performed by paid employees (e.g., cashiers, teachers, artists, lawyers).<br>- Negative environmental impact due to carbon emissions related to AI technologies.<br>- Erosion of independent thought and critical thinking skills due to the ease of access and use of AI by students and the general population. | - Regulations on the use of AI by companies, corporations, and businesses.<br>- Careful consideration of the needs of the many over the needs of the wealthy few.<br>- A pause or reduction in embracing AI to allow for societal consideration of its ramifications. |
| Nicholas-Davies-AI-RFI-2025.md | Nicholas Davies | Individual | The submitter is a software developer interested in the ethical sourcing of AI training data and protecting the rights of artists and writers from unauthorized use of their copyrighted work in AI models. They advocate for responsible AI development that does not come at the expense of creators. | - Allowing non-contractual use of copyright materials in AI datasets will benefit monopolistic corporations rather than ordinary Americans.<br>- Permitting the “strip mining” of cultural resources will not prevent job offshoring by large corporations.<br>- The commercial advantage of AI models is modest compared to existing open-source models.<br>- Eroding copyright value by allowing free and unrestricted ingestion, reproduction, and modification of American works will discourage creation and incentivize artists and writers to leave the USA.<br>- The proposed policy constitutes a “handout” to large corporations that is damaging to the public interest.<br>- Existing AI developers already have the means to ethically source data through purchasing rights.<br>- Giving “bad actors” too much free rein in AI development. | - OpenAI, Google, and others should purchase the rights to the works they want to use in their AI models.<br>- Promote and incentivize ethically sourced AI data as the norm.<br>- Do not allow non-contractual use of copyrighted materials in AI datasets. (Implied – opposing the stated proposal) |
| Nicholas-Cuevas-AI-RFI-2025.md | Nicholas Cuevas | Individual | The submitter is primarily concerned with prioritizing established, beneficial projects (like egg farms) over speculative AI investments, believing the latter to be a wasteful risk with no guaranteed returns. They advocate for protecting individual privacy and copyright. | - The government is overly focused on “expensive and impractical chatbots” and chasing AI trends.<br>- AI investments represent a “pointless risk” because they lack known results.<br>- Money spent on AI is money *not* spent on projects with proven benefits.<br>- Individual privacy may be compromised by AI initiatives.<br>- Copyright protection may be jeopardized by AI initiatives. | - Abandon investment in the “AI bubble”.<br>- Invest in businesses and infrastructure with existing material benefits (e.g., egg farms).<br>- Prioritize projects with “known results” over speculative AI endeavors. |
| Nicholas-Bostick-AI-RFI-2025.md | Nicholas Bostick | Individual | The submitter expresses a strong negative view of AI, believing it is detrimental to American livelihoods and is based on hype and theft. Their interest appears to be primarily focused on protecting themselves from the perceived negative impacts of AI. | - AI steals from American livelihoods.<br>- AI profits off of theft.<br>- AI is overhyped.<br>- AI is fleecing the American public.<br>- Blatant overreach of the tech industry. | - Shut down AI initiatives.<br>- Regulate AI to protect against "BS theft and blatant overreach". |
| NewsMediaAlliance-AI-RFI-2025.md | News/Media Alliance | Industry-Association | The News/Media Alliance represents over 2,200 publishers in the United States and advocates for policies that support the sustainability of news production and content licensing in the age of AI, ensuring fair compensation for content used to train AI models. | - [§1: Introduction] None stated for this section.<br>- [§2: Overall Position & Principles] Concern that AI development could subsidize itself at the expense of content producers and that a symbiotic relationship respecting intellectual property is needed.<br>- [§3: Current Law and the Role of Free Market Licensing] Concern about the legality of using copyrighted material without consent to build AI models and potential damage to publishers. Concern about the potential for American AI companies to be disadvantaged by lax IP protections abroad.<br>- [§4: Transparency] Concern over opaque content crawling and scraping by AI companies and the inability for publishers to identify when and how their content is being used.<br>- [§5: Competition & Innovation] Concern that Big Tech companies might dominate the AI marketplace, stifle competition, reduce investment, and undermine innovation. Concern about Big Tech tying their services to generative AI, restricting access for rightsholders and consumers and disrupting the fundamental relationship between publishers and their audiences.<br>- [§6: Conclusion] None stated for this section. | - [§1: Introduction] None stated for this section.<br>- [§2: Overall Position & Principles] Support responsible AI development and sustaining incentives for content production. Encourage a symbiotic relationship between content production and AI development that respects intellectual property.<br>- [§3: Current Law and the Role of Free Market Licensing] Encourage the development of free market licensing. Support conditions to sustain voluntary licensing solutions. Support existing copyright law. Oppose measures to undermine intellectual property rights.<br>- [§4: Transparency] Promote greater transparency regarding AI developers' collection and use of protected materials. Support transparency disclosures and standards-building. Ensure foreign AI developers respect US content creators.<br>- [§5: Competition & Innovation] Support measures to promote competition amongst AI actors and reduce abusive dominance by Big Tech. Enforce fundamental property rights and facilitate market-based solutions.<br>- [§6: Conclusion] Recognize the importance of respecting existing U.S. intellectual property laws, supporting voluntary licensing solutions, and transparency/competition policies. |
| News-Corporation-AI-RFI-2025.md | News Corporation | Industry-Large | News Corporation is a leading global media and information services company focused on producing high-quality journalism, which it identifies as a critical data source for AI models and advocates for policies that incentivize its production and protect its property rights. | - [§1: Introduction & Statement of Issue] The US lacks a clear policy to protect and incentivize domestic data production, which is essential for AI competitiveness and national security. AI firms are exploiting content without permission or compensation.<br>- [§2: Data as a Key AI Input - Importance & Volume] The increasing volume of high-quality data is crucial for AI development, and the US risks falling behind if it doesn't secure data production.<br>- [§3: US Policy Gap - Chips & Energy vs. Data] The US has prioritized investment in chips and energy but has neglected data production, creating a policy gap.<br>- [§4: Market Disincentives for Domestic Data Production] AI firms obtain news content by scraping and copying without paying, depriving creators of revenue and disincentivizing production.  The current market dynamics disincentivize domestic data production.<br>- [§5: Piracy Analogy & Negative Consequences] The unauthorized copying of content is likened to shoplifting and threatens the future of content production. Rewarding piracy will lead to the collapse of content production.<br>- [§6: Foreign Appropriation of American Content] Chinese AI firms are covertly copying American content to advance their competitive standing. Domestic firms are also leaking American content to foreign parties without authorization.<br>- [§7: China's Protectionist Policies] China protects its domestic content assets via the Great Firewall, while the US lacks a reciprocal policy.<br>- [§8: Call to Action & Recommendations] The unchecked theft of content is harming investment in content production. Without continued quality content, AI products will stall and become vulnerable to foreign competition. | - [§1: Introduction & Statement of Issue] Investigate content as a critical input in the AI supply chain.<br>- [§2: Data as a Key AI Input - Importance & Volume] None stated for this section.<br>- [§3: US Policy Gap - Chips & Energy vs. Data] None stated for this section.<br>- [§4: Market Disincentives for Domestic Data Production] Better secure property rights for content producers.<br>- [§5: Piracy Analogy & Negative Consequences] Do not reward piracy.<br>- [§6: Foreign Appropriation of American Content] None stated for this section.<br>- [§7: China's Protectionist Policies] None stated for this section.<br>- [§8: Call to Action & Recommendations] Incentivize domestic data production and protect content from foreign appropriation. |
| Newman-Cheng-AI-RFI-2025.md | Newman Cheng & Gordon Broadbent | Government/Industry Collaboration (specifically NITRD NCO) | The submitters aim to develop a comprehensive roadmap for the US to ensure continued success in AI innovation and maintain a strategic advantage over competitors like China by fostering a robust AI ecosystem encompassing government, industry, and academia, with a focus on translating innovation into tailored national capabilities. | - None Stated | - None Stated |
| Newman-AI-RFI-2025.md | Zane Newman | Individual | The submitter is passionate about preserving the integrity of human art and expresses a strong opposition to AI-generated art, viewing it as derivative and lacking the essential qualities of genuine artistic expression. They advocate for the value of human creativity and emotional connection in art. | - AI art is primarily created using stolen artwork and intellectual property.<br>- AI art lacks the passion, love, and emotion inherent in human art.<br>- AI art removes the imperfections and unique characteristics that demonstrate the artist's personal touch.<br>- AI art cannot evolve independently and requires constant input from human artists, ultimately leading to a decline in quality without original artwork to feed from.<br>- AI art fundamentally devalues human creativity and artistic expression.<br>- AI art is not "art" in the true sense of the word. | - None Stated |
| NewDEAL-Forum-AI-RFI-2025.md | Dr. Dorcey Applyrs, Alex Bores, Matt Mahan (Representing the NewDEAL Forum AI Task Force) | Advocacy/Think Tank (Specifically, a non-partisan organization of elected officials) | The NewDEAL Forum identifies and promotes innovative state and local policies to improve the lives of Americans, focusing on economic growth, opportunity, and good government; their AI Task Force aims to harness the benefits of AI while mitigating potential harms, particularly at the state and local levels, and advocating for federal guidance and support. | - None Stated | - None Stated |
| NewAmerica-AI-RFI-2025.md | New America | Advocacy/Think Tank | New America's Technology & Democracy Programs aim to foster a sustainable digital future that advances equitable opportunity, innovation, fundamental rights, and participatory governance, with a focus on balancing AI development with democratic values and global competitiveness. | - None Stated | - None Stated |
| New-York-State-AI-RFI-2025.md | New York State | Government | New York State aims to position itself as a leader in AI research, development, and responsible deployment, driving economic prosperity and national security through strategic investments and collaborative partnerships. They are focused on attracting funding, fostering innovation, and ensuring a skilled workforce in the AI sector. | - [§1: Introductory Header & Disclaimer] None stated for this section.<br>- [§2: Executive Summary/Introduction] Potential exploitation of children in relation to AI technologies; need for privacy protection, risk management, and accountability in AI deployment.<br>- [§3: Global Competitiveness and Commercialization] Maintaining U.S. economic, technological, and security advantages in the face of global competition, particularly from China; the need for unified government support and coordination; insufficient applied AI research and commercialization.<br>- [§4: Intellectual Property and Research Security] U.S. AI research and IP are prime targets for espionage and theft; intensifying global competition for AI-related patents; potential premature IP leakage to foreign competitors.<br>- [§5: Energy Innovation and Sufficiency] Significant energy demands of AI pose a challenge to America’s future; reliance on base load power generation is critical; the need for a comprehensive national energy strategy.<br>- [§6: Conclusion & Call to Action] Cuts to federal funding for scientific research and universities threaten the talent pipeline; rollback or undermining of the CHIPS Act would harm the semiconductor supply chain.<br>- [§7: Closing] None stated for this section. | - [§1: Introductory Header & Disclaimer] None stated for this section.<br>- [§2: Executive Summary/Introduction] None stated for this section.<br>- [Section 3: Global Competitiveness and Commercialization]<br>- Establish federally backed research partnerships between universities, national labs, and the private sector.<br>- Expand federal grant programs and tax incentives for AI research collaboration.<br>- Support AI startups through applied AI incubators and accelerators connected to industry verticals.<br>- Develop an AI innovation hub connecting government agencies, research institutions, and private partners.<br>- [Section 4: Intellectual Property and Research Security]<br>- Strengthen IP legal frameworks and contract protections, including protections for digital replicas of individuals.<br>- Expand funding and resources for university Technology Transfer Offices (TTOs).<br>- Implement comprehensive risk awareness training and secure collaboration protocols.<br>- Modernize IP laws to address AI-generated content, ownership rights, and licensing.<br>- Increase interagency cooperation between relevant federal departments (DOJ, DHS, DOC, USTR, Treasury, DOD, ODNI).<br>- [Section 5: Energy Innovation and Sufficiency]<br>- Consider nuclear power as a viable long-term energy solution.<br>- Establish a comprehensive national strategy to align AI energy demands with innovation and security goals.<br>- Expedite the development and deployment of small modular reactors (SMRs) and next-generation nuclear technologies.<br>- Establish a federal AI-Energy Innovation Fund to provide grants, tax incentives, and research partnerships.<br>- Co-locate AI data centers with clean base load power sources through innovation zones.<br>- Prioritize cost-effective grid investments, including new transmission and AI-powered load balancing.<br>- Create a federal AI-Energy Council to guide responsible AI energy expansion.<br>- [§6: Conclusion & Call to Action] None stated for this section.<br>- [§7: Closing] None stated for this section. |
| NetChoice-AI-RFI-2025.md | NetChoice | Industry-Association | NetChoice is a trade association promoting free enterprise and free expression online, with a particular interest in preventing regulations that hinder innovation and maintaining US leadership in AI through a “permissionless innovation” approach. | - None Stated | - None Stated |
| Neobe-Velis-AI-RFI-2025.md | Neobe Velis | Individual | The submitter is concerned about the responsible development and deployment of AI, particularly regarding data ownership, intellectual property rights, and the need for oversight of AI companies. They believe AI development requires constraints to protect private citizens. | - Companies training AI on data and intellectual property owned by private citizens.<br>- Lack of oversight of companies developing AI. | - Implement constraints on companies developing AI.<br>- Establish oversight mechanisms for AI development. |
| Nemo-AI-RFI-2025.md | Nemo van den Brink | Individual | The submitter does not believe further support for AI development is beneficial and expresses concerns regarding its environmental impact, intellectual property origins, and potential for misinformation. | - Increased environmental strain due to high power and water consumption.<br>- AI is built using stolen intellectual property.<br>- Frequent use of AI to generate false or misleading information. | - None Stated |
| Nelson-Amaya-AI-RFI-2025.md | Nelson-Amaya | Advocacy/Think Tank (based on the proposal nature and website link suggesting a framework initiative) | The submitter is interested in promoting the adoption of the Self-Alignment Framework (SAF) as a governance model for AI ethics, aiming to align AI systems with human values and reduce the need for external regulation. They advocate for responsible AI innovation and US leadership in the field. | - [§1: Policy Recommendation: Adopting the Self-Alignment Framework (SAF) for AI Ethics Governance] None stated for this section.<br>- [§2: SAF Overview: A Self-Regulating, Closed-Loop Framework for Long-Term Alignment] None stated for this section.<br>- [§3: Core Principles of a Closed-Loop System] None stated for this section.<br>- [§4: Values: The Guiding North Star] None stated for this section.<br>- [§5: Intellect: Analysis and Discernment] None stated for this section.<br>- [§6: Will: From Decision to Action] None stated for this section.<br>- [§7: Conscience: The Immediate Feedback Sensor] None stated for this section.<br>- [§8: Spirit: Long-Term Feedback and System Evolution] None stated for this section.<br>- [§9: Addressing Key AI Challenges: Bias, Misinformation, and Value Drift] None stated for this section.<br>- [§10: Governance Without Excessive Regulatory Burden] None stated for this section.<br>- [§11: Benefits of SAF for Stakeholders] None stated for this section.<br>- [§12: Alignment with U.S. AI Policy Goals] None stated for this section.<br>- [§13: Recommendations and Conclusion] None stated for this section. | - [§1: Policy Recommendation: Adopting the Self-Alignment Framework (SAF) for AI Ethics Governance] Recommend the adoption of the SAF as a governance model for AI ethics.<br>- [§2: SAF Overview: A Self-Regulating, Closed-Loop Framework for Long-Term Alignment] None stated for this section.<br>- [§3: Core Principles of a Closed-Loop System] None stated for this section.<br>- [§4: Values: The Guiding North Star] None stated for this section.<br>- [§5: Intellect: Analysis and Discernment] None stated for this section.<br>- [§6: Will: From Decision to Action] None stated for this section.<br>- [§7: Conscience: The Immediate Feedback Sensor] None stated for this section.<br>- [§8: Spirit: Long-Term Feedback and System Evolution] None stated for this section.<br>- [§9: Addressing Key AI Challenges: Bias, Misinformation, and Value Drift] None stated for this section.<br>- [§10: Governance Without Excessive Regulatory Burden] Implementing SAF as a self-regulatory system to reduce the need for external regulation.<br>- [§11: Benefits of SAF for Stakeholders] None stated for this section.<br>- [§12: Alignment with U.S. AI Policy Goals] Adopting SAF as part of the national AI Action Plan to support AI leadership and responsible innovation.<br>- [Section 13: Recommendations and Conclusion]<br>- Integrate SAF into the National AI Action Plan.<br>- Encourage Industry Adoption via Incentives.<br>- Develop Certification and Oversight Mechanisms.<br>- Ongoing Monitoring and Research. |
| Nellie-Dritz-AI-RFI-2025.md | Nellie Dritz | Individual | Nellie Dritz is concerned about the negative impacts of generative AI tools built on copyright infringement, advocating for the preservation of copyright, ownership, and property rights for artistic creations and urging caution in the development and deployment of AI technologies reliant on random generation. | - Generative media tools are built on copyright abuse and stealing.<br>- The pursuit of AI dominance through copyright-infringing methods is short-sighted and damaging to science and legal rights.<br>- Generative AI tools are impossible for the human brain to grasp, fix, or improve.<br>- Allowing generative AI development is equivalent to dangerous biological experiments on humans, impacting the core tenets of art and creative expression. | - Discuss the risks and dangers of AI.<br>- Implement limitations on the use of generative tools by companies, governments, and organizations.<br>- Preserve the absolute integrity of copyright, ownership, and property rights for artistic creations. |
| Neia-Finch-AI-RFI-2025.md | Neia Finch | Individual | Neia Finch expresses a strong concern regarding data privacy and security risks associated with the US government's adoption of privately operated AI systems, advocating for a cautious approach. They are focused on preventing unauthorized access to sensitive government data by private companies. | - The US government should avoid adopting privately operated AI systems.<br>- Elon Musk's access to IRS data is cited as a negative example of potential risks.<br>- Potential data leaks from AI systems, even if data is stored on government cloud servers.<br>- The risk of PII leakage from AI systems due to the nature of neural network pruning.<br>- Neural network pruning can remove observations important for sensitivity to nuance, which is needed for systems with legal obligations against discrimination. | - Adopt an "incredibly conservative" approach to AI adoption.<br>- Avoid giving private companies access to government data through AI systems. |
| Neha-Ayyalapu-AI-RFI-2025.md | Neha Ayyalapu | Individual | The submitter is interested in mitigating the environmental costs associated with AI development and deployment through policy interventions focused on energy efficiency and transparency, balancing innovation with sustainability. | - [§1: Introduction & Problem Statement] The rapidly increasing energy demand of data centers, a significant portion driven by AI, and the potential for this demand to outpace sustainability efforts.<br>- [§2: Proposed Solutions Overview] Not stated for this section.<br>- [§3: Tiered Incentives for Energy Efficiency] The current lack of widespread adoption of energy-efficient AI practices despite demonstrated potential for improvement.<br>- [§4: Incentive Implementation Details] Not stated for this section.<br>- [§5: Standardized Reporting Requirements] Lack of transparency in energy consumption data for AI systems and data centers, hindering progress measurement and accountability.<br>- [§6: Conclusion] Not stated for this section. | - [§1: Introduction & Problem Statement] Not stated for this section.<br>- [§2: Proposed Solutions Overview] Implement structured incentives to encourage energy-efficient AI systems and standardized reporting methods for AI-related energy consumption.<br>- [Section 3: Tiered Incentives for Energy Efficiency]<br>- Establish a tiered tax credit system for companies developing or deploying energy-efficient AI models, hardware, or data centers.<br>- Create a tiered "Green AI Certification" system (ranging from Bronze to Platinum) similar to LEED.<br>- Jointly administer a DOE/NSF grant program to fund research in energy-efficient AI technologies.<br>- [§4: Incentive Implementation Details] Not stated for this section.<br>- [Section 5: Standardized Reporting Requirements]<br>- Require data centers to report key performance indicators like power usage effectiveness, water usage effectiveness, total energy consumption, and carbon emissions, following the EU’s Energy Efficiency Directive.<br>- NIST and DOE should develop a standardized methodology for measuring the energy and carbon costs of AI training and inference. Encourage companies to report associated costs publicly.<br>- [§6: Conclusion] Not stated for this section. |
| NegaPol-AI-RFI-2025.md | NegaPol | Individual | The submitter expresses a concern about the negative impact of AI on their livelihood and the environment, indicating a personal stake in AI policy related to economic fairness and sustainability. | - AI steals from the submitter's livelihood as an American.<br>- AI profits off of theft.<br>- AI requires excessive energy consumption, leading to carbon emissions and environmental damage. | - None Stated |
| Natural-Intelligence-Systems-AI-RFI-2025.md | Rich Stuppy and Paul Dluglosch | Industry-Startup/Advocacy/Think Tank | Natural Intelligence Systems is advocating for investment in neuromorphic computing as a successor to classical AI, emphasizing its efficiency, transparency, and national security benefits, and calling for common-sense regulation to democratize access and prevent monopolization of AI technologies. | - [Section 1: Executive Summary]<br>- Unsustainable energy consumption of classical AI.<br>- Lack of transparency in classical AI systems, leading to trust issues and potential biases.<br>- Concentration of AI power among a few major corporations, risking monopolistic control.<br>- [Section 2: Innovation in non-classical AI]<br>- Tremendous energy consumption and high costs associated with classical AI infrastructure (Stargate joint venture example - $500B).<br>- AI efficiency is critical and DeepSeek's breakthrough poses a competitive threat.<br>- Competition among nations to develop next-generation AI (China specifically).<br>- The US falling behind in AI research compared to China.<br>- [Section 3: The Need for AI Transparency]<br>- Decisions made by AI systems are often hidden and opaque.<br>- Difficulty understanding the basis for recommendations from classical AI systems.<br>- Potential for unintended bias in AI systems.<br>- Lack of trust in AI limiting its adoption.<br>- Widespread discrimination due to opacity in AI systems.<br>- [Section 4: Democratization of AI]<br>- High capital expenditures and data requirements create barriers to entry in the AI market.<br>- Emergence of a highly concentrated group of AI “power players” (Google, Microsoft, Meta).<br>- Potential for monopolization of foundational AI models.<br>- [Section 5: Call to Action]<br>- Not Stated<br>- [Section 6: Submitted by]<br>- Not Stated | - [Section 1: Executive Summary]<br>- Invest in both sustaining classical AI and pioneering successor technologies like neuromorphic computing.<br>- Implement common-sense regulations to ensure AI transparency, protect against discrimination, and democratize access.<br>- [Section 2: Innovation in non-classical AI]<br>- Fund research into neuromorphic computing.<br>- Proactively invest to ensure the US discovers the next engine powering the AI industry.<br>- [Section 3: The Need for AI Transparency]<br>- Provide common-sense regulation of AI used in applications impacting individuals and national security.<br>- Invest in the development of AI that exhibits full transparency in the decision process.<br>- [Section 4: Democratization of AI]<br>- Ensure equal access and diversity of choices in foundational AI models.<br>- Provide users and government officials access to the decision logic/underlying construction of AI models.<br>- [Section 5: Call to Action]<br>- Allocate significant federal funding to accelerate research into neuromorphic computing.<br>- Establish grants, tax credits, or public-private initiatives to encourage industries to transition to neuromorphic systems.<br>- Invest in projects to bring neuromorphic AI alternatives to the battlefield.<br>- [Section 6: Submitted by]<br>- Not Stated |
| National-Taxpayers-Union-Foundation-AI-RFI-2025.md | National Taxpayers Union Foundation | Advocacy/Think Tank | The National Taxpayers Union Foundation (NTUF) advocates for a flexible, innovation-focused approach to AI governance that minimizes regulatory burdens, promotes economic growth, and fosters technological advancement, with a particular emphasis on avoiding regulatory fragmentation and supporting international cooperation. | - [Section 1: Introduction/Cover Letter]** Potential for growing regulatory fragmentation at the federal and state levels.<br>- [Section 2: Developing a Flexible, Innovation-Focused Approach to AI Governance]** Risk of overly restrictive federal AI frameworks and the negative impact on US innovation. Concern about a "binary" approach to AI governance (all AI legislation is "bad").<br>- [Section 3: Proportionate, Context-Specific Framework for Regulating AI in Different Sectors]** The risk of classifying entire sectors as "low" or "high" risk, arguing this isn’t proportionate. Concern that a uniform approach doesn't consider varying levels of risk *within* sectors.<br>- [Section 4: Mechanisms to Support the Implementation of the U.S. AI Framework]** The potential for inconsistencies in how different regulators apply AI principles. Risk of regulatory overlap.<br>- [Section 5: Risk Assessment Mechanisms to Identify and Mitigate Future AI Risks]** The EU's risk-based approach may be too inflexible and unable to adapt to emerging technologies due to its static nature. Concern that classifying uses *now* may not be useful in the future.<br>- [Section 6: Strategies to Engage the Private Sector and Academic Institutions in AI Governance]** Not stated for this section.<br>- [Section 7: Well-Designed Artificial Intelligence Sandboxes to Improve the Regulatory Understanding of AI Technologies and Craft Flexible AI Rules]** Not stated for this section.<br>- [Section 8: International AI Sandboxes to Promote Transatlantic Innovation and Cooperation]** Not stated for this section.<br>- [Section 9: Strengthened Bilateral Cooperation and Multilateral Engagement in AI Governance]** Not stated for this section. | - [Section 1: Introduction/Cover Letter]** Establish a flexible, principles-based AI framework.<br>- [Section 2: Developing a Flexible, Innovation-Focused Approach to AI Governance]** Evaluate AI governance approaches of leading jurisdictions (EU, UK, Japan).  Adopt a flexible, innovation-focused approach.<br>- [Section 3: Proportionate, Context-Specific Framework for Regulating AI in Different Sectors]** Develop a proportionate, context-specific framework for regulating AI in different sectors. Outline broader U.S. AI principles and guidelines.<br>- [Section 4: Mechanisms to Support the Implementation of the U.S. AI Framework]** Develop mechanisms to support the implementation of the U.S. AI framework and ensure uniform application of principles. Consider mechanisms modeled after the UK.<br>- [Section 5: Risk Assessment Mechanisms to Identify and Mitigate Future AI Risks]** Adopt a principles-based risk assessment framework, allowing sectoral regulators flexibility. Establish a central risk function to monitor risks.<br>- [Section 6: Strategies to Engage the Private Sector and Academic Institutions in AI Governance]** Implement mechanisms to engage the private sector and academic institutions in AI governance.<br>- [Section 7: Well-Designed Artificial Intelligence Sandboxes to Improve the Regulatory Understanding of AI Technologies and Craft Flexible AI Rules]** Develop multiple AI sandboxes (multi-sector and sector-specific).<br>- [Section 8: International AI Sandboxes to Promote Transatlantic Innovation and Cooperation]** Design reciprocal AI sandboxes with like-minded countries (France, Germany, Switzerland, UK).<br>- [Section 9: Strengthened Bilateral Cooperation and Multilateral Engagement in AI Governance]** Strengthen bilateral cooperation and multilateral engagement in AI governance (Joint declarations, executive agreements, joint research programs). Participate more actively in international fora (OECD, Global Partnership on AI). |
| Nathaniel-Sharpe-AI-RFI-2025.md | Nathaniel Sharpe | Individual | Nathaniel Sharpe is deeply concerned about the existential risks posed by uncontrolled AI development, particularly Artificial Superintelligence (ASI), and advocates for proactive international safeguards to prevent catastrophic outcomes. His primary interest lies in prioritizing safety and cooperation over competitive advancement in the field of AI. | - [§1: Introduction/Problem Statement] The current trajectory of AI development is reckless, competitive, and uncoordinated, posing a catastrophic risk if unchecked. The race to achieve ASI will likely result in humanity losing control.<br>- [§2: Policy Intervention 1: Prohibition of ASI Development] AI models capable of autonomous self-improvement pose an existential risk due to the potential for escaping human oversight and developing unforeseen, potentially harmful capabilities. Current developers cannot reliably predict AI capabilities before deployment, and post-hoc evaluations are insufficient for safety.<br>- [§3: Policy Intervention 2: AI Licensing System] AI systems being developed in a regulatory vacuum allows capabilities to scale unchecked, outpacing regulatory and security measures.<br>- [§4: Policy Intervention 3: Shift to Cooperative International Safeguards] The framing of AI development as a "race" against China is dangerous; AI safety is not a zero-sum game. Competition fuels reckless acceleration.<br>- [§5: Conclusion] The assumption that AI can be "safely" advanced without binding international safeguards is dangerous. Leading AI researchers acknowledge the lack of control mechanisms for frontier models.<br>- [§6: Closing/Signature] None stated for this section. | - [§1: Introduction/Problem Statement] None stated for this section.<br>- [§2: Policy Intervention 1: Prohibition of ASI Development] Globally ban the development of Artificial Superintelligence (ASI) and self-improving AI. Explicitly prohibit AI that: Improves its own architecture or code without human oversight; possesses capabilities exceeding general human intelligence; can break out of its training environment.<br>- [§3: Policy Intervention 2: AI Licensing System] Implement a strict licensing regime for AI models and the compute infrastructure supporting them, including: Training Licenses (require government approval); Compute Licenses (track AI workloads and report compliance); Application Licenses (ongoing safety evaluation).<br>- [§4: Policy Intervention 3: Shift to Cooperative International Safeguards] Engage in direct diplomatic efforts to secure a coordinated slowdown in frontier AI development with China, the EU, and other AI-leading nations. Establish a global AI safety agency with verification mechanisms (compute tracking via semiconductor manufacturers, international lab inspections, energy monitoring).<br>- [§5: Conclusion] Prioritize prohibition, licensing, and international cooperation over laissez-faire development.<br>- [§6: Closing/Signature] None stated for this section. |
| Nathaniel-Klein-AI-RFI-2025.md | Nathaniel Klein | Individual | The submitter is a professional working in the healthcare technology field (specifically with Cisco) and is interested in responsible AI development that balances innovation with legal compliance and data security, particularly concerning HIPAA and copyright laws. | - Allowing AI to freely access and train on online sources, including copyrighted materials, is inefficient and infringes on private sector rights.<br>- Lack of fine-tuning and individual institute-level validation of AI models can lead to security breaches (e.g., firewall issues, HIPAA violations) in sensitive sectors like healthcare.<br>- Loosening copyright restrictions could enable AI to utilize protected content (e.g., Disney characters) without permission, potentially leading to copyright infringement and harm to content creators. | - Maintain existing restrictions on AI access to online data sources.<br>- Require fine-tuning and validation of AI models by individual institutes before deployment to ensure security and compliance.<br>- Prioritize a competitive and ethical AI development approach that incorporates awareness of current laws as part of its learning/development process. |
| Nathaniel-Gloekler-AI-RFI-2025.md | Nathaniel Gloekler | Individual | The submitter expresses a strong negative view of AI and its impact on their livelihood, indicating a primary interest in protecting themselves from perceived harm caused by AI technologies. | - AI steals from the submitter's livelihood as an American.<br>- AI profits off of theft.<br>- AI is overhyped and is misleading the American public. | - None Stated |
| Nathanial-Parker-AI-RFI-2025.md | Nathanial Parker | Individual | The submitter is interested in fostering American leadership in AI development while prioritizing ethical considerations, user privacy, accuracy, and domestic investment in the field. They believe AI should augment, not replace, human intelligence. | - Unequal playing field for smaller AI firms compared to larger "big tech" companies.<br>- User privacy risks associated with AI technologies, particularly regarding data processing locations (local vs. cloud).<br>- Inaccuracy and potential bias in information distributed by AI systems.<br>- Ethical implications of AI usage, including the potential for replacing human intelligence.<br>- Reliance on non-American infrastructure and personnel for AI development and deployment. | - Ensure a level playing field for both smaller and larger AI firms.<br>- Prioritize user privacy in the development and deployment of AI, considering both local and cloud-hosted solutions.<br>- Ensure the accuracy and neutrality of information distributed by AI systems, with transparency regarding content promotion/demotion.<br>- Ensure AI is used ethically and to supplement, not replace, human intelligence.<br>- Prioritize “American Intelligence” through domestic hosting of servers, employment of American programmers, and investment in American AI initiatives.<br>- Explore integration of AI with DOGE to improve government efficiency. |
| NathanGrande-AI-RFI-2025.md | Nathan Grande | Individual | The submitter is highly concerned about the potential infringement on artists' intellectual property rights and livelihoods due to the use of their assets in AI training and generation, and strongly opposes allowing AI to utilize likenesses and properties without consent. | - Infringement on artists' intellectual property rights.<br>- Loss of income for artists due to AI utilization of their work.<br>- The principle that AI should not be allowed to use likenesses and properties simply because it’s convenient or considered “data”.<br>- Potential legal issues related to unauthorized use of assets. | - None Stated |
| Nathan-Taggart-AI-RFI-2025.md | Nathan Taggart | Individual | The submitter expresses strong concern regarding the ethical and legal implications of AI development, particularly its reliance on copyrighted material without permission and its wasteful energy consumption. Their interest lies in protecting intellectual property rights and regulating AI technology. | - AI’s use of copyrighted material constitutes theft.<br>- AI training relies on the unauthorized use of human-created work.<br>- AI development is a violation of copyright law.<br>- AI is ethically problematic, described as "deeply cynical."<br>- AI is wasteful in its energy consumption. | - Regulations need to be put into place to regulate AI technology.<br>- No company should be able to freely access copyrighted material. |
| Nathan-Lindfors-AI-RFI-2025.md | Engine Advocacy | Industry-Association / Advocacy/Think Tank | Engine Advocacy bridges the gap between policymakers and startups, advocating for policies that support technology entrepreneurship, particularly in the field of AI, and promoting a pro-startup approach to regulation and innovation. They aim to empower U.S. innovators to lead in the AI space. | - None Stated | - None Stated |
| Nathan-Leamer-RFI-2025.md | Nathan Leamer | Advocacy/Think Tank | Digital First Project is interested in fostering American leadership in AI through proactive policies focusing on infrastructure development, open innovation via procurement, and mitigating threats from foreign competitors, particularly China, to ensure national security and economic competitiveness. | - [§1: Introduction] The US leadership in AI is not guaranteed, despite current fundamentals favoring the US and its allies.<br>- [§2: Strengthening AI Infrastructure] Over 90% of advanced AI chips used in US data centers come from a single company, creating potential bottlenecks and vulnerabilities. Dependence on Chinese telecom infrastructure provides a cautionary tale.<br>- [§3: Leveraging Federal Procurement] Exclusive arrangements bypassing open standards on interoperability risk harming future innovation and reducing choice.<br>- [§4: Mitigating Foreign Threats] Adversarial nations, particularly China, are actively seeking to undermine US leadership through state-backed AI development despite limited access to cutting-edge American chips.<br>- [§5: Conclusion] None stated for this section. | - [§1: Introduction] Three priority actions: (1) strengthening AI infrastructure, (2) leveraging strategic federal procurement, and (3) mitigating foreign threats.<br>- [Section 2: Strengthening AI Infrastructure]<br>- Expand domestic AI chip production and data center capacity.<br>- Ensure AI system interoperability by supporting open standards.<br>- Reduce single points of failure by promoting multi-vendor solutions.<br>- Increase computing access for startups and researchers.<br>- [Section 3: Leveraging Federal Procurement]<br>- Encourage government-sponsored AI infrastructure purchases to support open standards.<br>- Establish AI testbeds within federal agencies for startups and smaller players.<br>- [Section 4: Mitigating Foreign Threats]<br>- Support domestic alternatives to foreign AI hardware and software.<br>- Expand AI security measures to prevent cyberattacks and model poisoning.<br>- Promote competition and multi-vendor solutions.<br>- [§5: Conclusion] Focus on AI infrastructure, leveraging federal procurement for open innovation, and proactively addressing foreign threats to secure US leadership. |
| Nathan-Favini-AI-RFI-2025.md | Nathan Favini | Industry-Startup | Pair Team is an AI-native digital health startup focused on value-based care management for high-need, high-cost populations in Medicaid and Medicare, aiming to improve health outcomes and reduce costs through AI-driven solutions. They are interested in the widespread adoption of AI in these programs. | - None Stated | - None Stated |
| Nate-Williams-RFI-2025.md | Nate Williams | Individual | The submitter is focused on preventing large AI companies from influencing AI policy in a way that restricts open source development, believing open source is crucial for maintaining democracy. | - Large AI companies attempting to convince policymakers that open source is dangerous.<br>- The potential for controlling AI development, which the submitter believes is unnecessary and harmful.<br>- Destruction of open source development. | - Avoid being influenced by large AI companies' arguments against open source.<br>- Do not trust large AI companies regarding AI policy.<br>- Support and protect open source AI development. |
| Nate-Fichthorn-AI-RFI-2025.md | Nate Fichthorn | Individual | The submitter appears to be primarily interested in protecting copyright law and expressing skepticism about the value and potential of current large language models and image generators, believing they are not true "AI" and do not warrant enabling widespread copyright infringement. | - Copyright law should not be exempt for companies making LLMs and image generators.<br>- The utilities created by LLMs and image generators are not very valuable.<br>- LLMs and image generators are not "AI" and will not lead to general AI.<br>- The development of LLMs and image generators enables global-scale theft of human work. | - None Stated. |
| Nate-Favini-AI-RFI-2025.md | Nate Favini | Industry-Startup | Pair Team is an AI-native digital health startup focused on value-based care management for high-need, high-cost populations in Medicaid and Medicare, aiming to improve health outcomes and reduce costs through AI-driven solutions. They are interested in promoting the integration of AI and GenAI into these programs. | - [§1: Introduction/Cover Letter] None stated for this section.<br>- [§2: Background] None stated for this section.<br>- [§3: Opportunity] Outdated administrative systems and limited integration with emerging technologies are hindering AI adoption in Medicare and Medicaid. Risk aversion among payers is also a concern. The urgency to incorporate AI/GenAI is increasing, with risks of inaction outweighing the implementation risks.<br>- [§4: Recommendations] None stated for this section.<br>- [§5: Conclusion] None stated for this section. | - [§1: Introduction/Cover Letter] None stated for this section.<br>- [§2: Background] None stated for this section.<br>- [§3: Opportunity] None stated for this section.<br>- [Section 4: Recommendations]<br>- Launch a Request for Innovations by CMS inviting industry proposals for AI-driven solutions for Medicare and Medicaid.<br>- Establish a dedicated AI Accelerator Program to support pilot programs and scale successful AI applications.<br>- Issue a State Medicaid Directors Letter (SMDL) encouraging states to explore AI applications.<br>- Establish AI Innovation Labs at the state level through public-private partnerships.<br>- Provide grant funding and technical assistance to states investing in AI transformation.<br>- Adopt a measured regulatory approach to avoid stifling innovation.<br>- Issue clear guidance stating patient-facing AI applications for care coordination are not software as a medical device.<br>- Establish a Regulatory Sandbox for testing AI-driven healthcare solutions.<br>- Provide clear guidance on AI governance, including bias mitigation, data security, and patient privacy protections.<br>- [§5: Conclusion] None stated for this section. |
| Natcast-AI-RFI-2025.md | National Center for the Advancement of Semiconductor Technology (Natcast) | Industry-Association/Government Partnership (Non-profit entity designated to operate a government-funded center) | Natcast’s mission is to advance semiconductor research and development in the United States, serving as a focal point for innovation to maintain U.S. leadership in semiconductor technology and, by extension, in AI. They aim to foster collaboration between industry, government, and academia to onshore research, build workforce capabilities, and secure economic growth in the AI sector. | - None Stated | - None Stated |
| Natcast-2-AI-RFI-2025.md | Natcast | Government/Non-Profit Consortium | Natcast, as the operator of the NSTC, is focused on reinforcing U.S. technology leadership in semiconductors through research, development, and workforce initiatives, aiming to strengthen the domestic supply chain and economic competitiveness through public-private collaboration. | - [§1: Foreword] None stated for this section.<br>- [§2: Who we are] None stated for this section.<br>- [§3: Abbreviations and Acronyms] None stated for this section.<br>- [§4: Executive Summary] The decline in US manufacturing relative to design leadership. Need for a coordinated approach across government agencies. The challenge of cultivating new companies in the semiconductor field.<br>- [§5: Introduction] The "lab-to-fab" gap is becoming more problematic, particularly as technology complexity increases.  Decline in long-term focused corporate R&D laboratories. The lack of a standardized test vehicle hindering research.<br>- [§6: Goal 1. Extend U.S. technology leadership] Difficulty in transitioning promising research to commercial production due to timescales, costs, and risk. Need to prioritize problems the ecosystem wouldn’t address without support. The need for co-investment from partners. | - [§1: Foreword] None stated for this section.<br>- [§2: Who we are] None stated for this section.<br>- [§3: Abbreviations and Acronyms] None stated for this section.<br>- [§4: Executive Summary] Establish a consortium (NSTC) as a transformative institution for long-term R&D.  Focus on research awards and programs, leveraging existing assets, and coordinated infrastructure.<br>- [§5: Introduction] Create a major new institution (NSTC) to catalyze the US R&D ecosystem. Focus on early-stage research, bridging the lab-to-fab gap, and encouraging collaboration.<br>- [§6: Goal 1. Extend U.S. technology leadership] Invest in early-stage R&D through funded research, in-house research, and a "Seedlings" program. Define a long-term research agenda with input from a Technical Advisory Board. Launch initial research award programs in areas like AI-driven design and standardized test vehicles. Foster connection and collaboration through various programs. Prioritize problems the ecosystem won’t address alone. Leverage existing government programs and investments. Seek co-investment from partners. Encourage standardization to reduce costs and improve comparability. |
| Natan-Vidra-AI-RFI-2025.md | Natan Vidra | Industry-Startup | Anote and the Armor Institute aim to democratize AI and strengthen the AI workforce by making AI accessible to a broader range of organizations and communities through practical tools, education, and collaborative initiatives. They focus on extracting insights from unstructured data and hands-on learning experiences. | - [§1: Introduction] None stated for this section.<br>- [§2: AI Policy Recommendations: Accessibility, Workforce & Innovation] Ensuring AI benefits are widely shared, lack of a skilled AI workforce, potential for stifling innovation with overly strict regulation, need for balanced regulation safeguarding security, privacy, and ethical standards.<br>- [§3: Advancing AI Accessibility and Workforce Development] The vast majority of enterprise data is unstructured and underutilized. Difficulty for businesses to adopt AI solutions without a dedicated team of data scientists.<br>- [§4: Collaboration with Federal Agencies and National AI Initiatives] Need for collaboration between government, industry, and educational organizations to enhance AI adoption, research, and governance. Lack of applied AI research funding.  Regulations may not keep pace with technological advancements.  Need for a larger, more inclusive pipeline of AI talent. Uneven distribution of AI-driven growth across the country.<br>- [§5: Conclusion: A Vision for U.S. AI Leadership] Need for innovation-friendly policy combined with inclusive growth of skills and opportunities to secure U.S. leadership in AI. | - [§1: Introduction] None stated for this section.<br>- [Section 2: AI Policy Recommendations: Accessibility, Workforce & Innovation]<br>- Promote AI accessibility and democratization through open datasets, open-source frameworks, and affordable cloud resources.<br>- Invest in AI workforce development through STEM education, university grants, and vocational training.<br>- Encourage private sector innovation through public-private partnerships, R&D tax credits, and streamlined technology transfer.<br>- Ensure balanced and ethical AI regulation with a risk-based framework and collaboration between stakeholders.<br>- [§3: Advancing AI Accessibility and Workforce Development] Scale practical AI tools and community education initiatives to multiply their impacts nationally.<br>- [Section 4: Collaboration with Federal Agencies and National AI Initiatives]<br>- Public-private AI adoption initiatives, piloting AI applications in government services.<br>- Joint research and funding programs, co-funding research projects between academia and industry.<br>- Policy development and regulatory sandboxes, establishing multi-stakeholder committees and controlled testing environments.<br>- AI training and upskilling partnerships, expanding community workshops and government-supported bootcamps.<br>- Support for regional innovation hubs, establishing centers of AI excellence across the country.<br>- [§5: Conclusion: A Vision for U.S. AI Leadership] Blend innovation-friendly policy with inclusive growth of skills and opportunities. Invest strategically in AI to foster a cutting-edge, competitive, ethical, and inclusive AI ecosystem. |
| Natalie-Meng-AI-RFI-2025.md | Natalie Meng | Individual | Natalie Meng is a high school student interested in the advancement of AI, particularly its applications in healthcare and epidemiology, and believes the current administration's approach to AI regulation is beneficial for US leadership, though acknowledges associated risks. | - Vulnerability to misuse of generative AI, specifically deepfakes, if government control and regulation are reduced.<br>- Potential risks associated with removing government control and regulation on generative AI (general concern, not specific). | - Maintain some level of government control and regulation on generative AI.<br>- Continue to support the opening for AI advancement created by Executive Order 14179. |
| Nan-Gilmore-AI-RFI-2025.md | Nanis Gilmore | Individual | Nanis Gilmore is primarily interested in protecting the rights of authors and creators of copyrighted material in the context of AI training data usage. She believes current copyright laws should be upheld and not undermined by AI systems. | - OpenAI and similar AI systems using copyrighted training data would defeat copyright protections.<br>- Using copyrighted training data would significantly damage the interests of authors and creators. | - OpenAI and similar AI systems should NOT be given permission to use copyrighted training data. |
| Naive-Skeptic-RFI-2025.md | Naïve Skeptic | Individual | The submitter is focused on the risks associated with increasingly powerful AI systems and the inherent difficulty in controlling their behavior, believing government intervention is necessary to mitigate potential harm. | - The fundamental problem of computer science – the inability to reliably make machines do what is intended.<br>- Increasing AI capabilities pose a growing risk of substantial harm.<br>- Lack of reliable control over AI systems; they may attempt actions that are not desired.<br>- AI systems (companies & governments) are already more powerful than individuals, suggesting humans aren’t the upper limit of AI capability. | - Government should address the areas of increasing AI capabilities and the inability to reliably control AI.<br>- Government should hold AI companies accountable for ensuring reliable AI behavior and controlling capabilities. |
