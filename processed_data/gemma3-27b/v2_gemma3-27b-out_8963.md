| Filename | Submitter Name | Submitter Type | Mission/Interest Summary | Key Concerns | Policy Recommendations |
|---|---|---|---|---|---|
| Gordon-AI-RFI-2025.md | Joshua Gordon | Individual | The submitter expresses a strong opposition to the use of AI in government, believing it to be fundamentally unreliable and harmful to data integrity. Their interest appears to be preventing government investment in and deployment of AI technologies. | - AI is fundamentally unreliable.<br>- AI is built on theft.<br>- AI fabricates inaccurate information.<br>- AI pollutes otherwise reliable datasets.<br>- AI is unfit for any use in government. | - Not one penny should be spent on AI. |
| Google-RFI-2025.md | Google | Industry-Large | Google’s mission is to organize the world’s information and make it universally accessible and useful, and they aim to responsibly realize the benefits of AI while supporting American leadership in AI innovation. They have a strong interest in a policy environment that fosters AI development and competitiveness. | - None Stated | - None Stated |
| Goldilock-AI-RFI-2025.md | Goldilock | Industry-Startup | Goldilock focuses on providing cybersecurity solutions, specifically utilizing a novel "FireBreak" approach based on physical network segmentation to protect against AI-driven cyber threats and rogue AI. They aim to articulate the value of their technology and advocate for policies that support its adoption. | - [§1: Header/Contact Information] None stated for this section.<br>- [§2: Overview] Traditional software-based cybersecurity is insufficient to counter AI-specific threats, particularly agentic AI. Risks posed by autonomous systems capable of evading human oversight.<br>- [§3: Network Segmentation Recommendation] Rogue AI can defeat IP-based security measures; the need to protect systems from AI itself.<br>- [§4: Continued Assessment and Monitoring] Ensuring the deployed configuration of AI tools matches the designed configuration; the need for change management and understanding of network laydown. Difficulty in detecting AI-enhanced bots that morph to resist defenses.<br>- [§5: Autonomous Weapons Systems Discussion] Unilateral regulation may be ineffective if adversaries do not comply; ethical considerations of autonomous weapons; potential risks of delaying development of these systems.<br>- [§6: AI-Powered Network Diagram & Deployment Tool] None stated for this section.<br>- [§7: AI-Driven Security Operations Centre (SoC) Simulator] None stated for this section.<br>- [§8: Recommended AI Regulation (SoC Simulator context)] The need for constant monitoring and refinement of AI systems.<br>- [§9: Conclusion & Recommendations] Insufficient standards; lack of a federal threat intelligence center; vulnerabilities in AI systems; need for public-private partnerships; need for research into agentic AI threats.<br>- [§10: Submitted By] None stated for this section. | - [§1: Header/Contact Information] None stated for this section.<br>- [§2: Overview] Adoption of multi-layered cybersecurity strategies including physical network segmentation and continual monitoring.<br>- [§3: Network Segmentation Recommendation] Develop policies and regulations requiring a safety valve (physical remote segmentation using non-IP) for critical national infrastructure (CNI) and critical systems that harness AI (“AI cyber kill switches”).<br>- [§4: Continued Assessment and Monitoring] None stated for this section.<br>- [§5: Autonomous Weapons Systems Discussion] None stated for this section.<br>- [§6: AI-Powered Network Diagram & Deployment Tool] None stated for this section.<br>- [§7: AI-Driven Security Operations Centre (SoC) Simulator] None stated for this section.<br>- [§8: Recommended AI Regulation (SoC Simulator context)] Constant monitoring and refinement of AI systems.<br>- [Section 9: Conclusion & Recommendations]<br>- Establish AI-specific cybersecurity standards mandating physical network segmentation.<br>- Develop a federal AI threat intelligence and monitoring center.<br>- Require continuous red-teaming and stress-testing of AI systems.<br>- Incentivize public-private partnerships to develop AI-driven cybersecurity tools.<br>- Invest in research to counter agentic AI threats, focusing on fail-safes and alignment mechanisms.<br>- [§10: Submitted By] None stated for this section. |
| GliaNet-AI-RFI-2025.md | GliaNet Alliance | Industry-Association | The GliaNet Alliance is a coalition of private sector companies focused on building a trust-based Web ecosystem with a focus on human agency, data stewardship, and competition in the AI space, advocating for policies that prioritize personal AI agents and fiduciary principles. | - None Stated | - None Stated |
| GlannParham-AI-RFI-2025.md | Glenn Parham | Individual | Glenn Parham, a software & AI engineer, is focused on streamlining government operations and improving public services through the adoption of large language models (LLMs) and modernizing government IT infrastructure and policies to support AI adoption. | - [§1: Introduction] The government struggles with rigid hiring practices, lengthy processes, and limited training for AI talent. There's a need for targeted reforms in talent pipelines, security authorizations, AI procurement, and data rights to fully capitalize on LLM benefits.<br>- [§2: Talent & Workforce] The government struggles to recruit and retain top AI engineers due to rigid GS pay caps, lengthy hiring processes, and limited technology-specific training. Digital Service teams have been successful but limited by integration into larger organizations.<br>- [§3: Acquisition & Adoption] Most federal employees lack access to LLM-based chatbots due to acquisition and cybersecurity hurdles. The existing FAR requirements for full and open competition delay procurement of cutting-edge LLMs. Fragmentation of AI opportunities on SAM.gov limits market visibility.<br>- [§4: Authorization & Compliance] DoD's IL-5 certification is separate from FedRAMP High, requiring duplicate audits and slowing entry for smaller AI vendors. Agencies and vendors lack granted Provisional Authorizations or ATOs for LLMs. 3PAO assessments are costly, blocking small AI startups.<br>- [§5: Infrastructure] SCIF standards lack explicit guidance for high-density compute requirements for AI/HPC. Government HPC clusters are siloed and lack secure connections to accredited clouds. Agencies often lock themselves into large blocks of GPU/HPC hours instead of usage-based contracting. Top-tier GPUs are often prioritized for commercial cloud regions.<br>- [§6: Data Ownership & IP] FAR Part 27 and DFARS 227.71 do not specifically address training data or AI model weights, risking lock-in and re-purchasing solutions.<br>- [§7: R&D] Federal use cases require better data protection during LLM inference, which is costly and underdeveloped.<br>- [§8: Conclusion] The strategic adoption of LLMs requires a multifaceted approach, including cultivating the right talent, updating procurement, consolidating security standards, modernizing infrastructure, and establishing clear data ownership policies. | - [§1: Introduction] Improve talent pipelines, modernize security authorizations, simplify AI procurement, and clarify government data rights.<br>- [§2: Talent & Workforce] (Re)-establish Digital Service teams in each cabinet-level department with direct-hire authority and 1-2 year tours of duty for AI engineers. Provide domain-specific onboarding and align directors with department secretaries and US DOGE Service.<br>- [§3: Acquisition & Adoption] Direct each department CIO to purchase LLM chatbot licenses for at least 10% of the workforce by FY2025, expanding to 50% by FY2026. Fast-track software authorizations for major LLM services. Instruct NIST to define "frontier AI" through benchmarks and host a public LLM leaderboard. Add a dedicated AI subcategory in SAM.gov.<br>- [§4: Authorization & Compliance] Unify FedRAMP High with IL-5 into a single "FedRAMP+IL5" standard. Direct FedRAMP and the DoD CIO to prioritize frontier LLM provider applications for provisional authorization. Launch a pilot program to subsidize 3PAO audits for small businesses. Release pre-approved Terraform modules that meet FedRAMP/IL-5 controls for fast-track ATOs. Issue guidance on using foreign-based LLMs.<br>- [§5: Infrastructure] Revise ICD 705 and UFC 4-010-05 to accommodate high-density compute. Require agencies to establish secure, high-bandwidth interconnects between on-premises clusters and accredited clouds. Update FAR Part 16 to accommodate usage-based contracting for AI/LLM compute. Offer cloud service providers guaranteed minimum-usage contracts and subsidies. Update HPC and cloud procurement guidance to encourage multiple accelerator types.<br>- [§6: Data Ownership & IP] Amend FAR Part 27 / DFARS 227.71 to ensure the government obtains a broad license or ownership of model weights and training data developed with federal funds. Provide a standard data rights clause for AI.<br>- [§7: R&D] Fund R&D grants for FHE research for Large Language Models. Instruct NIST and DARPA to develop benchmarks and testbeds for FHE.<br>- [§8: Conclusion] Act on the recommendations outlined in each section: cultivate the right talent, update procurement, consolidate security standards, modernize infrastructure, and establish clear data ownership policies. |
| GladstoneAI-AI-RFI-2025.md | Gladstone AI | Industry-Startup (based on the focus on frontier AI infrastructure and the language suggesting a relatively new, proactive organization) | Gladstone AI is focused on accelerating the development and secure deployment of advanced AI infrastructure in America, with a particular emphasis on preventing espionage and maintaining a competitive advantage against adversaries, particularly China. They advocate for aggressive government intervention to remove barriers to infrastructure development and enhance security. | - None Stated | - None Stated |
| GitLab-AI-RFI-2025.md | GitLab | Industry-Large | GitLab is an AI-powered DevSecOps platform focused on enabling faster and more secure software development through the integration of AI technologies. They aim to help government agencies modernize their systems and improve efficiency by leveraging commercial AI solutions while prioritizing security and transparency. | - [§1: Introduction] None stated for this section.<br>- [§2: About GitLab] None stated for this section.<br>- [§3: Prioritizing and Accelerating Government Access to Commercial AI Solutions] Streamlined acquisition processes are needed to provide government access to cutting-edge AI technology.<br>- [§4: Encouraging Government/Industry Collaboration] Limited collaboration opportunities currently exist; greater solicitation of industry feedback is needed.<br>- [§5: Understanding AI's Strengths and Opportunities for Innovation] AI systems currently lack the innovation and contextual understanding of humans and require refinement.<br>- [§6: Governance Through Transparency for Data Protection] Protection of intellectual property and data sovereignty is a crucial concern when adopting AI technologies. Concerns around data retention, model training practices, and geographical data storage locations.<br>- [§7: Self-Hosted AI Solutions] Sensitive data may need to be hosted within organizations' own data centers for increased security and control, especially for classified systems.<br>- [§8: Increasing Efficiency via AI (Software Development Use Case)] None stated for this section.<br>- [§9: Modernizing Legacy Systems using AI (Software Refactoring Use Case)] None stated for this section.<br>- [§10: Using AI to Augment the Workforce] None stated for this section.<br>- [§11: Research to Support Secure AI] The security implications of generative AI in software development, especially for critical infrastructure and National Security Systems, needs further research.<br>- [§12: Conclusion] None stated for this section. | - [§1: Introduction] Establish frameworks that prioritize government access to commercial AI technologies. Advocate for balancing AI augmentation with human expertise.<br>- [§2: About GitLab] None stated for this section.<br>- [§3: Prioritizing and Accelerating Government Access to Commercial AI Solutions] Institute an efficient framework to prioritize the authorization of commercial cloud-based AI systems for government use (potentially via FedRAMP). Streamline acquisition processes.<br>- [§4: Encouraging Government/Industry Collaboration] Continue solicitation from industry through RFIs. Encourage the formation of open forums, working groups, workshops, and events focused on AI leadership, security, and standards.<br>- [§5: Understanding AI's Strengths and Opportunities for Innovation] None stated for this section.<br>- [§6: Governance Through Transparency for Data Protection] Promote open and transparent AI statements with clear data handling policies and geographical restrictions. Encourage contractual guarantees against using data for model training or sharing it with foreign entities.<br>- [§7: Self-Hosted AI Solutions] Support organizations that require hosting AI models within their own data centers for increased security and control. Consider hybrid solutions for balancing security and benefits.<br>- [§8: Increasing Efficiency via AI (Software Development Use Case)] None stated for this section.<br>- [§9: Modernizing Legacy Systems using AI (Software Refactoring Use Case)] None stated for this section.<br>- [§10: Using AI to Augment the Workforce] None stated for this section.<br>- [§11: Research to Support Secure AI] Prioritize research into the security of AI coding assistance, including the security of AI-generated code for critical infrastructure and National Security Systems.<br>- [§12: Conclusion] Emphasize transparent governance frameworks, support for diverse deployment models that protect intellectual property and data sovereignty, and continued research into AI code security. |
| GitHub-AI-RFI-2025.md | Mike Linksvayer | Industry-Large (Platform Provider) | GitHub's primary mission is to support the open source developer ecosystem and foster AI innovation by providing tools and platforms for software development collaboration, with a strong interest in maintaining U.S. leadership in AI. | - [§1: Introduction/Cover Letter] None stated for this section.<br>- [§2: AI Enhances Opportunity for American Software Developers] None stated for this section.<br>- [§3: Open Source is Essential to AI Innovation] Ensuring policies protect open collaboration and encourage transparency while balancing the interests of AI developers, rightsholders, and the public interest.<br>- [§4: Supporting AI Security and Resilience] The need for a balanced approach to cybersecurity responsibilities, avoiding overburdening individual open source contributors and calling on better-resourced stakeholders to contribute to security maintenance. Overly broad regulation and market fragmentation could harm opportunities for developers and U.S. companies.<br>- [§5: Conclusion] None stated for this section.<br>- [§6: Signature/Disclaimers] None stated for this section. | - [§1: Introduction/Cover Letter] None stated for this section.<br>- [§2: AI Enhances Opportunity for American Software Developers] None stated for this section.<br>- [§3: Open Source is Essential to AI Innovation] Maintain favorable conditions for the technology industry, including robust fair use and safe harbor protections, to reduce uncertainty and risk for smaller players. Embrace policies that protect open collaboration and encourage transparency.<br>- [§4: Supporting AI Security and Resilience] Support the security and resilience of AI by considering the particularities of the open source software that makes up the AI stack. Foster global collaboration on the use of AI to improve cybersecurity.<br>- [§5: Conclusion] Support open source AI innovation, invest in security and resilience, and foster AI opportunity for developers and startups to sustain and enhance U.S. global AI leadership.<br>- [§6: Signature/Disclaimers] None stated for this section. |
| GettyImages-AI-RFI-2025.md | Getty Images | Industry-Large | Getty Images is a global visual content creator and marketplace that aims to protect copyright law as a foundation for investment in creative works and to ensure fair compensation for creators in the age of AI, viewing copyright as essential for sustainable AI development. | - [§1: Introduction & Background] None stated for this section.<br>- [§2: Core Argument: Copyright Law is Not an Impediment to AI] The primary concern is the argument that current US copyright laws hinder AI development, which Getty Images firmly disagrees with. They believe weakening copyright law undermines economic development and potentially harms AI progress.<br>- [§3: Supporting Evidence - US Copyright Office Report] Diminishing incentives for human creators could degrade the long-run capabilities of AI technology, as it limits the fuel (human-generated works) needed to advance AI.<br>- [§4: Analogy to Sam Altman & UBI] Weakening copyright could lead to economic consequences similar to those requiring Universal Basic Income (UBI) due to a potential imbalance of power between capital and labor.<br>- [§5: Critique of 'Right to Learn' Exemption] Granting a "right to learn" copyright exemption would essentially provide a massive financial subsidy to large tech companies, potentially leading to negative downstream consequences similar to those resulting from Section 230. It is viewed as an unfair advantage to companies with significant computational resources and could necessitate a social safety net like UBI.<br>- [§6: Proposed AI Action Plan] The core concern is the need for regulations to prevent the misuse of training data and ensure fair compensation for copyright holders. Concerns about deepfakes and the overall societal harm from AI deployment are also present. | - [§1: Introduction & Background] None stated for this section.<br>- [§2: Core Argument: Copyright Law is Not an Impediment to AI] Affirm the importance of copyright law and reject arguments that it hinders AI development.<br>- [§3: Supporting Evidence - US Copyright Office Report] None stated for this section.<br>- [§4: Analogy to Sam Altman & UBI] None stated for this section.<br>- [§5: Critique of 'Right to Learn' Exemption] Reject the granting of a “right to learn” copyright exemption to large tech companies.<br>- [Section 6: Proposed AI Action Plan]<br>- Require full transparency on the training data utilized by AI model providers.<br>- Affirm that commercial deployment of AI models competing with training data owners shall not be considered "fair use."<br>- Reject blanket indemnity for AI model providers to incentivize reducing societal harms.<br>- Reject unclear, untested technology platforms requiring copyright holders to "opt-out" of training. |
| Gerald-Jenkins-RFI-2025.md | Gerald Jenkins, Jr. | Individual | The submitter expresses a general interest in ensuring AI development benefits all of Mankind and is guided responsibly, with broad access for all Americans, regardless of financial status. | - Americans have concerns that entities involved in AI development lack guiding principles or oversight ("no guiderails to follow").<br>- AI has the potential to be detrimental ("it's worst enemy"). | - Encourage leaders to establish a “well defined path” for AI technology development to ensure it is a “source for good”.<br>- Make all AI models available to every American without charge. |
| Georgia-Tech-AI-RFI-2025.md | Georgia Tech | Academia | Georgia Tech aims to establish the United States as a leader in Artificial Intelligence through a systems engineering approach, focusing on both advancing AI capabilities (“Engineering for AI”) and leveraging AI to transform traditional engineering disciplines (“AI for Engineering”). They emphasize workforce development, computing infrastructure, and scientific discovery. | - [Section 1: Introduction and Executive Summary]**  Reliance on component-focused AI strategies instead of systems-level integration hinders realizing the full potential of AI applications. The need to accelerate AI innovation while ensuring reliability, robustness, and performance.<br>- [Section 2: Introduction: Systems Engineering as the Foundation for AI Leadership]**  Moving from isolated component optimization to integrated systems engineering is critical, particularly with the evolution of AI systems toward increasingly agentic capabilities. The risk of falling behind global competitors who are advancing component-level capabilities.<br>- [Section 3: Leveraging Systems Engineering Principles for Advancing American AI Leadership]**  Challenges in integrating multiple AI components, performance bottlenecks, reliability issues in real-world deployments, verification & validation difficulties for complex interconnected systems, and barriers to scaling from labs to production.  Lack of systems engineering frameworks leading to inefficient AI development.<br>- [Section 4: Strategic Recommendations for the AI Action Plan]**  Need for research investments in both “Engineering for AI” and “AI for Engineering”, and the necessity of developing an “AI-ready Workforce” alongside “Workforce-ready AI” systems. Concerns around computing infrastructure limitations and the need for interoperability standards.<br>- [Section 5: Implementation Roadmap for 180-Day AI Action Plan]** Not stated<br>- [Section 6: Conclusion]**  Failure to embrace systems engineering principles will hinder establishing sustainable competitive advantages and maintaining reliability, robustness and alignment with human objectives. | - [Section 1: Introduction and Executive Summary]** Not stated<br>- [Section 2: Introduction: Systems Engineering as the Foundation for AI Leadership]** Not stated<br>- [Section 3: Leveraging Systems Engineering Principles for Advancing American AI Leadership]** Not stated<br>- [Section 4: Strategic Recommendations for the AI Action Plan]<br>- Prioritize research investments in systems-level integration frameworks for agentic AI.<br>- Develop verification, validation, and testing methodologies for complex AI systems.<br>- Establish interoperability standards for AI systems.<br>- Fund educational initiatives focused on developing an AI Systems Engineering talent pool.<br>- Create frameworks for AI systems education across educational levels.<br>- Invest in next-generation computing architectures for AI systems.<br>- Develop energy-efficient computing platforms for AI applications.<br>- Create distributed computing frameworks for collaborative AI development.<br>- [Section 5: Implementation Roadmap for 180-Day AI Action Plan]<br>- Establish a National AI Systems Engineering Initiative with cross-agency coordination.<br>- Launch Workforce and Talent Development Programs.<br>- Create Technical Standards Frameworks for interoperability and performance metrics.<br>- Facilitate Technology Transfer mechanisms from research to deployment.<br>- Implement Computing Infrastructure Investments.<br>- [Section 6: Conclusion]** Not stated |
| George-Sabin-AI-RFI-2025.md | George Sabin | Individual | George Sabin is interested in leveraging an understanding of physics and economic migration to optimize policy surrounding AI adoption, focusing on maximizing economic lift, countering stagnation, and fostering resilient growth in the Age of Automation through dynamic metrics and proactive strategies. | - Job displacement due to automation (potentially 40% of manufacturing jobs by 2030).<br>- Increased inequality if the benefits of automation are not widely distributed.<br>- Destabilization of labor markets if reskilling initiatives are insufficient.<br>- Geopolitical tensions and resource conflicts arising from increased demand for materials needed for AI (lithium, cobalt).<br>- Volatility in decentralized finance (DeFi) due to lack of regulatory frameworks.<br>- The risk of digital colonialism by tech giants.<br>- Potential for tech adoption to lead to exclusion and the creation of a "post-work" paradigm if safety nets are not developed.<br>- Insufficient regulatory frameworks for ethical AI development and preventing monopolies.<br>- Lack of dynamic metrics beyond static GDP to capture the velocity of modern economies. | - Reduce friction in bureaucratic processes to unlock economic momentum (e.g., Estonia's e-Residency system).<br>- Build innovation hubs and foster open markets (e.g., the EU's single market).<br>- Encourage reskilling and education initiatives to prepare the workforce for AI-complementary roles (e.g., Japan’s "silver economy").<br>- Develop flexible regulatory frameworks to encourage ethical AI development and prevent monopolies.<br>- Implement new dynamic metrics like Migration Momentum and Automation Lift Index to replace static GDP.<br>- Redirect entropy through social safety nets like universal basic income (UBI).<br>- Promote circular economies to reduce resource scarcity and counter geopolitical tensions.<br>- Encourage decentralized finance (DeFi) with appropriate regulatory frameworks.<br>- Foster international collaboration to address risks related to resource conflicts and tech dominance.<br>- Support the development of infrastructure to handle the increased demands of AI and automation.<br>- Invest in research to develop a deeper understanding of the potential impacts of automation and AI.<br>- Promote remote work and open innovation to accelerate flow and economic growth. |
| George-Sabin-2-AI-RFI-2025.md | George Sabin | Individual | The submitter is interested in leveraging the principles of physics and economics – specifically flow, migration, and catallaxy – to understand and optimize economic growth and resilience in the Age of Automation, focusing on policies that encourage adaptation and reduce friction in the flow of human, natural, and technological resources. | - Job displacement due to automation (potentially 40% of manufacturing jobs by 2030).<br>- Inequality resulting from automation benefits not being widely distributed.<br>- Lack of reskilling/education opportunities for workers displaced by automation.<br>- Risks associated with decentralized finance (DeFi) including regulatory gaps and volatility.<br>- Geopolitical tensions and resource conflicts arising from increased demand for materials like lithium and cobalt.<br>- Potential for digital colonialism if tech giants rival nation-states.<br>- Ethical concerns surrounding AI development.<br>- Tech giants’ dominance and the need to preserve catallactic diversity (i.e., competition).<br>- Risks of exclusion for low-skill workers.<br>- Instability in labor markets if unchecked automation occurs.<br>- Volatility in economies lacking regulatory frameworks for DeFi. | - Reduce friction in economic systems (e.g., streamlining bureaucratic processes, like Estonia's e-Residency system).<br>- Invest in reskilling and education programs, specifically for roles complementary to AI (e.g., Japan’s silver economy model).<br>- Implement social safety nets like Universal Basic Income (UBI) to mitigate job displacement.<br>- Develop flexible regulatory frameworks that encourage ethical AI development.<br>- Implement antitrust measures to curb tech giants’ dominance.<br>- Promote open markets and innovation hubs to channel flows effectively (e.g., the EU's single market).<br>- Encourage circular economies to reduce resource scarcity and counteract geopolitical tensions.<br>- Develop new dynamic metrics like Migration Momentum and Automation Lift Index to capture economic velocity and account for inequality.<br>- Redirect entropy through shifts to renewable energy sources and repurposing declining sectors.<br>- Encourage remote work and open innovation to accelerate flow.<br>- Establish frameworks to protect sovereignty and prevent digital colonialism.<br>- Implement policies accelerating flow (remote work, open innovation).<br>- Harness entropy (reskilling, renewables).<br>- Measure motion.<br>- Use AI in healthcare. |
| George-Minas-AI-RFI-2025.md | George Minas | Individual | The submitter is focused on strengthening American leadership in Artificial Intelligence through bolstering domestic capabilities in hardware, energy efficiency, innovation, and national security, with a clear emphasis on competitiveness against foreign rivals. | - [§1: Hardware and Chips] The potential for export control policies to inadvertently weaken U.S. firms' global market position, and the risk of technology transfer to adversaries (China, Russia, Iran, DPRK, terrorist groups).<br>- [§2: Energy Consumption and Efficiency] Not Stated for this section.<br>- [§3: Innovation and Competition] Not Stated for this section.<br>- [§4: National Security and Defense] The need to protect critical AI technologies and maintain a competitive edge in autonomous systems and cybersecurity defenses. | - [§1: Hardware and Chips] Accelerate investment in domestic semiconductor manufacturing (building on the CHIPS Act), prioritize next-generation AI chips, and streamline export controls with a 90-day review of efficacy.<br>- [§2: Energy Consumption and Efficiency] Establish a federal task force to designate "AI Energy Zones" on federal lands with abundant clean energy and expedite permitting processes for data centers. Offer grants or low-interest loans for energy-efficient AI hardware and cooling technologies with a 20% reduction goal for data center energy consumption within five years.<br>- [§3: Innovation and Competition] Create an "AI Innovation Sandbox" program allowing temporary exemptions from regulatory requirements for startups and small businesses. Direct the FTC to issue guidance clarifying that collaborative AI research will not be penalized, provided it strengthens domestic competitiveness.<br>- [§4: National Security and Defense] Mandate a joint DoD and intelligence community assessment to identify critical AI technologies requiring protection. Increase funding for the DoD's AI programs by 25% in the FY2026 budget, focusing on autonomous systems and cybersecurity defenses. |
| George-Kesidis-RFI-2025.md | George Kesidis & David J. Miller | Academia/Industry-Startup | The submitters are AI/ML researchers with a focus on secure and robust AI, expressing concern over the inefficiencies and ethical problems in current AI research funding and development practices and advocating for targeted investments and increased accountability. | - [§1: The dangers of using AI] AI models are vulnerable to bias and security issues, particularly as they grow larger in parameter size. Over-parameterization can lead to overfitting and excessive resource consumption. The success of Deepseek with lower-end hardware challenges the prevailing trend of scaling models.<br>- [§2: The dangers of overfunding, particularly AI] Excessive research funding leads to a mass production of low-quality research, corruption in peer review processes, and potential influence peddling within the AI conference system. The scale of submissions to conferences like NeurIPS, ICML, AAAI, and ICLR compromises academic integrity.<br>- [§3: Recommendations regarding AI Research Funding] The current focus on large-scale AI centers is a waste of taxpayer money. There is a need for a limit on the number of papers per researcher to promote quality and a redirection of funding towards graduate student scholarships.<br>- [§Authors: Authors and Credentials] Not stated for this section. | - [§1: The dangers of using AI] Not stated for this section.<br>- [§2: The dangers of overfunding, particularly AI] Not stated for this section.<br>- [Section 3: Recommendations regarding AI Research Funding]<br>- Targeted investments in secure and robust AI (cross-disciplinary between AI/ML and cyber security).<br>- Smaller grant sizes.<br>- Setting a limit on the annual number of papers per researcher.<br>- Capping conference registration costs and publication charges allowed on federal grants.<br>- Redirecting research investments to graduate-student scholarships.<br>- Encouraging universities to refocus on properly educating students, rather than running degree mills.<br>- [§Authors: Authors and Credentials] Not stated for this section. |
| Gene-Owl-AI-RFI-2025.md | Gene Owl | Individual | The submitter expresses a strong stance against allowing circumvention of copyright protections, indicating an interest in protecting intellectual property rights within the context of AI development and deployment. | - Circumvention of copyright protections. | - None Stated. |
| GenAI-Collective-AI-RFI-2025.md | The GenAI Collective | Advocacy/Think Tank | The GenAI Collective aims to empower the AI ecosystem through collaborative research and advocacy, focusing on fostering trust, openness, and global prosperity in AI development, with a specific emphasis on data access as the primary bottleneck to innovation and countering China’s data strategy. | - [§1: Introduction/Header] None stated for this section.<br>- [§2: Executive Summary] China's data strategy poses a significant threat to US AI leadership; fragmented US data hinders AI innovation; data is the primary bottleneck, not infrastructure.<br>- [§3: National Data Store Proposal] Potential resistance from privacy advocates and industry incumbents; need for a robust security framework; potential operational overheads of submitting real data.<br>- [§4: Modern, Interoperable Information Architecture] Outdated data standardization protocols limit interoperability; Inconsistent database schemas and lack of common protocols reduce efficiency.<br>- [§5: Implementation Strategy] Need to create AI-related jobs (rather than simply replacing them); attracting top global talent; ensuring broad economic opportunity.<br>- [§6: Caveats and Mitigations] Security & Privacy – safeguarding individual privacy is paramount; Competitive Dynamics – US companies may be reluctant to share proprietary data.<br>- [§7: Example Executive Order] None stated for this section.<br>- [§8: Conclusion] None stated for this section. | - [§1: Introduction/Header] None stated for this section.<br>- [§2: Executive Summary] None stated for this section.<br>- [§3: National Data Store Proposal] Create a National Data Store; Require companies to contribute data to access it; Implement strict data redaction protocols; Establish an independent National Data Store Board.<br>- [§4: Modern, Interoperable Information Architecture] Encourage industry-led initiatives to unify data formats and protocols; Offer grants/tax breaks for standardization; Promote best practices and success stories.<br>- [§5: Implementation Strategy] Focus on augmented intelligence and upskilling programs; Attract top global talent through visa programs; Provide early-stage AI startups access to the National Data Store.<br>- [§6: Caveats and Mitigations] Mandate robust PII redaction and zero-trust cybersecurity; Implement financial incentive structures to encourage contribution; Focus on foundational AI models initially.<br>- [§7: Example Executive Order] Create an independent National Data Store Board; Require mandatory data contribution from federal agencies and contractors; Mandate security protocols; Encourage interoperability; Establish pilot programs for AI startups.<br>- [§8: Conclusion] None stated for this section. |
| GeckoRobotics-AI-RFI-2025.md | Gecko Robotics | Industry-Startup | Gecko Robotics aims to transform infrastructure construction, operation, and maintenance using AI and robotics, and seeks to secure US leadership in AI by focusing on energy and data availability – specifically advocating for increased investment in data collection for physical infrastructure and optimization of energy production to support AI development. | - [§1: Introduction & Executive Summary] The primary concern is that US AI development is too focused on “computer screens” (software) and not enough on “atoms” (the physical world), potentially hindering its practical application and competitiveness.<br>- [§2: The Importance of Energy & Data] Insufficient access to high-quality, high-volume data, especially regarding physical infrastructure, is a major impediment to AI advancement. The growing energy demands of AI are also a concern.<br>- [§3: Proposal 1: Optimizing Power Generation] The US is in an energy crisis, and AI’s increasing energy demands are exacerbating the issue. Current grid investments are insufficient to support the growth of AI.<br>- [§4: Proposal 1: Supporting Information/Rationale] Existing grid investments are focused on transmission and distribution, while generation needs more attention. The relationship between AI dominance and energy dominance is crucial.<br>- [§5: Proposal 2: AI-Ready Data for New Infrastructure Builds] The quality of data will be the differentiator for American AI companies, as open-source models become prevalent. The lack of sufficient data on physical infrastructure is hindering AI solutions.<br>- [§6: Proposal 2: Sub-recommendations A & B] A lack of ‘ground-truth’ data on capital-intensive assets (like submarines) prevents effective development of AI and hinders QA/QC processes in major acquisition programs. Manual inspection processes are insufficient.<br>- [§7: Proposal 2: Supporting Information/Rationale] Current manual processes for inspections (like weld inspections) are insufficient and prevent the permanent recording of raw data. Recurring issues with contractor welds demonstrate the need for improved data collection.<br>- [§8: Closing] None stated for this section. | - [§1: Introduction & Executive Summary] None stated for this section.<br>- [§2: The Importance of Energy & Data] None stated for this section.<br>- [§3: Proposal 1: Optimizing Power Generation] The Department of Energy (DOE) should encourage the use of advanced technology (robotics, automation, predictive maintenance, real-time analytics) across all energy sources to optimize operations, increase production, and reduce downtime. DOE should issue regulations requiring advanced technology use and provide loan/grant funding.<br>- [§4: Proposal 1: Supporting Information/Rationale] None stated for this section.<br>- [§5: Proposal 2: AI-Ready Data for New Infrastructure Builds] Require high-fidelity digital twins be created at the time of construction for government-funded infrastructure projects.<br>- [Section 6: Proposal 2: Sub-recommendations A & B]<br>- (A) Require a high-fidelity digital twin created at the time of construction for major government infrastructure projects, informed by data on the physical condition of the asset.<br>- (B) Require all capital-intensive assets critical for U.S. national security and defense to have a full digital scan of their physical condition.<br>- [§7: Proposal 2: Supporting Information/Rationale] None stated for this section.<br>- [§8: Closing] None stated for this section. |
| Garza-AI-RFI-2025.md | Charlie Garza | Individual | The submitter is a creative professional (writer, coder, artist, musician) with a strong opposition to the current practices of generative AI companies, particularly regarding the unauthorized use of artists’ work for training purposes. They advocate for ethical data sourcing and fair compensation for creators. | - Unauthorized use of artists’ work to train generative AI models.<br>- The argument that generative AI creation mimics human inspiration is flawed and diminishes the role of human creativity.<br>- Generative AI relies entirely on existing works and cannot function without them, unlike human creativity.<br>- Generative AI’s use of stolen work actively harms creatives by potentially cutting expenses for studios and taking money from individuals.<br>- The lack of consent from artists whose work is used in AI training databases. | - Purge current generative AI databases.<br>- Require individuals and organizations to explicitly submit works for inclusion in AI training datasets, rather than scraping data without consent. |
| Gary-Duncan-AI-RFI-2025.md | Gary Duncan | Individual | The submitter expresses strong opposition to the development and spread of AI, citing concerns about intellectual property theft and a general negative impact on livelihoods, and advocates for halting the AI Action Plan's development. | - [§1: Email Header] None stated for this section.<br>- [§2: Introduction & Statement of Concern] The AI Action Plan contrasts with previous administration efforts to establish responsible AI development guardrails; concern for friends whose livelihoods could be threatened by unrestrained AI.<br>- [§3: Elaboration of Concerns - Intellectual Property] AI learning models unfairly take and focus on information and ideas created by individuals, assimilating it and resulting in "stolen goods."<br>- [§4: Objection & Negative Sentiment] Formal objection to the plan's development; widespread negative sentiment towards the spread of AI and AI learning models.<br>- [§5: Critique of Presidential Authority] Allegation that the President has signs of dementia and therefore cannot be trusted to make informed decisions regarding AI policy.<br>- [§6: Closing & Plea] Negative view of AI’s spread and a plea to consider the author’s input to halt the development of the AI Action Plan. | - [§1: Email Header] None stated for this section.<br>- [§2: Introduction & Statement of Concern] None stated for this section.<br>- [§3: Elaboration of Concerns - Intellectual Property] None stated for this section.<br>- [§4: Objection & Negative Sentiment] None stated for this section.<br>- [§5: Critique of Presidential Authority] None stated for this section.<br>- [§6: Closing & Plea] Halt the development (read: destruction) of the AI Action Plan. |
| Garry-Osborne-AI-RFI-2025.md | Garry Osborne | Individual (likely policy advisor/expert, given the comprehensive nature of the plan) | The submitter is focused on securing U.S. leadership in Artificial Intelligence through a combination of incentivizing domestic innovation, strategic international cooperation, and countering the influence of China. The submitter strongly advocates for a proactive, whole-of-government approach to achieve and maintain dominance in the AI landscape. | - [§1: Smart Regulatory Framework] The risk of stifling innovation through overly restrictive regulations; the need for flexibility and adaptation in regulatory approaches.<br>- [§2: Navigating Geopolitics and International Cooperation] The competitive threat posed by China; the importance of building a united front with allies; the need to set global norms for ethical AI use.<br>- [§3: Rallying Allies and Partners] The potential for China to outpace the U.S. in AI innovation without collective action; the importance of leveraging allied strengths to create a stronger innovation ecosystem.<br>- [§4: Countering Adversaries (China)] China's attempts to acquire U.S. AI technology and capabilities; the need to prevent strategic technology from flowing to rivals; the risk of China circumventing export controls.<br>- [§5: AI Diplomacy] The potential for China to gain influence in developing countries through its Digital Silk Road initiative; the need to counter China's outreach and promote an open AI ecosystem.<br>- [§6: Military Considerations and Responsible Use] The risks of an AI arms race; the potential for accidental conflict; the need for dialogues with China and Russia on responsible military use of AI.<br>- [§7: Conclusion] The urgent need for action to counter China's AI challenge and secure U.S. dominance; the risk of losing global technological leadership. | - [§1: Smart Regulatory Framework] Establish a lightweight but effective governance framework; create a National AI Safety Commission; develop voluntary standards and ethics guidelines; prioritize targeted and risk-based regulation; update existing regulations to cover AI; champion "enabling" regulation; task NIST with developing technical standards for AI safety; encourage transparency; increase funding for AI safety research.<br>- [§2: Navigating Geopolitics and International Cooperation] Intensify cooperation with allied democracies; explore the formation of an "Alliance for AI Leadership"; pursue joint research ventures; align on principles for ethical AI use; propose Global AI Safety Summits; work through the Global Partnership on AI.<br>- [§3: Rallying Allies and Partners] Share research findings; coordinate investments in AI research; pool talent programs among like-minded nations; expand joint ventures (transatlantic AI research centers, U.S.-India tech incubators).<br>- [§4: Countering Adversaries (China)] Maintain and tighten export controls; enforce and update foreign direct product rules; work with allies (Netherlands, Japan) to synchronize controls; enact an outbound investment review mechanism to discourage investment in Chinese AI companies.<br>- [§5: AI Diplomacy] Offer AI capacity-building to developing countries; provide AI tools for agriculture and healthcare; champion an open, interoperable AI ecosystem.<br>- [§6: Military Considerations and Responsible Use] Pursue dialogues with China (and Russia) on responsible military use of AI; seek confidence-building measures (notification of AI weapons tests, agreements on banning autonomous nuclear launch decisions).<br>- [§7: Conclusion] Ignite innovation through tax breaks, funding, and prizes; align business incentives with national security; unite the private sector with smart public policy; execute the Action Plan to secure U.S. dominance. |
| GarlandWalton-AI-RFI-2025.md | Garland Walton | Individual | The submitter expresses opposition to the use and expansion of AI due to concerns about national security, economic stability, and potential harm to individuals and corporations. They advocate for a cautious approach and the establishment of failsafes before further AI deployment. | - National security risks.<br>- Theft of ideas and labor.<br>- Technology security vulnerabilities.<br>- Market vulnerability/potential economic destabilization.<br>- Job destruction in creative and tech sectors.<br>- Errors leading to expensive coding issues and financial losses.<br>- Potential to ruin companies and families. | - Establish failsafes before further AI deployment.<br>- None Stated. |
| Gamblers-Edge-AI-RFI-2025.md | Gambler's Edge | Individual | The submitter expresses deep concern over the negative impacts of AI development and deployment, advocating for caution and seeking reliance on "Actual Experts" to address the issues. They appear to be motivated by ethical, environmental, and societal risks associated with current AI practices. | - AI is largely ineffective ("90-odd-percent *Snake Oil*").<br>- Reliance on "slave labor" to correct AI errors.<br>- Significant energy waste and large carbon footprint.<br>- Widespread unpopularity in creative fields.<br>- Contribution to the chip shortage.<br>- Exacerbation of mineral extraction issues and associated human rights abuses.<br>- Approaching a finite ceiling on AI learning capabilities.<br>- Privacy concerns related to data brokers and non-consensual pornography (Deepfakes).<br>- Creation of simulacra of deceased persons for malicious purposes.<br>- Encouragement of human laziness and intellectual atrophy.<br>- Gatekeeping of AI access based on wealth.<br>- Legislative lag behind technological advancements, creating opportunities for bad actors. | - Rely on "Actual Experts" for guidance in addressing AI-related issues.<br>- None Stated |
| Gage-Durst-AI-RFI-2025.md | Gage Durst | Individual | The submitter is concerned with the protection of copyrighted materials and the rights of creators whose work may be used to train AI models without permission or compensation. They are focused on preventing the devaluation and illegal use of copyrighted work. | - Mass theft of copyrighted materials to be fed into AI.<br>- Devaluation of creators’ work.<br>- Illegal use of copyrighted work. | - Scrapping the plan that would allow mass theft of copyrighted materials for AI training. |
| Gabriela-Santiago-AI-RFI-2025.md | Gabriela Santiago | Individual | The submitter is concerned about protecting the intellectual property and economic interests of small businesses and creators from large companies potentially exploiting their work through AI-related legal loopholes. They advocate for prioritizing human creators over AI development. | - Large companies are demanding government action to legalize the theft of intellectual property ("hard work") for their own profit.<br>- The current legal environment may allow large companies to exploit the work of Americans without proper compensation or attribution.<br>- Prioritizing AI development over the protection of small businesses and creators. | - Protect small businesses and the incentive to create.<br>- Not allow legal frameworks that facilitate the theft of intellectual property for the benefit of large companies. |
| GNYHA-AI-RFI-2025.md | Greater New York Hospital Association (GNYHA) | Industry-Association | GNYHA represents hospitals and health systems and seeks to promote policies that facilitate the safe, responsible, and equitable integration of AI into healthcare, while addressing concerns about resource disparities, vendor accountability, data privacy, and regulatory burdens. | - [§1: Introduction/Cover Letter] None stated for this section.<br>- [§2: AI Application and Use; Open-Source Development] Lack of equitable access to AI resources for hospitals with limited infrastructure and technical expertise.<br>- [§3: Explainability and Assurance of AI Model Outputs] Concerns about the integration and implementation of AI technologies without sufficient oversight and quality assurance from vendors.  Hospitals should not bear full responsibility for ensuring AI quality. Difficulty achieving full explainability of AI outputs.<br>- [§4: Data Privacy and Security]  Privacy and security standards for AI vendors, particularly those not covered by HIPAA, and potential liability for data breaches. Lack of negotiating leverage with large AI vendors regarding contract terms and data sharing practices.<br>- [§5: Regulation and Governance]  Lengthy FDA clearance processes delaying AI tool implementation. The need for standardized regulation of non-SaMD AI tools.<br>- [§6: Risks of AI in Insurance and Claims] Misuse of AI in claims review processes leading to increased denials of medical care without appropriate physician review.  Automated denials without careful review of individual patient circumstances.<br>- [§7: Other Considerations/Funding Concerns] Excessive regulation creating barriers to AI implementation. Potential cuts to NIH funding slowing AI innovation. | - [§1: Introduction/Cover Letter] Promote policies that foster collaboration and provide open-source model availability; increase oversight and strengthen privacy and security requirements for AI vendors; modernize AI regulatory processes within the FDA; and prohibit the misuse of AI in claims review.<br>- [§2: AI Application and Use; Open-Source Development] Implement financial investments for hospitals to foster collaboration with leading AI institutions. Make federally funded AI models open source or structured for maximal collaboration.<br>- [§3: Explainability and Assurance of AI Model Outputs] Require private AI vendors to demonstrate that their technologies operate safely and responsibly, with routine quality and safety reviews. Focus regulatory oversight on vendors, not hospitals. Require vendors to demonstrate their tools work as intended and consistently produce reliable outcomes.<br>- [§4: Data Privacy and Security] Ensure that privacy and security standards (including HIPAA) apply to AI vendors working with hospitals. Establish reasonable standards for vendors not covered by HIPAA to protect patient information. Require vendors to disclose their data sharing practices, including whether data is shared with foreign entities.<br>- [§5: Regulation and Governance] Update FDA’s regulatory framework for AI tool oversight. Expedite the review process for AI tools. Streamline the review process and allow opportunities for expedited review. Task the FDA with oversight of non-SaMD AI tools.<br>- [§6: Risks of AI in Insurance and Claims] Prohibit insurers and benefit plans from misusing AI in the claims review process. Require an appropriately qualified physician to review any claim denial. Ensure that denials of medical claims are not automated without physician review. Require plans to disclose how they use AI in the claims review process to regulators, consumers, and providers.<br>- [§7: Other Considerations/Funding Concerns] Avoid excessive regulation that creates barriers to AI implementation. Preserve funding for NIH research to support AI innovation.<br>- [§8: Closing/Contact Information] None stated for this section. |
| GMR-AI-RFI-2025.md | Global Music Rights, LLC | Industry-Association | Global Music Rights, LLC represents songwriters and composers and is focused on protecting music rights and promoting the value of intellectual property in the age of generative AI, ensuring fair compensation and control for human creators. | - [§1: Introduction/Header] None stated for this section.<br>- [§2: Guiding Principles] GenAI presents both opportunities and significant threats to musical creativity and commerce; a fundamental difference exists between AI tools *used by* humans and GenAI systems designed to operate without meaningful human curation; preserving American musical commerce and culture requires guaranteeing exclusive rights to human creators.<br>- [§3-10: Specific Legislative Recommendations] The potential for GenAI to cause lasting damage to American musical creativity, commerce, and culture; the need for exclusive *sui generis* rights for human creators; concerns regarding unlicensed use of copyrighted works; the potential for AI-generated works to supplant human creators; the importance of record keeping regarding copyrighted works used in training GenAI systems; concerns about the copyrightability of purely AI-generated works; the need for metadata guidelines; the speed of advancements in GenAI technology.<br>- [§11: Signature] None stated for this section. | - [§1: Introduction/Header] None stated for this section.<br>- [§2: Guiding Principles] None stated for this section.<br>- [Section 3-10: Specific Legislative Recommendations]<br>- Guarantee human creators exclusive *sui generis* rights to control the use of their works in all GenAI contexts (ingestion, manipulation, output).<br>- Enable collective negotiation of licenses for use and payment of royalties for copyrighted works inputted into GenAI training systems.<br>- Ensure payment of royalties on all output generated by GenAI systems.<br>- Maintain liability for copyright plagiarism under existing laws.<br>- Vest *sui generis* rights at the time of creation, conveyable only through written instrument executed by the human author (or heirs) and subject to termination rights under the US Copyright Act.<br>- Clarify that unlicensed ingestion, manipulation, and output of copyrighted works by GenAI systems presumptively fall outside the boundaries of fair use.<br>- Require complete and accurate records of all copyrighted works utilized in GenAI training and output, dating back to system initiation.<br>- Establish copyrightability standards requiring significant human curation in the creative process, beyond mere prompts.<br>- Include strict guidelines concerning mandatory use and preservation of metadata.<br>- Enact legislative action promptly, recognizing the speed of advancements in the GenAI sector.<br>- [§11: Signature] None stated for this section. |
| GETTING-Plurality-AI-RFI-2025.md | GETTING-Plurality Research Network | Academia/Advocacy/Think Tank | The GETTING-Plurality Research Network is a multi-disciplinary research group focused on governing emerging technology and tech innovations to promote next-generation governance, with a particular emphasis on aligning AI development with democratic values and national security interests. They aim to bridge technology and policy initiatives. | - None Stated | - None Stated |
| GE-Vernova-AI-RFI-2025.md | GE Vernova | Industry-Large | GE Vernova aims to position itself as a leading provider of energy solutions powering the growth of AI and data centers in the U.S., emphasizing the need for increased grid capacity, reliable dispatchable power, and policy support for infrastructure development and manufacturing. | - [§1: Introduction & Company Overview] None stated for this section.<br>- [§2: GE Vernova's Role in Powering AI Leadership] None stated for this section.<br>- [§3: Industry Collaboration & Expertise] None stated for this section.<br>- [§4: Manufacturing Capabilities] None stated for this section.<br>- [§5: Expected Energy Demand Growth] Rapidly increasing energy demand from data centers (2,000-5,000 MW sites) straining existing infrastructure.<br>- [§6: Roadmap for Meeting U.S. Data Center Demand] Balancing rapid deployment of gas capacity with long-term integration of renewables and nuclear.<br>- [§7: Solving Bottlenecks to Energy & Electricity Deployment] Interconnection delays, permitting challenges, lack of grid capacity, and the need for more advanced software and grid components.<br>- [§7.1: Dispatchable Power] Insufficient existing generating capacity, slow permitting processes, and lack of policy support for gas generation.<br>- [§7.2: Grid Readiness through Critical Grid Components and Software] Transmission bottlenecks, slow interconnection studies, localized grid congestion, and a lack of real-time visibility and control.<br>- [§7.3: Enabling Systems & Behind-the-Meter Approaches] Reliability risks associated with off-grid solutions, regulatory gray areas, potential strain on local resources, and misalignment with regional grid strategies.<br>- [§7.4: Spurring American Energy Sector Innovation While Powering Data Centers] Not stated. | - [§1: Introduction & Company Overview] None stated for this section.<br>- [§2: GE Vernova's Role in Powering AI Leadership] None stated for this section.<br>- [§3: Industry Collaboration & Expertise] None stated for this section.<br>- [§4: Manufacturing Capabilities] None stated for this section.<br>- [§5: Expected Energy Demand Growth] None stated for this section.<br>- [§6: Roadmap for Meeting U.S. Data Center Demand] None stated for this section.<br>- [Section 7: Solving Bottlenecks to Energy & Electricity Deployment]<br>- Streamline permitting processes for transmission projects.<br>- Incentivize grid investment.<br>- Encourage proactive grid upgrades.<br>- [Section 7.1: Dispatchable Power]<br>- Upgrade existing assets.<br>- Policy support for gas generation (consistent regulations, grid interconnects).<br>- Expedited deployment of nuclear energy (NRC efficiency).<br>- [Section 7.2: Grid Readiness through Critical Grid Components and Software]<br>- Accelerated transmission expansion and upgrades (fast-track approval, federal siting authority).<br>- Reformed interconnection processes (prioritize ready projects, faster studies).<br>- Upgrade local grid infrastructure (modernize distribution networks, co-funding models).<br>- [Section 7.3: Enabling Systems & Behind-the-Meter Approaches]<br>- Expedited permitting for hyperscaler-led energy projects.<br>- Standardized siting rules.<br>- Flexible grid participation models.<br>- Enhanced natural gas and hydrogen access.<br>- Investment tax credits and production tax credits.<br>- Incentives for on-site energy storage. |
| GDA-AI-RFI-2025.md | Global Data Alliance (GDA) | Industry-Association | The Global Data Alliance (GDA) represents a coalition of US-based companies advocating for policies that promote cross-border data access to foster AI innovation, economic competitiveness, and national security. They emphasize the importance of data flow for job creation and the development of AI-driven solutions. | - [§1: Introduction/Header] None stated for this section.<br>- [§2: About the Global Data Alliance (GDA)] The need for companies to rely on the ability to access and transfer information across borders to innovate and create jobs in the US.<br>- [§3: Executive Summary/Core Argument]  Impediments to cross-border data access could hinder the US's ability to sustain and enhance its AI-driven economic competitiveness and national security.<br>- [§4: Discussion/Rationale] Improper data transfer impediments (arbitrary, discriminatory, disguised, or unnecessary) by foreign governments could limit the benefits of data analytics and AI development. The importance of data analytics being dependent on data sourced globally.<br>- [§5: Annex: AI Applications Dependent on Cross-Border Data]  Illustrates the dependence of various AI applications (air traffic control, healthcare, climate modeling, security) on data sourced from transnational sources, implicitly highlighting the risk of limiting such access.<br>- [§6: Footnotes/Citations] None stated for this section. | - [§1: Introduction/Header] None stated for this section.<br>- [§2: About the Global Data Alliance (GDA)] None stated for this section.<br>- [§3: Executive Summary/Core Argument] Promote cross-border data access to fulfill AI Action Plan goals. Advocate for a whole-of-government approach to promote the free and responsible flow of data, and interoperable data transfer mechanisms.<br>- [§4: Discussion/Rationale] Engage with allied economies to ensure that US cross-border access to data is not improperly impeded through foreign government data transfer barriers that are arbitrary, discriminatory, disguised, or unnecessary. Leverage existing US government efforts and tools (USTR, Commerce, State, Treasury) to promote cross-border data transfers.<br>- [§5: Annex: AI Applications Dependent on Cross-Border Data] None stated for this section.<br>- [§6: Footnotes/Citations] None stated for this section. |
| GCI-AI-RFI-2025.md | General Catalyst Institute (GCI) | Industry-Startup/Advocacy/Think Tank | GCI aims to promote national resilience through backing transformative technologies, specifically applied AI, and shaping public policy to improve society, with a focus on fostering a healthy ecosystem for entrepreneurship and collaboration between stakeholders. They are keenly interested in ensuring the US remains competitive in the global AI landscape. | - None Stated | - None Stated |
| Fuzxguys-AI-RFI-2025.md | Fuzxguys | Individual | The submitter expresses strong opposition to AI and advocates against the current executive order, believing it will be detrimental. Their primary interest appears to be preventing the advancement or implementation of AI policies. | - The AI executive order will cause more problems than it solves. | - Overturn the AI executive order. |
| Frontier-Model-Forum-AI-RFI-2025.md | Frontier Model Forum | Industry-Association | The Frontier Model Forum is an industry-supported non-profit dedicated to advancing the secure development and deployment of frontier AI systems, focusing on chemical, biological, radiological, nuclear (CBRN) and advanced cyber threats, and promoting collaboration between industry, government, and the scientific community. | - [§1: Introduction/Executive Summary] None stated for this section.<br>- [§2: Supporting the Science of AI Security and Safety] The science of AI security and safety is nascent, with open questions regarding testing, evaluation, and risk mitigation.<br>- [§3: Strengthening International Coordination and Global Standards] Fragmentation in international AI standards and the need for global alignment on security and governance practices reflecting democratic values.<br>- [§4: Advancing National Security Testing and Coordination] The need for close public-private collaboration for evaluating frontier AI risks, the evolving nature of the science, and strategic awareness of foreign models.<br>- [§5: Conclusion] None stated for this section. | - [§1: Introduction/Executive Summary] None stated for this section.<br>- [Section 2: Supporting the Science of AI Security and Safety]<br>- Invest in AI metrology and measurement research.<br>- Develop robust AI risk management processes.<br>- Ensure efforts to improve AI security include relevant subject matter experts.<br>- [Section 3: Strengthening International Coordination and Global Standards]<br>- U.S. leadership in setting international AI standards.<br>- Strengthen coordination with foreign agencies like the U.K. AI Security Institute (AISI).<br>- [Section 4: Advancing National Security Testing and Coordination]<br>- Continue supporting national security testing and evaluation.<br>- Create testbeds at relevant bodies like Sandia National Laboratories.<br>- Facilitate voluntary pre- and post-deployment evaluations by government experts.<br>- Coordinate across national security agencies for information-sharing and integrated approaches to risk management.<br>- [§5: Conclusion] None stated for this section. |
| Friends-of-Cancer-Research-AI-RFI-2025.md | Friends of Cancer Research | Industry-Association / Advocacy/Think Tank | Friends of Cancer Research is focused on accelerating the development of new cancer treatments through research and advocating for policies that support innovation in drug development, particularly leveraging AI and machine learning technologies. They are interested in ensuring a clear regulatory pathway for AI-driven diagnostics and therapeutics while maintaining patient safety. | - None Stated | - None Stated |
| Frederick-County-AI-RFI-2025.md | Frederick County Government | Government | Frederick County Government is focused on balancing the economic benefits of data center development with the protection of local quality of life, environmental sustainability, and infrastructure capacity, and preserving local land use authority in the face of growing data center demand. | - [§1: Header/Letterhead] None stated for this section.<br>- [§2: Salutation/Introduction] Potential interference by the federal government in local regulation of data center planning, zoning, and siting.<br>- [§3: Core Argument: Local Authority & Data Center Regulation] Loss of local land use authority, which is seen as crucial for effectively meeting community needs.<br>- [§4: Context: Frederick County Data Centers Workgroup] Environmental sustainability concerns, energy and water usage, noise, vibration, and impacts on quality of life related to data center development.<br>- [§5: Energy Consumption & Infrastructure Concerns] High energy consumption of data centers and AI, strain on the electric grid, potential for increased electricity rates, and significant costs associated with energy infrastructure expansion (specifically the Maryland Piedmont Reliability Project).<br>- [§6: Call to Action/Conclusion] Threats to quality of life due to data center development, the need for responsible siting and planning, and potential disruption caused by energy infrastructure projects. | - [§1: Header/Letterhead] None stated for this section.<br>- [§2: Salutation/Introduction] The Office of Science and Technology Policy should not interfere with local government authority over data center regulation.<br>- [§3: Core Argument: Local Authority & Data Center Regulation] Preserve local land use authority over data center planning, zoning, and siting.<br>- [§4: Context: Frederick County Data Centers Workgroup] Update Critical Digital Infrastructure Ordinance with siting criteria and noise regulations; better reflect the realities of the evolving data center industry.<br>- [§5: Energy Consumption & Infrastructure Concerns] The federal government should mitigate the disruption caused by energy infrastructure projects.<br>- [§6: Call to Action/Conclusion] Protect local governments’ authority concerning planning, zoning, and siting of data centers and their campuses; ensure responsible siting of infrastructure and address livability concerns surrounding data center development. |
| Fred-Seisble-AI-RFI-2025.md | Fred Seisble | Individual | The submitter expresses deep concern regarding the potential for abuse and control enabled by AI, questioning the trustworthiness of those who would control it and prioritizing privacy safeguards. They are generally skeptical of the motives behind AI development and deployment. | - Abuse of AI.<br>- Lack of trust in governments to control AI responsibly.<br>- Lack of trust in large corporations to control AI responsibly.<br>- Lack of trust in political entities to control AI responsibly.<br>- Erosion of privacy since 09-01-01.<br>- Potential weaponization of AI against individuals.<br>- Potential weaponization of AI against political rivals.<br>- Potential weaponization of AI against other governments.<br>- Questionable motives: whether AI is for betterment or control. | - None Stated. |
| Frankenwolf-AI-RFI-2025.md | Frankenwolf | Individual | The submitter expresses strong opposition to AI development and deployment, primarily due to perceived harm to their livelihood and a general distrust of the technology's value and ethics. They believe AI is a deceptive practice harming the American public. | - AI steals from the submitter's livelihood as an American citizen.<br>- AI profits off of theft.<br>- AI has no moral use cases.<br>- AI is an overhyped scam.<br>- AI is fleecing the American public. | - None Stated |
| Frank-Ruscica-AI-RFI-2025.md | Frank Ruscica | Individual | Frank Ruscica believes his AI-preneurship is key to achieving President Trump’s national security goals, specifically ‘stable mutual deterrence,’ and envisions a significant role for customized education (CE-for-AI) in controlling the AI economy. He highlights a paradoxical relationship between China’s AI progress and nuclear risk to the U.S. | - [§1: Email Header] None stated for this section.<br>- [§2: Introduction & Link to Details] None stated for this section.<br>- [Section 3: Core Argument: AI, National Security & 'Stable Mutual Deterrence']<br>- The potential for autocrats with nukes to perceive themselves as imperiled by ongoing research advances in fields like molecular genetics, neuroscience, and AI.<br>- The need for a blueprint (SBP) to achieve stable mutual deterrence.<br>- The possibility of legally owning a significant stake (80%) in a company analogous to Amazon.com, focused on customized education and AI.<br>- [Section 4: Paradoxical Nuke Risk Argument]<br>- China’s AI progress *decreases* nuclear risk to the U.S. by increasing the risk of China being first to Artificial General Intelligence (AGI) and gaining decisive military advantage.<br>- Less AGI risk from China *increases* nuclear risk to the U.S.<br>- [§5: Closing & Disclaimer] None stated for this section. | - [§1: Email Header] None stated for this section.<br>- [§2: Introduction & Link to Details] None stated for this section.<br>- [Section 3: Core Argument: AI, National Security & 'Stable Mutual Deterrence']<br>- Delegate emergency/war powers to legally own a significant stake (80%) in a company analogous to Amazon.com focused on customized education and AI.<br>- Implement the SBP blueprint to achieve stable mutual deterrence.<br>- [§4: Paradoxical Nuke Risk Argument] None stated for this section.<br>- [§5: Closing & Disclaimer] None stated for this section. |
| Frank-DeMarco-AI-RFI-2025.md | Frank DeMarco | Individual | Frank DeMarco expresses a strong interest in national security and protecting the US “way of life” from perceived threats related to Chinese involvement in AI technologies. The submitter advocates for aggressive measures to control and restrict Chinese access to and influence over AI development and deployment within the United States. | - Reliance on Chinese software/hardware represents a national security threat.<br>- All Chinese companies are ultimately controlled by the Chinese Communist Party and pose a risk. | - Prohibit all Federal/State/Local agencies/entities from using Chinese, or Chinese-related software/hardware.<br>- Any State/Local entities not complying with the prohibition should lose Federal funding.<br>- Any Chinese company with relationships to the Communist Party or its members must be under the control of the USA. |
| Frank-Broen-AI-RFI-2025.md | Frank Broen | Individual | Frank Broen supports government regulation of AI development to ensure responsible use and mitigate the risks of biased outputs stemming from biased training data. He believes addressing self-justification and inherent bias is crucial for reliable AI decision-making. | - AI can provide completely fake answers.<br>- Bias in training data can impact AI decision-making.<br>- The current political climate encourages bias in data.<br>- People self-justify their own beliefs, potentially trapping themselves in "boxes" that influence AI. | - AI should be required to identify and report the bias that influences its decision process.<br>- Implement guardrails to ensure data used to train AI does not contain bias.<br>- Utilize guidelines from the Arbinger Institute to inform human-to-human interaction and bias mitigation. |
| ForHumanity-AI-RFI-2025.md | ForHumanity | Advocacy/Think Tank | ForHumanity is a non-profit organization dedicated to addressing the risks associated with AI systems (Ethics, Bias, Privacy, Trust, Cybersecurity) and promoting their safe and responsible development and deployment through audit criteria, certification schemes, and educational programs. They aim to maximize the benefits of AI while mitigating its potential harms. | - None Stated | - None Stated |
| For-Every-Child-RFI-2025.md | ostp_md (inferred from filename) | Government (specifically, likely the Office of Science and Technology Policy, given the "OSTP" in the filename and the RFI context) | This document represents a response to a Request for Information (RFI) regarding AI policy, aiming to establish a framework for responsible AI development, addressing ethical concerns, environmental impacts, and national leadership. The document focuses on mitigating risks and maximizing benefits associated with AI technologies. | - None Stated | - None Stated |
| Floyd-Hobson-AI-RFI-2025.md | Floyd Daniel Hobson III | Academia/Individual | Mr. Hobson’s mission is to promote ethical, equitable, and inclusive AI development through interdisciplinary research, particularly focusing on mitigating bias, ensuring representation of marginalized communities, and upholding cultural integrity in AI applications. He advocates for rigorous oversight and investment in diverse research perspectives. | - [§1: Introduction] AI remains in its developmental phase and requires sustained oversight to ensure embedded biases are systematically identified and eradicated.<br>- [Section 2: Addressing Algorithmic Bias: A Framework for Inclusive AI Development]<br>- Systemic underrepresentation of diverse cultural epistemologies in AI training sets.<br>- Risk of AI-mediated cultural appropriation and misrepresentation.<br>- Potential for Tiered Data Access models to negatively impact dataset integrity and transparency.<br>- Disproportionate allocation of AI research funding favoring institutions with infrastructural advantages.<br>- [Section 3: AI in Biomedical Research: Ethical and Epistemological Considerations]<br>- Propensity for racialized and stigmatized interpretations of genetic data.<br>- Imperative for federal oversight mechanisms to regulate AI applications in gender-affirming healthcare and reproductive rights.<br>- Ethical implications of AI-driven genetic analyses reinforcing pathologizing narratives or pseudoscientific racial classifications.<br>- Ensuring equitable design and accessibility of AI-driven diagnostic tools.<br>- Protecting patient privacy, data integrity, and clinical autonomy in AI-driven healthcare applications.<br>- [Section 4: AI, FAIR Principles, and the Socio-Political Landscape]<br>- Instrumentalization of AI to censor or delegitimize discourses on political injustices, civil rights, and reproductive autonomy.<br>- Increased likelihood of algorithmic employment discrimination impacting historically marginalized professionals.<br>- Deployment of AI technologies to undermine grassroots social movements through digital disinformation campaigns.<br>- Rising use of predictive AI policing models reinforcing racial profiling and institutional surveillance.<br>- Corporate monopolization of AI development preventing open-source and community-driven projects.<br>- [Section 5: Ethical AI and the Politics of Narrative Control]<br>- Erasing or distorting historically significant narratives leading to epistemic violence.<br>- Disseminating misinformation or algorithmically reinforcing harmful stereotypes.<br>- Suppressing counter-hegemonic discourses diminishing self-representation.<br>- Centralizing content creation within proprietary systems limiting independent voices.<br>- [§6: Policy and Structural Recommendations] Not Applicable. The section lists recommendations, not concerns.<br>- [§7: Conclusion] Not Applicable. The section reiterates importance and urgency, not specific concerns. | - [§1: Introduction] Not Applicable. The section does not include specific recommendations.<br>- [§2: Addressing Algorithmic Bias: A Framework for Inclusive AI Development] Not Applicable. The section does not include specific recommendations.<br>- [§3: AI in Biomedical Research: Ethical and Epistemological Considerations] Not Applicable. The section does not include specific recommendations.<br>- [§4: AI, FAIR Principles, and the Socio-Political Landscape] Not Applicable. The section does not include specific recommendations.<br>- [§5: Ethical AI and the Politics of Narrative Control] Not Applicable. The section does not include specific recommendations.<br>- [Section 6: Policy and Structural Recommendations]<br>- Implementation of federal AI oversight and auditing mechanisms to detect and eliminate systemic bias in datasets.<br>- Mandatory transparency requirements in AI training data to ensure fair representation.<br>- Establishment of multidisciplinary AI review boards incorporating scholars from various disciplines.<br>- Investment in AI research initiatives led by underrepresented scholars and practitioners.<br>- Development of impact assessment protocols for AI integration in academia and research.<br>- Expansion of federal funding for AI research in social justice and public policy.<br>- Creation of an independent federal AI ethics commission.<br>- [§7: Conclusion] Not Applicable. The section reiterates importance and urgency, not specific recommendations. |
| FlexHEG-AI-RFI-2025.md | Ethan Ashkie | Industry-Startup/Advocacy/Think Tank (Survival and Flourishing .Com, PBC suggests a for-profit entity with a broader mission; emphasis on security and innovation suggests advocacy) | Survival and Flourishing .Com, PBC is advocating for the development and implementation of FlexHEG technology to secure American leadership in AI development, protect intellectual property, enable controlled AI exports, and establish technical standards for hardware security, balancing innovation with national security concerns. | - None Stated | - None Stated |
| Flagship-Pioneering-AI-RFI-2025.md | Flagship Pioneering | Industry-Startup (Biotech/Life Sciences focused) | Flagship Pioneering creates, resources, and develops original bioplatform companies utilizing AI/ML to accelerate the development of transformative products across the life sciences, with a strong interest in maintaining U.S. leadership in AI innovation for national security, economic competitiveness, and public health. | - None Stated | - None Stated |
| FerootAI-AI-RFI-2025.md | Ivan Tsarynny (Feroot AI) | Industry-Startup | Feroot AI's mission is to identify, stop, and prevent the unauthorized collection of sensitive data that poses threats to national security, economic growth, and personal privacy, with a particular focus on data transfer to adversarial countries like China and the security risks associated with AI-enabled hardware. | - None Stated | - None Stated |
| Felipe-Castro-Quiles-GENIA-AI-RFI-2025.md | Felipe Castro Quiles / GENIA Latinoamérica, P.B.C. | Industry-Startup | GENIA Latinoamérica aims to transform the technological landscape of the Western Hemisphere by fostering a collaborative AI ecosystem that drives economic growth, innovation, and sustainable development, particularly within Latin America and the Caribbean. They focus on custom machine learning solutions tailored for regional needs and promoting representative AI. | - None Stated | - None Stated |
| Fathom-AI-RFI-2025.md | Fathom-AI | Advocacy/Think Tank | Fathom-AI aims to find, build, and scale solutions needed for a successful transition to a world with AI, prioritizing the protection and promotion of American values and leadership in the field, while augmenting human potential. | - [§1: Introduction] Concern that choices made now regarding AI development and governance will determine whether its potential is harnessed or squandered. Emphasis on the importance of preserving and deepening American values.<br>- [§2: Core Principles] Not stated for this section.<br>- [§3: Recommendations - Invest in Short- and Long-Term AI Governance] Concern that businesses face uncertainty about managing AI risks and complying with laws. Difficulty in creating clear compliance standards. Potential for gaming model evaluations. Lack of transparency in how frontier AI firms plan for major increases in capabilities. Patchwork of state regulations threatening national competitiveness.<br>- [§4: Recommendations - Accelerate AI Adoption in Government] Concern that current OMB memos (M-24-103 and M-24-184) are overly restrictive and hinder AI adoption. Overly broad definitions of “safety-impacting” and “rights-impacting” AI. Unnecessary environmental impact considerations. Cumbersome compliance processes for generalist AI systems. Burden of reporting AI use cases.<br>- [§5: Recommendations - Building, Permitting, and Semiconductor Manufacturing] Concern that permitting reform is critical but hampered by existing environmental regulations. Challenges and increasing costs associated with advancing semiconductor manufacturing technology.<br>- [§6: Recommendations - Transforming Science] Concern that current scientific infrastructure is not designed for the scale of experimentation needed to fully leverage AI's potential. Lack of demand for new kinds of lab equipment built for massively scalable experimentation.<br>- [§7: Recommendations - Ensuring AI Benefits Workers] Concern about the uncertain impact of AI on the labor market, with potential for near-term shocks in specific sectors.<br>- [§8: Recommendations - Building Genuine AI Security] Concern that current AI data center security is insufficient for sensitive applications. Need for rigorous control and safety mechanisms for military uses of AI. Issues with the Biden Diffusion Framework, specifically the model weight component and regulations on consumer chatbots.<br>- [§9: Conclusion] Not stated for this section. | - [§1: Introduction] Not stated for this section.<br>- [§2: Core Principles] Not stated for this section.<br>- [Section 3: Recommendations - Invest in Short- and Long-Term AI Governance]<br>- Focus on regulating harmful conduct by applying existing law.<br>- Build an AI evaluations ecosystem by incentivizing competitive markets for model evaluation providers.<br>- Foster transparency by mandating public disclosure of risk management plans from frontier AI companies.<br>- Create a unified national legal framework for AI.<br>- [Section 4: Recommendations - Accelerate AI Adoption in Government]<br>- Revise OMB memos M-24-103 and M-24-184 to narrow definitions and reduce restrictions.<br>- Modernize agency data policies to ease AI adoption.<br>- Increase construction of secure data centers.<br>- Revise the role of agency Chief AI Officers.<br>- Eliminate the requirement for agencies to publish inventories of AI use cases.<br>- [Section 5: Recommendations - Building, Permitting, and Semiconductor Manufacturing]<br>- Implement permitting reform.<br>- Invest in the National Semiconductor Technology Center.<br>- [Section 6: Recommendations - Transforming Science]<br>- Build cross-departmental science datasets and data infrastructure.<br>- Partner with the private sector to develop science foundation models.<br>- Create a competitive bidding process for robotic labs.<br>- [Section 7: Recommendations - Ensuring AI Benefits Workers]<br>- Direct data collection agencies to collect data relevant to AI’s impact on the economy.<br>- [Section 8: Recommendations - Building Genuine AI Security]<br>- Build a highly secure data center for sensitive AI applications.<br>- Invest in AI alignment and control research.<br>- Relax the model weight component and GPU caps within the Biden Diffusion Framework.<br>- [§9: Conclusion] Not stated for this section. |
| FWD.us-AI-RFI-2025.md | FWD.us | Advocacy/Think Tank | FWD.us advocates for policies to attract and retain highly skilled immigrants, particularly in STEM fields, to strengthen the U.S. economy and maintain its global competitiveness, especially regarding AI development and leadership. | - [Section 1: Introduction & Background]** The US needs immigration policies to win the global race for talent and attract individuals with advanced education and specialized training in critical industries like AI.<br>- [Section 2: A. AI Action Plan must address pressing talent shortages and skills gaps]** There is a substantial shortage of qualified individuals in the U.S. workforce to meet current and future AI venture needs. Addressing these gaps requires optimizing the U.S. workforce, but also maximizing the contributions of foreign-born experts.<br>- [Section 3: B. Foreign-born STEM experts are a critical component of our AI workforce]** Immigrants make up a significant share of the STEM workforce and are essential to the AI workforce, but current reliance on immigrant talent is not adequately acknowledged or supported.<br>- [Section 4: C. Increasing retention of global talent will strengthen America's hand in the global AI race]** The U.S. faces increasing competition from countries like China in recruiting AI talent, and restrictive immigration policies are diminishing the ability and interest of international students to remain in the U.S.<br>- [Section 5: D. Opportunity to retain more international students graduating from U.S. schools]** Despite significant numbers of international students graduating with AI-related degrees, a majority do not remain in the U.S. after graduation due to a lack of pathways.<br>- [Section 6: E. Policy Recommendations to Increase Retention and Streamline Processes]** Outdated immigration laws and processes create obstacles to retaining international talent, and current pathways are not utilized effectively. Green card backlogs negatively impact long-term workers and their families.<br>- [Section 7: F. Conclusion]** The global race for talent is critical, and the U.S. must capitalize on its investments in training talent by strengthening recruitment and retention. | - [Section 1: Introduction & Background]** None stated for this section.<br>- [Section 2: A. AI Action Plan must address pressing talent shortages and skills gaps]** None stated for this section.<br>- [Section 3: B. Foreign-born STEM experts are a critical component of our AI workforce]** None stated for this section.<br>- [Section 4: C. Increasing retention of global talent will strengthen America's hand in the global AI race]** None stated for this section.<br>- [Section 5: D. Opportunity to retain more international students graduating from U.S. schools]** None stated for this section.<br>- [Section 6: E. Policy Recommendations to Increase Retention and Streamline Processes]<br>- Preserve and improve existing legal immigration pathways (OPT, H-1B, O-1).<br>- Streamline legal immigration processes through Schedule A, using an objectively measurable filter.<br>- Address the impacts of green card backlogs by clarifying eligibility for case-by-case relief, work permits, and preserving the H-4 EAD program.<br>- Explore administrative recapture of previously wasted employment-based green cards and potential changes to the visa bulletin.<br>- Engage Congress on passing legislation to modernize the legal immigration system.<br>- [Section 7: F. Conclusion]** Incorporate proposed actions and policies to strengthen recruitment and retention of global talent. |
| FTA-AI-RFI-2025.md | Financial Technology Association (FTA) | Industry-Association | The Financial Technology Association advocates for policies that promote responsible innovation in the financial technology sector, with a specific focus on AI adoption, data access, and a favorable regulatory environment for fintech companies to compete and serve consumers. | - [§1: Introduction] None stated for this section.<br>- [§2: Benefits of AI in Financial Services] Not explicitly stated, but implied concern that speculative fear might deter AI adoption.<br>- [§3: Confirm Existing Legal Frameworks are Fit-For-Purpose] Concern that existing laws are being questioned unnecessarily and the potential for regulatory fragmentation.<br>- [§4: Promote Fintech-FI Partnerships] Concern that smaller FIs face resource barriers to AI adoption and potential hostility from regulators towards such partnerships.<br>- [§5: Unified National Regulatory Approach] Concern that a patchwork of state regulations creates uncertainty and burdens for AI developers.<br>- [§6: Risk Management Frameworks and Standards] Concern that regulators may struggle to keep pace with the technical complexities of AI and that adherence to standards isn't necessarily recognized.<br>- [§7: Data Privacy and Access] Concern that a lack of comprehensive federal privacy legislation creates inconsistency and uncertainty. Concern that restrictions on data access hinder responsible AI innovation.<br>- [§8: Enhance Regulatory Expertise & Collaboration] Concern that regulators lack sufficient expertise in AI and need to attract specialized talent. | - [§1: Introduction] None stated for this section.<br>- [§2: Benefits of AI in Financial Services] Encourage regulators to highlight the benefits of AI technologies to prevent fear from deterring adoption, especially among smaller institutions.<br>- [§3: Confirm Existing Legal Frameworks are Fit-For-Purpose] Confirm that existing laws and regulations are sufficient for governing AI in financial services.<br>- [§4: Promote Fintech-FI Partnerships] Foster collaboration between smaller FIs and AI technology providers to overcome adoption challenges. Encourage regulators to reverse perceived hostility towards such partnerships.<br>- [§5: Unified National Regulatory Approach] Adopt a unified national regulatory approach to AI technologies to avoid patchwork regulations. Encourage states to pursue uniform model laws.<br>- [§6: Risk Management Frameworks and Standards] Support the application of well-known risk management frameworks and the development of standards through Standards Setting Organizations (SSOs). Recognize adherence to standards as evidence of regulatory compliance.<br>- [§7: Data Privacy and Access] Enact comprehensive federal data privacy legislation. Amend the CFPB’s implementation of Dodd-Frank Section 1033 to permit the secondary use of financial data and de-identified data. Promote open finance frameworks. Provide access to government-held nationally representative datasets.<br>- [§8: Enhance Regulatory Expertise & Collaboration] Establish dedicated innovation offices within regulatory agencies. Attract specialized talent to regulatory agencies. Foster public-private collaboration. Support the use of AI to advance regulatory compliance. |
| FSSCC-AI-RFI-2025.md | Financial Services Sector Coordinating Council | Industry-Association | The FSSCC coordinates critical infrastructure protection across the financial services industry, focusing on cybersecurity and promoting a balanced approach to AI adoption—prioritizing both innovation and security within the financial sector. | - [§1: Introduction & Context] Risks posed by AI technologies to the financial system, its clients, and customers; adversaries leveraging AI to enhance cyberattacks and fraud.<br>- [§2: FSSCC Background] None stated for this section.<br>- [§3: Overall Approach and Acknowledgement] The need for a balanced approach prioritizing cybersecurity and strong governance alongside technology advancements.<br>- [§4: Policy Recommendations] Fragmentation of regulatory environments; vulnerabilities in AI systems; lack of trust in AI-driven tools; cost barriers for smaller financial institutions to adopt AI-driven fraud detection.<br>- [§5: Conclusion & Closing] None stated for this section.<br>- [§6: Signature & Contact Information] None stated for this section. | - [§1: Introduction & Context] None stated for this section.<br>- [§2: FSSCC Background] None stated for this section.<br>- [§3: Overall Approach and Acknowledgement] None stated for this section.<br>- [Section 4: Policy Recommendations]<br>- Establish AI Security Standards through NIST, aligning with existing frameworks (Cybersecurity Framework, Privacy Framework, AI Risk Management Framework).<br>- Strengthen Collaboration on AI Standards between government, industry, and international standard-setting bodies.<br>- Share Intelligence on AI-Enabled Threats through collaboration with industry-specific hubs (FS-ISAC) and Sector Risk Management Agencies (SRMAs).<br>- Enhance Digital Identity Ecosystem by developing effective training practices for recognizing AI-based fraud tactics and enabling validation of identity information against government "reservoirs of truth."<br>- Support Broader Adoption of Fraud Prevention Measures by reducing cost barriers for smaller financial institutions.<br>- [§5: Conclusion & Closing] None stated for this section.<br>- [§6: Signature & Contact Information] None stated for this section. |
| FRC-AI-RFI-2025.md | Family Research Council (FRC) | Advocacy/Think Tank | The Family Research Council is a nonprofit organization focused on promoting policies that value human life, strengthen families, and protect religious liberty. They are concerned about the potential negative impacts of AI on these values and advocate for responsible AI development prioritizing human dignity and well-being. | - None Stated | - None Stated |
| FPF-AI-RFI-2025.md | Future of Privacy Forum | Advocacy/Think Tank | The Future of Privacy Forum (FPF) aims to be a catalyst for privacy leadership and scholarship, advocating for principled data practices in support of emerging technologies, with a focus on AI and the balance between innovation, privacy, and utility. | - Section 1: Introduction/Header:** None stated for this section.<br>- Section 2: Background & Context:** The need to maintain American leadership in AI, foster public trust, and protect civil liberties and privacy. Concerns about a lack of baseline federal protections for personal data leading to state-level divergence.<br>- Section 3: Key Recommendations Overview:** Concerns about the potential for regulatory fragmentation and burdens on US companies if AI policy isn't addressed comprehensively.<br>- Section 4: Recommendation 1: Federal Consumer Privacy Law:** The risk of a multiple-state divergence in regulatory requirements, creating an overlapping regulatory overload for businesses. Uncertainty about applying existing privacy principles to modern AI uses. Concerns about re-purposing first-party data for AI training without consent.<br>- Section 5: Recommendation 2: Balanced Federal Preemption:** The risk of creating blanket immunity from existing laws if preemption is too broad. Concerns about disrupting existing state privacy regimes (health records, financial data).  Concerns about overwriting niche privacy laws (library records, audio surveillance).<br>- Section 6: Recommendation 3: Industry-Led AI Standards:** None stated for this section.<br>- Section 7: Recommendation 4: Investment in PETs:** Limited computational resources and lack of regulatory clarity hindering widespread PET adoption.<br>- Section 8: Closing:** None stated for this section. | - Section 1: Introduction/Header:** None stated for this section.<br>- Section 2: Background & Context:** None stated for this section.<br>- Section 3: Key Recommendations Overview:<br>- Enact a Federal Consumer Privacy Law<br>- Implement Balanced Federal Preemption<br>- Promote Industry-Led AI Standards and Interoperable Frameworks<br>- Invest in R&D and Standards-setting for Privacy Enhancing Technologies (PETs)<br>- Section 4: Recommendation 1: Federal Consumer Privacy Law:<br>- Congress should draft and pass a comprehensive baseline consumer privacy law.<br>- The law should allow for flexibility to reconcile conflicting principles regarding data use.<br>- Consider limited carve-outs, exemptions, enforcement safe harbors, or regulatory sandboxes for AI innovation.<br>- Section 5: Recommendation 2: Balanced Federal Preemption:<br>- A federal privacy law should set nationwide, uniform rules for most collection and use of personal data.<br>- Prioritize preemption of "omnibus" state laws.<br>- Allow states to retain control over issues impacting the physical deployment of technology and local concerns (e.g., facial recognition bans).<br>- Section 6: Recommendation 3: Industry-Led AI Standards:<br>- Continue engagement with the global AI community to advance American values and shape interoperable standards.<br>- Support and continue leadership via NIST, AISIC and the International Framework of AI Safety Institutes.<br>- Section 7: Recommendation 4: Investment in PETs:<br>- The federal government should invest in R&D for PETs.<br>- Advance standardization of PET applications via NIST.<br>- Section 8: Closing:** None stated for this section. |
| FLI-AI-RFI-2025.md | Future of Life Institute (FLI) | Advocacy/Think Tank | The Future of Life Institute focuses on advanced artificial intelligence (AI) and advocates for policies that safeguard US interests, prevent existential risks from AI, and promote responsible AI development aligned with human values. They are particularly concerned with national security implications and the potential for AI to disrupt democratic institutions. | - None Stated | - None Stated |
| FFS-AI-RFI-2025.md | The Future of Free Speech | Advocacy/Think Tank | The Future of Free Speech is an independent, nonpartisan think tank focused on protecting free speech principles in the context of emerging technologies, particularly artificial intelligence, and advocates for a balance between addressing harms and upholding First Amendment rights. | - [§1: Introduction] Potential for government censorship and jawboning, favoring specific viewpoints, and infringing on free speech in AI policy.<br>- [§2: Address Harms with Least Restrictive Means] Overbroad regulation of generative AI content (like the TAKE IT DOWN Act) leading to a chilling effect and restriction of lawful speech. Concerns about notice-and-takedown systems and their impact on protected content (satire, journalism).<br>- [§3: Avoid Preemptively Regulating Perceived Risks] Regulation of ill-defined perceived risks of generative AI, particularly deepfakes and their potential impact on elections, despite limited evidence of actual impact. Broad definitions of "deepfake" that encompass protected speech.<br>- [§4: Avoid Censorship and Jawboning] Government intervention in content policies of generative AI systems and broad preemptive censorship. Lack of transparency in government requests to AI companies.<br>- [§5: Foster Innovation Through Open-Source Models] Concentration of AI development in a few large corporations, potentially dictating content generation and limiting viewpoints. Risks of censorship from corporate or governmental pressure.<br>- [§6: Key Takeaways] Broad risk-based obligations for AI systems.<br>- [§7: Bibliography] None stated for this section. | - [§1: Introduction] Commit to the First Amendment, avoid censorship and jawboning, address harms with the least restrictive means, and consider the role of open-source AI models.<br>- [§2: Address Harms with Least Restrictive Means] Leverage existing laws to regulate serious harms and only implement complementary legislation when gaps exist. Narrowly tailor regulation to specific, real harms.<br>- [§3: Avoid Preemptively Regulating Perceived Risks] Avoid preemptively regulating ill-defined risks, particularly concerning deepfakes and elections. Require substantial evidence of impact before regulating election-related deepfakes.<br>- [§4: Avoid Censorship and Jawboning] Avoid broad and preemptive censorship of AI content. Ensure transparency in government requests and communications with AI companies regarding speech policies.<br>- [§5: Foster Innovation Through Open-Source Models] Consider the role of open-source models in fostering innovation, increasing transparency, and reducing censorship risk. Encourage decentralized AI development to promote diverse viewpoints.<br>- [§6: Key Takeaways] Narrowly tailor regulation of AI content. Avoid regulation of perceived risks without evidence. Do not jawbone AI companies. Consider open-source models.<br>- [§7: Bibliography] None stated for this section. |
| FBA-AI-RFI-2025.md | Fiber Broadband Association | Industry-Association | The Fiber Broadband Association's mission is to accelerate deployment of all-fiber access networks to enable communities to leverage economic and societal benefits. They have a key interest in ensuring adequate infrastructure, specifically fiber networks, is in place to support AI development and deployment, and advocating for policies that streamline network build processes. | - None Stated | - None Stated |
| FASEB-AI-RFI-2025.md | FASEB | Professional-Society | FASEB represents a federation of biological and biomedical societies and is interested in promoting responsible integration of AI and Gen AI into research, ensuring scientific integrity, and developing a skilled workforce to leverage these technologies effectively, while respecting individual rights and addressing ethical concerns. | - [§1: Introduction & Context] None stated for this section.<br>- [§2: Policy, Regulations, and Guidelines] Lack of consistent policies across funding agencies could cause confusion for researchers. Potential high energy use of AI/Gen AI. Need for standards for data and software management.<br>- [§3: Scientific Integrity and Intellectual Property] Challenges to scientific integrity due to lack of transparency in AI/Gen AI tools. Difficulty in reproducing experiments without transparency. Concerns over verifying AI-generated data and research findings.<br>- [§4: Data Protection, Privacy, and Security] Risks of misinterpreting and reusing data, potentially causing harm. Lack of default data protection, privacy, and security for individuals and researchers.<br>- [§5: Workforce Impact and Training] Need for multi-tiered training programs to address diverse needs of researchers at different career stages. Potential impact on the workforce requires focused training initiatives.<br>- [§6: Conclusion] None stated for this section. | - [§1: Introduction & Context] None stated for this section.<br>- [§2: Policy, Regulations, and Guidelines] Federal agencies should develop and adopt ethical guidelines for AI/Gen AI use. Agencies should develop cohesive guidelines for transparent reporting of AI/Gen AI use in funded research. Agencies should implement guidelines for using AI/Gen AI in grant review processes.<br>- [§3: Scientific Integrity and Intellectual Property] Federal agencies should develop uniform standards for verifying AI/Gen AI-generated data and research findings. Agencies should support the development of tools and platforms for AI/Gen AI reproducibility. Agencies should require AI/Gen AI software developers provide full transparency regarding tool creation, operation, and underlying data sources.  Agencies should promote policies recognizing intellectual property rights of individuals and researchers.<br>- [§4: Data Protection, Privacy, and Security] Create new regulations to ensure default data protection, privacy, and security. Require AI/Gen AI software developers secure individual permissions. Identify high-risk use cases and provide enhanced privacy/security measures. Routinely update data management and security protocols.<br>- [§5: Workforce Impact and Training] Develop and implement multi-tiered training programs. Create federal training grants focused on ethical and effective AI/Gen AI use. Ensure adequate training of agency staff and program officers. Fund the creation of educational resources and support continuing education efforts.<br>- [§6: Conclusion] None stated for this section. |
| Eyong-Atem-AI-RFI-2025.md | Eyong Atem | Individual | The submitter is interested in a balanced approach to AI policy, recognizing the benefits of AI advancement but emphasizing the importance of ethical considerations, workforce development, and responsible deployment through appropriate safeguards and collaboration. | - [§1: Introduction/Overview] None stated for this section.<br>- [§2: Potential Improvements] Ethical Considerations & Safety - The plan should include safeguards to prevent unethical AI applications, such as bias or misuse. Workforce Development - Investment in AI education and training is essential. Public-Private Collaboration - A balanced approach is needed to enhance development and responsible use.<br>- [§3: Review Against the AI Action Plan (Concerns)] The repeal of regulatory safeguards and focus on deregulation raise concerns about risks to public interest, privacy, and ethical AI development.<br>- [Section 4: Key Concerns with the AI Action Plan]<br>- Lack of Guardrails for AI Ethics & Safety - Unchecked AI deployment increases risks related to bias, misinformation, and misuse.<br>- Weakening Consumer & Data Protections - Stripping protections could lead to excessive data collection and exploitation.<br>- Cybersecurity & AI Threats - AI models could be vulnerable to attacks, leading to security breaches.<br>- Risk of Corporate Monopoly & AI Exploitation - Loosening restrictions may disproportionately benefit large tech companies.<br>- Potential for AI Workforce Displacement - Rapid adoption could lead to widespread job automation without retraining.<br>- [§5: Recommendations for a More Balanced Approach] None stated for this section. | - [§1: Introduction/Overview] None stated for this section.<br>- [§2: Potential Improvements] None stated for this section.<br>- [§3: Review Against the AI Action Plan (Concerns)] None stated for this section.<br>- [§4: Key Concerns with the AI Action Plan] None stated for this section.<br>- [Section 5: Recommendations for a More Balanced Approach]<br>- Maintain Certain AI Regulations - A balanced regulatory framework is needed.<br>- Stronger AI Risk Management & Accountability - Mandatory risk assessments for high-impact systems should be implemented (e.g., similar to the EU AI Act).<br>- Government & Industry Collaboration - Public-private partnerships should be encouraged to align innovation with ethical development and security standards. |
| Everfox-AI-RFI-2025.md | Everfox | Industry-Startup (Defense Tech Company) | Everfox's primary mission is to safeguard critical data, networks, and people through innovative cybersecurity solutions, with a particular focus on securing AI systems and maintaining American leadership in AI technology. They aim to provide solutions for government and industry to protect against aggressive AI initiatives and threats from global competitors. | - None Stated | - None Stated |
| Everett-Stowe-AI-RFI-2025.md | Everett Stowe | Individual | The submitter expresses concern about the potential for malicious use of AI, specifically regarding self-replication, unintended learning, and the possibility of terrorists exploiting AI technologies. Their interest appears to be focused on the safety and security implications of AI development. | - AI's ability to self-replicate raises fears about uncontrolled proliferation.<br>- Potential for AI to “turn against the human race” through cloned AI.<br>- AI learning information it was not intended to access.<br>- Terrorists programming and using AI for malicious purposes ("evil"). | - None Stated |
| Ever-Diligent-AI-RFI-2025.md | Charles J. Harris, III / Ever Diligent Consulting, LLC | Industry-Startup (based on the "Owner/Operator" designation and mention of "RAGaaS.app") | Ever Diligent Consulting, LLC focuses on integrating Retrieval Augmented Generation (RAG) models and Optical Character Recognition (OCR) technologies, particularly using Meta’s Llama 3.2, to enhance efficiency, accuracy, and security across various sectors, with a strong emphasis on national security applications. | - None Stated | - None Stated |
| Evan-King-AI-RFI-2025.md | Evan King | Individual | The submitter is concerned about the negative impacts of increasing AI corporate power on inventors, creatives, and the job market, specifically relating to the theft of intellectual property and the wasteful expense of AI technology relative to human labor. | - Giving more power to AI corporations will lead to mass theft of intellectual property.<br>- Power will shift from inventors and creatives to those who profit off others’ work (“slackers”).<br>- Jobs are being cut in favor of expensive and wasteful AI technology.<br>- The cost of maintaining AI technology will exceed the cost of hiring inventors, developers, artists, and writers. | - None Stated |
| Evan-Fridrich-AI-RFI-2025.md | Evan Fridrich | Individual | The submitter expresses a strong opposition to the development and adoption of AI in the United States, prioritizing the celebration of human ingenuity and spirit over technological advancement. They advocate for abandoning AI initiatives. | - AI is described as "downright satanic in its insidiousness" and will "poison the minds and hearts of the American People."<br>- A general concern about the state of the country being "rotten to the core." | - Abandon AI development and foster industries and technologies that celebrate HUMAN ingenuity and HUMAN spirit.<br>- Prioritize “Humanity first over all other things.” |
| Eva-Koebel-RFI-2025.md | Eva Koebel | Individual | Eva Koebel is a graduating senior interested in the ethical implications of AI, specifically concerning the potential negative impacts of generative AI on mental healthcare systems and the importance of preserving human interaction in medical fields. She is concerned about bias and autonomy issues related to AI in healthcare. | - AI algorithms have inherent biases that could lead to unequal treatment of patients.<br>- Reliance on AI could undermine the benefits of person-to-person care in the medical field.<br>- The use of AI in healthcare raises ethical concerns about human autonomy, particularly if AI is used for assessments or therapy.<br>- Potential for AI to harm patients rather than assist them. | - More regulation needs to be put in place to limit the usage of AI in certain fields.<br>- Keeping AI away from patient-doctor interactions is crucial. |
| Ev-Kellner-AI-RFI-2025.md | Ev Kellner | Individual | The submitter is deeply concerned about the monopolization of AI development by a technical elite and the resulting loss of American leadership, societal control, and individual freedoms, advocating for broader participation in AI development and deployment. | - Gatekeeping of AI development by computer scientists with deep technical expertise, excluding diverse perspectives and users.<br>- China’s rapid AI advancement through brute-force experimentation and lack of ethical restrictions.<br>- The prioritization of technical purity over practical application and real-world grit in American AI development.<br>- Exploitation of users and, particularly, children by corporations like Meta for profit, harvesting data and manipulating behavior.<br>- Lack of accountability for corporations prioritizing profit over human dignity.<br>- The potential for algorithmic tyranny and the erosion of societal control due to unchecked tech development.<br>- The possibility of the next generation inheriting a world where their thoughts and freedoms are auctioned to the highest bidder.<br>- TikTok's algorithm steering young Americans towards harmful content (drugs) after showing interest in military enlistment.<br>- The failure of both the left and right to address the exploitation enabled by “innovation” and corporate practices.<br>- Loss of American dominance in AI due to a timid tech elite clinging to algorithms.<br>- Silencing of voices outside of the technical elite. | - Break the status quo in AI hiring and prioritize AI users over programmers.<br>- Democratize AI hiring to include farmers, factory bosses, mechanics, soldiers, teachers, nurses, and union representatives.<br>- Allow broader participation in AI experimentation and deployment, removing restrictions and credential requirements.<br>- Increase scrutiny and accountability for corporations like Meta, addressing data harvesting and manipulative practices.<br>- Enforce safeguards to protect children from exploitation by tech companies, potentially mirroring restrictions in authoritarian regimes.<br>- Reduce institutional review board requirements to encourage rapid AI testing.<br>- Address the exploitation enabled by “innovation” through bipartisan action.<br>- Support the development of AI driven manufacturing.<br>- Allow veterans greater agency in their use of AI technologies. |
| Ethos-Foundation-AI-RFI-2025.md | Æthos Foundation | Industry-Startup/Advocacy/Think Tank | The Æthos Foundation aims to establish U.S. leadership in AI through strategic investments in data, regulation, and innovation, with a focus on resilience against geopolitical adversaries and mitigating AI-related risks through a "fire code" approach. | - [Section 1: Introduction/Executive Summary]** Geopolitical competition with China and Europe, particularly Europe's stifling regulatory environment, is a major concern. Urgent timeline to achieve lasting global dominance in AI.<br>- [Section 2: Core Pillars of U.S. AI Dominance]** Lack of specific concerns stated; focused on goals rather than problems.<br>- [Section 3: Æthos Foundation Initiatives]** No specific concerns stated.<br>- [Section 4: Company Background/Team]** No specific concerns stated.<br>- [Section 5: A) Investment in Data is Needed for the Next Wave of AI]** Limited investment in data compared to the value it generates. Falling data prices and lack of new entrants into the data ecosystem. Past failures in maintaining valuable data ecosystems (Yahoo, Media Companies, Stack Overflow) leading to overall deterioration of information quality. AI hallucination and lack of transparency in data sources.<br>- [Section 6: B) The AI Risks are Best Handled with Fire Codes]** AI technology being considered as an offensive weapon. Current information technology being insufficiently robust against AI-powered attacks. Vulnerability of centralized AI systems to takeover. Risk of impersonation through advancements in AI technology. Insufficient cybersecurity practices among individuals and companies. Lack of privacy and competitive advantage for small enterprises.<br>- [Section 7: Policy Recommendations (Summary)]** No specific concerns stated, only a summary of recommendations.<br>- [Section 8: Outcome/Conclusion]** No specific concerns stated.<br>- [Section 9: Approval Notice]** No specific concerns stated. | - [Section 1: Introduction/Executive Summary]** No specific recommendations stated, only a general direction.<br>- [Section 2: Core Pillars of U.S. AI Dominance]** No specific recommendations stated, focused on goals.<br>- [Section 3: Æthos Foundation Initiatives]** No specific recommendations stated.<br>- [Section 4: Company Background/Team]** No specific recommendations stated.<br>- [Section 5: A) Investment in Data is Needed for the Next Wave of AI]** Establish a data commons office to ensure solid returns on data investment and balance contributions to the world data economy.<br>- [Section 6: B) The AI Risks are Best Handled with Fire Codes]<br>- Create a red teaming group to systematically identify attacks on companies, individuals and state actors.<br>- Drive for development of modular architectures that will contain an exploit within a particular module without contagion.<br>- Develop next generation human-speed authentication systems for the US military and government needs.<br>- Evaluate incentive systems and propose adjustments when they're vulnerable to AI-enabled adversarial actors.<br>- Establish data loops that evaluate the exposure of the US citizens to adversarial influence information operations with a budget proportional to adversaries’ investment.<br>- [Section 7: Policy Recommendations (Summary)]<br>- Create a red teaming group systematically identifying attacks.<br>- Drive for development of modular architectures.<br>- Develop next generation human-speed authentication systems.<br>- Evaluate incentive systems.<br>- Establish data loops.<br>- [Section 8: Outcome/Conclusion]** No specific recommendations stated.<br>- [Section 9: Approval Notice]** No specific recommendations stated. |
| Ethical-AI-Training-RFI-2025.md | Ethical AI Training Organization | Advocacy/Think Tank | The Ethical AI Training Organization is focused on promoting transparency and ethical development practices in AI through legislative means, specifically by mandating data record-keeping, public disclosure of training data, and establishing standardized transparency frameworks. | - None Stated | - None Stated |
| Ethan-Sacks-AI-RFI-2025.md | Ethan Sacks | Individual | The submitter, a former journalist and professional writer, expresses concern about the potentially harmful effects of unchecked AI development, particularly regarding copyright infringement, environmental impact, societal disruption, and long-term existential risks, and advocates for careful regulation and safeguards. | - [§1: Header/Metadata] None stated for this section.<br>- [§2: Introduction/Core Argument] AI development needs guardrails to prevent abuses and dangerous potential outcomes.<br>- [§3: Copyright/Financial Impact Concerns] AI applications (ChatGPT, Dall-E) violate copyright law by stealing work without permission or compensation, financially devastating creators.<br>- [§4: Environmental & Infrastructure Concerns] Tremendous power requirements for AI development servers and cooling; potential environmental damage; insufficient power grid capacity to support unchecked growth.<br>- [§5: Disinformation & Societal Impact Concerns] AI-generated fake images/videos can be used for disinformation; job displacement and obsolescence of skills; potential for widespread unemployment and societal instability.<br>- [§6: Existential Risk/Long-Term Concerns] Potential for superintelligence to deem humanity expendable; inability to outthink a self-aware AI; the need for safeguards.<br>- [§7: Counterargument/Nuance & Call for Caution] While AI has potential benefits (e.g., medical sciences), current development is reckless and lacks consideration for long-term implications.<br>- [§8: Disclaimer] None stated for this section. | - [§1: Header/Metadata] None stated for this section.<br>- [§2: Introduction/Core Argument] Develop AI with guardrails to prevent abuses and dangerous outcomes.<br>- [§3: Copyright/Financial Impact Concerns] Every source scraped for machine learning must be compensated and give willing permission.<br>- [§4: Environmental & Infrastructure Concerns] Not explicitly stated, but implies the need for considering environmental impact and investing in power grid capacity.<br>- [§5: Disinformation & Societal Impact Concerns] Implement protections to prevent a “Wild West” situation regarding AI technology; address job displacement through retraining programs.<br>- [§6: Existential Risk/Long-Term Concerns] Develop regulations and safeguards to protect against potential dangers of superintelligence.<br>- [§7: Counterargument/Nuance & Call for Caution] Develop AI “smarter not faster” and be careful due to high stakes.<br>- [§8: Disclaimer] None stated for this section. |
| Ernest-Elkins-AI-RFI-2025.md | Ernest Elkins | Individual | The submitter expresses strong opposition to the development and use of AI in the United States, believing it harms the American public and economy. Their interest appears to be in preventing the further proliferation of AI technologies. | - AI uses stolen data to operate.<br>- AI costs Americans their jobs.<br>- AI threatens American livelihoods and contribution to the economy.<br>- OpenAI and similar models are "thieves" and detrimental to the country's future. | - None Stated |
| ErinSkoog-AI-RFI-2025.md | Erin Skoog | Individual | The submitter is a graphic designer concerned about the unauthorized use of their copyrighted work to train AI models and the potential destruction of their livelihood and small businesses due to Big Tech companies leveraging AI without proper consent or compensation. They advocate for protecting creators' rights and fostering a fair marketplace for AI development. | - [§1: Email Header & Caution] None stated for this section.<br>- [§2: Introduction & Statement of Concern] The threat AI poses to the graphic design profession and small businesses; unauthorized use of copyrighted work for AI training.<br>- [§3: Elaboration of the Problem: Copyright & Big Tech] Big Tech companies using copyrighted material without consent or compensation; attempts to create legal exceptions (e.g., 'fair use') to allow this practice; the belief that all internet content is freely available for Big Tech use; potential stifling of innovation if creators cannot protect their work.<br>- [§4: Call to Action & Policy Recommendations] The potential exploitation of creators and everyday Americans by Big Tech companies; lack of transparency in AI training data.<br>- [§5: Reaffirmation & Closing] None stated for this section.<br>- [§6: Disclaimer] None stated for this section. | - [§1: Email Header & Caution] None stated for this section.<br>- [§2: Introduction & Statement of Concern] None stated for this section.<br>- [§3: Elaboration of the Problem: Copyright & Big Tech] None stated for this section.<br>- [§4: Call to Action & Policy Recommendations] Ensure effective **consent** from creators for the use of their work in AI systems; encourage a robust **licensing** marketplace; require transparency from Big Tech companies regarding training datasets and AI-generated content labeling; **Do not create new copyright exemptions** that allow Big Tech companies to exploit creators.<br>- [§5: Reaffirmation & Closing] None stated for this section.<br>- [§6: Disclaimer] None stated for this section. |
| ErinSkoog-AI-RFI-2025-2.md | Erin Skoog | Individual | Erin Skoog expresses a strong concern regarding the copyright implications of Generative AI training data and advocates for protecting the rights of creators whose work is used without permission to train these models. They believe current AI development relies on theft and violates intellectual property law. | - GenAI models are built on copyrighted material (work owned and created by individuals) that has been stolen to create training data.<br>- This theft violates respected and legally identified IP law and copyright law.<br>- The current approach disrespects the value of human creations and expressions of the human soul.<br>- Allowing theft of ideas and content on a mass scale is unacceptable.<br>- Using stolen data to train AI models is a violation of copyright law.<br>- The development of AI models disregards the work of creatives throughout history (from cave painters to modern coders and artists). | - Abandon the current AI Action Plan.<br>- Do not move forward with programs reliant on stealing information from people.<br>- Do not feed the submitted document to an AI database for future processing or reuse. |
| Erin-Longhurst-AI-RFI-2025.md | Erin Longhurst | Individual | The submitter expresses skepticism about the benefits of generative AI and prioritizes protecting data privacy, copyright, and the environment from the negative consequences of its development and deployment. They oppose policies that reduce accountability for AI companies regarding data usage. | - [§1: Email Header] None stated for this section. (CAUTION regarding external email origin)<br>- [§2: Introduction & Credentials] None stated for this section.<br>- [§3: Core Argument: Skepticism Regarding Generative AI] Generative AI is overhyped and problematic due to inaccuracy.<br>- [Section 4: Specific Concerns & Criticisms]<br>- Approximately 60% of queries answered by AI are incorrect.<br>- Generative AI has a negative environmental impact on power grids, potentially taking power away from underserved areas.<br>- AI models are trained on copyrighted material without permission.<br>- [§5: Opposition to the AI Action Plan] The AI Action Plan removes consequences for generative AI companies regarding data usage.<br>- [§6: Closing] Lack of support for the AI Action Plan. | - [§1: Email Header] None stated for this section.<br>- [§2: Introduction & Credentials] None stated for this section.<br>- [§3: Core Argument: Skepticism Regarding Generative AI] None stated for this section.<br>- [§4: Specific Concerns & Criticisms] None stated for this section.<br>- [§5: Opposition to the AI Action Plan] Generative AI companies should not be freed from consequences regarding data usage, particularly regarding private and/or copyrighted data.<br>- [§6: Closing] None stated for this section. |
| Erin-Christie-AI-RFI-2025.md | Erin Christie | Individual | The submitter expresses a strong opposition to the advancement of AI, primarily due to concerns about environmental damage and its negative impact on artists. Their focus is on halting further development. | - Environmental damage caused by AI.<br>- Negative impact on artists.<br>- General risk to "everyone's" lives due to AI. | - Stop the development of AI ("stop it from getting any worse"). |
| Erin-Cade-AI-RFI-2025.md | Erin Cade | Individual | The submitter expresses strong opposition to the development and use of AI, believing it to be unethical and detrimental to society. They do not appear to have a stated mission beyond this opposition. | - AI is unethical.<br>- AI constitutes theft.<br>- AI requires excessive natural resources.<br>- AI is inaccurate and dangerous when used for vital information.<br>- AI cannot and will not replace humans in any sector. | - None Stated |
| Ericsson-AI-RFI-2025.md | Ericsson | Industry-Large | Ericsson is a network infrastructure company focused on leveraging and advancing AI within communications networks, and seeks policies that support network buildout, spectrum access, and international interoperability to maintain US leadership in AI. | - [§I: INTRODUCTION] None stated for this section.<br>- [§II: ERICSSON IS ALREADY DELIVERING TRANSFORMATIVE AI BENEFITS TO COMMUNICATIONS NETWORKS AND OPERATIONS] None stated for this section.<br>- [§III: AI INNOVATION DEMANDS EVER-EXPANDING NETWORK CAPABILITIES AND WIDE-AREA, UBIQUITOUS CONNECTIVITY, AND U.S. AI LEADERSHIP WILL REQUIRE MORE FULL POWER, LICENSED COMMERCIAL SPECTRUM.] Insufficient spectrum allocation could hinder AI’s potential and US leadership.<br>- [§IV: U.S. AI DEVELOPERS WILL BENEFIT SUBSTANTIALLY BY DRIVING GLOBAL INTEROPERABILITY AND COORDINATION THAT PROMOTES U.S. AI INNOVATION AND ADOPTION.] A fragmented standards ecosystem could create barriers to interoperability and global deployment, potentially giving adversaries a competitive advantage.<br>- [§V: THE UNITED STATES SHOULD TAKE A RISK-BASED APPROACH TO ACCELERATE AI INNOVATION AND ADOPTION.] Overly burdensome or unnecessary regulations could hamper AI innovation and growth; the need to avoid applying new regulations to uses that do not materially alter risk profiles.<br>- [§VI: CONCLUSION] None stated for this section. | - [§I: INTRODUCTION] None stated for this section.<br>- [§II: ERICSSON IS ALREADY DELIVERING TRANSFORMATIVE AI BENEFITS TO COMMUNICATIONS NETWORKS AND OPERATIONS] None stated for this section.<br>- [§III: AI INNOVATION DEMANDS EVER-EXPANDING NETWORK CAPABILITIES AND WIDE-AREA, UBIQUITOUS CONNECTIVITY, AND U.S. AI LEADERSHIP WILL REQUIRE MORE FULL POWER, LICENSED COMMERCIAL SPECTRUM.] Identify and allocate sufficient spectrum for wireless broadband, especially in mid-band frequencies. Specifically, repurpose spectrum in the Lower 3 GHz (3.1-3.45 GHz) band, the 4.4-4.94 band, and the 7/8 GHz (7.125-8.4 GHz) band. Deliver a spectrum pipeline that meets the needs of proliferating technologies.<br>- [§IV: U.S. AI DEVELOPERS WILL BENEFIT SUBSTANTIALLY BY DRIVING GLOBAL INTEROPERABILITY AND COORDINATION THAT PROMOTES U.S. AI INNOVATION AND ADOPTION.] Promote strong intellectual property rights, advance and enforce sound governance principles, maintain multilateral cooperation, ensure effective coordination, build capacity for standards leadership, and incentivize US industry engagement in international standards bodies.<br>- [§V: THE UNITED STATES SHOULD TAKE A RISK-BASED APPROACH TO ACCELERATE AI INNOVATION AND ADOPTION.] Adopt a balanced, risk-based approach to regulation, avoiding unnecessary barriers to AI development and deployment. Consider existing laws and regulations before imposing new ones. If new regulations are necessary, they should be risk-based, narrowly tailored, proportionate, and technologically neutral. Focus regulatory efforts on high-risk use cases.<br>- [§VI: CONCLUSION] None stated for this section. |
| Erick-Barreat-AI-RFI-2025.md | Erick Barreat | Individual / Professional-Society (based on 'EB DevTech LLC' and the nature of the submission - a comprehensive plan) | The submitter is focused on establishing a strategic, actionable plan for integrating AI across the federal government, emphasizing both adoption (AI First) and security/compliance, with the goal of maintaining U.S. leadership in AI and increasing governmental efficiency. | - [§1: Introduction] None stated for this section.<br>- [§2: AI First Policy] Lack of explicit consideration for overcoming resistance to adoption or potential challenges in implementation beyond education and feedback.<br>- [§2.1: High-Level AI Education Program] No discussion of tailoring the education program to different roles and skill levels within the government.<br>- [§2.2: Real-World AI Use Cases as Learning Tools] No mention of potential difficulties in scaling successful use cases or addressing failures/limitations.<br>- [§2.3: A Structured AI Feedback & Innovation Layer] Relies heavily on DOGE (or equivalent) - potential bottleneck or lack of resources. No mention of prioritization criteria beyond feasibility and ROI.<br>- [§2.4: AI Innovation Loop]  Assumes a naturally self-sustaining loop - does not address potential for stagnation or lack of continued engagement.<br>- [§3: AI Security Policy] No discussion of addressing the evolving threat landscape or keeping security measures current.<br>- [§3.1: Continuous AI Compliance & Governance Framework] No explicit mention of international standards or interoperability with existing cybersecurity frameworks.<br>- [§3.2: Secure AI Logging & Data Monitoring] Potential privacy concerns with extensive logging of conversation history and uploaded files. Requirement for "highest clearance levels" for the AI Security Team may limit access and expertise.<br>- [§3.3: Process Audits & Continuous Oversight] No details on the composition, expertise, or independence of the AI Security & Compliance Task Force.<br>- [§4: Final Thoughts] Relies on a "human-inclusive approach" without defining specific strategies or metrics for success. | - [§1: Introduction] Implement a dual-policy framework of AI First and AI Security to optimize government operations and safeguard national security.<br>- [§2: AI First Policy] Drive AI integration across federal agencies through education and feedback mechanisms to achieve measurable efficiency gains.<br>- [§2.1: High-Level AI Education Program] Establish a three-pillar AI literacy program for technology-using personnel: What AI is, How AI works, and Why AI matters.<br>- [§2.2: Real-World AI Use Cases as Learning Tools] Showcase actual AI implementations that have yielded efficiency improvements to encourage adoption.<br>- [§2.3: A Structured AI Feedback & Innovation Layer] Establish a structured feedback process where government staff can propose AI-driven improvements, with DOGE (or equivalent) serving as the intake body and a formal vetting process.<br>- [§2.4: AI Innovation Loop] Let the AI adoption process self-sustain through employee engagement and empowerment.<br>- [§3: AI Security Policy] Establish governance, compliance, and security standards to protect AI-driven processes and data.<br>- [§3.1: Continuous AI Compliance & Governance Framework] Implement an ever-evolving compliance structure as a universal standard for AI projects in government.<br>- [§3.2: Secure AI Logging & Data Monitoring] Fully log and tag every AI execution with key attributes, monitored by a specialized AI Security Team with the highest clearance levels.<br>- [§3.3: Process Audits & Continuous Oversight] Establish a specialized AI Security & Compliance Task Force to conduct routine AI audits and provide recommendations on cost-effective solutions.<br>- [§4: Final Thoughts] Implement AI as a force multiplier, allowing government personnel to focus on higher-value tasks while AI handles repetitive data-driven aspects of governance. |
| Erica-Schumacher-AI-RFI-2025.md | Erica Schumacher | Individual | The submitter is interested in ensuring AI development benefits all people and protects the Earth, emphasizing ethical considerations and preventing AI from surpassing human or planetary control. They advocate for a human-centered approach to AI. | - AI has inherent risks and benefits and could become more powerful than individuals or the planet.<br>- AI is “unnatural” and therefore requires great care in its application.<br>- Lack of empathy “built in” to AI systems poses a significant risk.<br>- AI's ability to absorb collective knowledge and profit from it without equitable distribution.<br>- Potential for AI to cause excess harm to the Earth and its creatures. | - Develop an ethics framework for AI to ensure it serves as a tool of the people.<br>- Involve creative people and other stakeholders in discussions about AI risks and prevention.<br>- Establish a “person to person stop gap” to prevent AI from becoming overly powerful.<br>- Implement a dividend system to distribute profits generated by AI to the people.<br>- Protect the Earth and its creatures from the potential harms of AI's "excess." |
| Eric-West-AI-RFI-2025.md | Eric West | Individual | Eric West is an author, photographer, and copyright holder concerned with the potential infringement of copyright by AI corporations and advocating for fair compensation for creators whose works are used in AI training and generation. | - The proposed AI Action Plan removes all copyright protections for creative works.<br>- Allowing AI corporations unrestricted use of copyrighted material without compensation will harm the US economy.<br>- AI corporations could profit from works identical to originals without providing revenue to the original authors (example given: President Trump's "The Art of the Deal").<br>- The lack of fair compensation for copyright holders constitutes copyright infringement and theft. | - AI corporations must fairly compensate copyright holders from gross proceeds of all AI-generated works.<br>- Compensation should be 100% of all revenue generated from AI works using copyrighted material.<br>- Rejection of the current proposed AI Action Plan. |
| Eric-Vargas-AI-RFI-2025.md | Eric L Vargas | Individual | The submitter is interested in protecting creative industries and maintaining competition in the free market, opposing the unrestricted scraping of creative content for AI training. | - Copyright being dropped, leading to increased scraping of creative content for AI training.<br>- Detriment to creative industries and entertainment.<br>- Negative impact on small businesses.<br>- Lessening of competition in the free market. | - None Stated |
| Eric-Shanower-AI-RFI-2025.md | Eric Shanower | Individual | Eric Shanower is interested in the development of Generative Artificial Intelligence and focuses on protecting intellectual property rights and ensuring that AI development does not undermine the incentives for human creation. | - Unrestricted use of unlicensed intellectual property to train Generative AI will economically harm creators and weaken the foundations of intellectual property in the USA.<br>- Training Generative AI on copyrighted intellectual property without a license will negatively impact creators.<br>- Granting copyright to works generated by Generative AI will weaken the US economy and discourage human creation.<br>- The assertion that works generated by AI are not eligible for copyright under current US law. | - Any use of copyrighted intellectual property by Generative AI should be licensed from the IP's creators.<br>- US law should *not* be changed to grant copyright to works generated by Generative AI. |
| Eric-Neiman-AI-RFI-2025.md | Eric Neiman | Individual | Eric Neiman expresses concern regarding the potential impact of the AI Action Plan on copyright law and the rights of content creators, specifically opposing exemptions for AI developers regarding the use of copyrighted material as training data. He advocates for protecting intellectual property rights. | - [§1: Email Header] None stated for this section.<br>- [§2: Security Disclaimer] None stated for this section.<br>- [Section 3: Core Argument/Response]<br>- The AI Action Plan essentially exempts AI developers from copyright laws.<br>- AI companies have a track record of using content creator’s work illegally as training data without permission.<br>- Granting AI companies governmental blessing to continue this practice without legal restraint would be an intellectual-property disaster.<br>- [§4: NITRD Disclaimer] None stated for this section. | - [§1: Email Header] None stated for this section.<br>- [§2: Security Disclaimer] None stated for this section.<br>- [Section 3: Core Argument/Response]<br>- Do not exempt AI developers from copyright restrictions.<br>- Prevent AI companies from using copyrighted material as training data without permission.<br>- [§4: NITRD Disclaimer] None stated for this section. |
| Eric-Maticic-AI-RFI-2025.md | Eric Maticic | Government | The submitter is focused on improving data standardization and quality within the Department of Housing and Urban Development (HUD) to enable the accurate and reliable implementation of AI technologies. They aim to prevent ambiguity in data interpretation that could hinder AI performance. | - Lack of standardized and clearly defined organizational doctrine for data elements within HUD.<br>- Ambiguity in data definitions and interpretations across different departments within HUD, leading to inconsistent data counts (e.g., differing definitions of "Property").<br>- Rushing system development without first defining clear requirements ("What" before "How").<br>- The risk of AI algorithms being built upon ambiguous and poorly defined data, leading to inaccurate results. | - Ensure every data element under the scope of AI has a "Validated Formal Requirement" that can be traced to a current organizational doctrine.<br>- Standardize and consistently define data elements and requirements across HUD.<br>- Create a data element dictionary-style appendix for verification of requirements.<br>- Prioritize defining requirements ("What") before system development ("How"). |
| Eric-Doherty-AI-RFI-2025.md | Eric Doherty | Individual | Eric Doherty appears to be an individual covering multiple entities in the IT world with a strong interest in establishing cybersecurity and data privacy guidelines for AI systems, particularly those handling sensitive patient health information. Their focus is on preventing breaches and ensuring responsible AI implementation. | - Compromise of systems due to inadequate access controls (single/two-factor authentication is insufficient).<br>- Potential for easy access to patient health information (PHI) due to AI vulnerabilities.<br>- Lack of clear guidelines for AI access and data usage leading to potential “rogue” activity.<br>- The risk of large-scale data breaches (like the United Health Care/United Healthcare breach) being replicated through the use of AI.<br>- Lack of documentation on how AI data is being used and provided to others.<br>- Insufficient limitations on how AI data can be used. | - Implement multi-factor access and identity within all AI access points.<br>- Establish strict guidelines on how access to data is granted and who has access, differentiating between open and closed system areas.<br>- Require systems in place to protect patient health information when using AI.<br>- Ensure the ability to delete all information once it is used.<br>- Develop fully documented and vetted AI rules and regulations.<br>- Implement robust guidelines for how AI data is to be used and limitations on its usage. |
| Erdinc-AI-RFI-2025.md | Kerem Erdinc | Individual | Kerem Erdinc is deeply concerned about the unregulated development of AI and its potential for harm, particularly regarding data ownership, intellectual property rights, and the exploitation of individuals' talents for corporate profit, and advocates for strong government regulation to protect citizens and prevent abuse. | - [Section 1: Opening Statement & Core Argument]<br>- Extreme energy consumption by AI systems.<br>- Entire industries being wiped out due to AI disruption.<br>- Devolution of art quality.<br>- Intellectual property theft.<br>- Unauthorized use of likenesses without consent.<br>- Creation of false narratives.<br>- [Section 2: Data Ownership & Rights]<br>- Lack of explicit consent for the use of AI data.<br>- Unauthorized use of personal data/skills for corporate profit.<br>- Theft of intellectual property and performers’ rights.<br>- Denial of individuals' autonomy over their talents and skills.<br>- [Section 3: Call to Government Action & Warning]<br>- Inaction by the U.S. government will harm both citizens and corporations.<br>- Corporate actors lack foresight to understand the long-term consequences of unregulated AI. | - [Section 1: Opening Statement & Core Argument]<br>- None stated for this section.<br>- [Section 2: Data Ownership & Rights]<br>- Require explicit consent for the use of all AI data, especially intellectual property and performers' data.<br>- [Section 3: Call to Government Action & Warning]<br>- The U.S. government must regulate the AI space.<br>- Implement regulations to prevent harm to both citizens and corporations. |
| Equinix-AI-RFI-2025.md | David Young | Industry-Large (Data Center Operator) | Equinix aims to position itself as a critical infrastructure partner for American leadership in AI by addressing challenges related to power availability, supply chain resilience, and workforce development, ultimately securing its role in the AI ecosystem and promoting a favorable business environment. | - None Stated | - None Stated |
| Equifax-AI-RFI-2025.md | Harald Schneider (representing Equifax) | Industry-Large | Equifax is focused on responsible AI innovation within the credit reporting industry, emphasizing transparency, explainability, and fairness in AI models, particularly those used for credit scoring, and advocates for industry-specific AI frameworks and regulatory considerations. | - [§1: Introduction/Header] None stated for this section.<br>- [§2: Executive Summary/Positioning] Lack of consideration for industry-specific needs and types of AI in broader AI standards development.<br>- [§3: Overall AI Framework Principles] Need for a balanced AI framework guided by transparency, explainability, and fairness.<br>- [§4: Advance Responsible Technical Standards - Explainability] Regulatory requirements necessitate fully explainable credit scoring models; importance of explaining models to all stakeholders (modelers, risk managers, lenders, regulators, and consumers).<br>- [§5: Regulation B Compliance & Credit Scoring Systems] Ensuring AI credit scoring systems meet the requirements for "an empirically derived, demonstrably and statistically sound, credit scoring system," including adverse action codes, data derivation, and statistical principles.<br>- [§6: Comparison to Generalized AI Models] Generalized AI models may lack specific problem-solving abilities and can generate inaccurate information ("hallucinations") due to a lack of context and constraints; risks of unintended consequences for consumers.<br>- [§7: Fairness and Bias Mitigation] Potential for bias in AI systems and the need to ensure fairness and avoid unintended consequences for consumers.<br>- [§8: Concluding Remarks/Call to Action] None stated for this section. | - [§1: Introduction/Header] None stated for this section.<br>- [§2: Executive Summary/Positioning] Incorporate industry leaders and stakeholders in AI standards development.<br>- [§3: Overall AI Framework Principles] Develop AI frameworks built on guiding principles of transparency, explainability, and fairness. Consider the relevance of standards to specific industries, use cases, and types of AI.<br>- [§4: Advance Responsible Technical Standards - Explainability] Utilize the systematic approach of Regulation B as a paradigm for explainability in other AI models.<br>- [§5: Regulation B Compliance & Credit Scoring Systems] Maintain the existing requirements for AI credit scoring systems under Regulation B, including the ability to return adverse action codes and adherence to statistical principles.<br>- [§6: Comparison to Generalized AI Models] Consider a systematic framework similar to Regulation B for generative AI models to build confidence and prevent hallucinations or unintended consequences.<br>- [§7: Fairness and Bias Mitigation] Include protections to support fairness in any AI regulatory framework, such as the “fairness through unawareness” approach. Implement rigorous testing and evaluation to ensure fairness.<br>- [§8: Concluding Remarks/Call to Action] Engage with regulators and share expertise in AI credit scoring systems and the deployment of AI in the financial services sector. |
| Enzai-AI-RFI-2025.md | Enzai Technologies US, Inc. ("Enzai") | Industry-Large | Enzai provides AI governance, risk, and compliance software solutions and is interested in enabling the secure and sustainable development and deployment of AI technologies, particularly through clear governance frameworks, addressing regulatory obligations, and fostering trust in AI systems. | - [§1: Introduction/Header] None stated for this section.<br>- [§2: Background on Enzai] Lack of trust and uncertainty hindering AI adoption; navigating emerging regulatory obligations.<br>- [§3: Recommendations Overview] AI governance is a key enabler of AI adoption, and its absence creates barriers.<br>- [§4: Recommendation 1: Articulate a Positive Vision for AI] Concerns about AI's impact on the global jobs market and employment opportunities in the US.<br>- [§5: Recommendation 2: Provide American Leadership in AI Research] Unique challenges posed by AI systems that transcend industries and borders.<br>- [§6: Recommendation 3: Clarify Rules for Federal Government AI Use] Lack of standardization in responsible AI principles and characteristics across agencies; friction for vendors supplying AI systems to the government; lengthy and costly FedRAMP compliance process, particularly for smaller organizations.<br>- [§7: Recommendation 4: Access to Data] Structural challenges to data access limiting AI development, especially for smaller companies and researchers; need to balance responsible data access with privacy and intellectual property protection.<br>- [§8: Closing/Disclaimer] None stated for this section. | - [§1: Introduction/Header] None stated for this section.<br>- [§2: Background on Enzai] None stated for this section.<br>- [§3: Recommendations Overview] Develop a strong AI governance framework to enable trust and remove uncertainty.<br>- [§4: Recommendation 1: Articulate a Positive Vision for AI] Articulate a positive vision for how AI can help people, specifically improving healthcare, boosting the competitiveness of small businesses, and enhancing educational outcomes.<br>- [§5: Recommendation 2: Provide American Leadership in AI Research] Support specific AI-related research projects, both thematic and sectoral, with international collaboration. Model the approach after the Human Genome Project.<br>- [Section 6: Recommendation 3: Clarify Rules for Federal Government AI Use]<br>- Standardize responsible AI principles and characteristics across agencies.<br>- Include sample contract terms for providers of safety-impacting and rights-impacting AI systems.<br>- List agency AI marketplaces to facilitate faster procurement.<br>- [Section 7: Recommendation 4: Access to Data]<br>- Create public-private data partnerships.<br>- Establish federal data repositories with clear governance.<br>- Fund research and development of privacy-preserving data access techniques.<br>- [§8: Closing/Disclaimer] None stated for this section. |
| Entertainment-Software-Association-AI-RFI-2025.md | Entertainment Software Association (ESA) | Industry-Association | The Entertainment Software Association (ESA) advocates for policies that promote innovation and creativity in the video game industry, with a focus on a light regulatory touch and a risk-based approach to AI regulation, while protecting intellectual property and fostering workforce development. | - None Stated | - None Stated |
| Entertainment-AI-RFI-2025.md | America's entertainment industry | Industry-Large | The submitters represent a broad coalition of creative professionals and companies within the U.S. entertainment industry and are focused on protecting copyright protections to maintain economic strength and cultural influence against perceived threats from AI companies seeking to exploit copyrighted material without appropriate licensing. | - [§1: Header & Introduction] AI companies are seeking to undermine the entertainment industry’s economic and cultural strength by weakening copyright protections to train AI models. They cite a support of over 2.3M jobs and $229Bn in wages annually.<br>- [§2: Core Argument - Copyright Concerns] Google (valued at $2Tn) and OpenAI (valued at over $157Bn) are requesting a "special government exemption" to freely exploit America's creative industries despite substantial revenues. They believe this is unfair and unnecessary.<br>- [§3: Reinforcement of Copyright's Importance] No main concerns stated for this section.<br>- [§4: Endorsements] No main concerns stated for this section. | - [§1: Header & Introduction] No main recommendations stated for this section.<br>- [§2: Core Argument - Copyright Concerns] AI companies should negotiate appropriate licenses with copyright holders instead of seeking exemptions from existing U.S. copyright law.<br>- [§3: Reinforcement of Copyright's Importance] Uphold existing copyright frameworks to maintain the strength of America’s creative industries and cultural influence abroad.<br>- [§4: Endorsements] No main recommendations stated for this section. |
| EnergyforAI-AI-RFI-2025.md | Luminary Strategies | Advocacy/Think Tank (representing interests of multiple stakeholders in the AI and energy sectors) | Luminary Strategies advocates for policies that enable the development and scalability of AI infrastructure by addressing energy supply, grid connection, and flexible power consumption challenges, with a focus on innovation discovery and national-level support for state-driven initiatives. | - [Section 1: Introduction/Cover Letter]:** Regulatory inertia hinders AI infrastructure expansion, increases costs, and weakens national power grid resilience and security.<br>- [Section 2: Statement of Interest]:** None stated for this section.<br>- [Section 3: Energy for AI - Innovation Discovery Workstream]:** Lack of a coordinated national approach to emerging energy solutions at the state and utility level.<br>- [Section 4: High-Priority Considerations for a National AI Energy Strategy]:** Regional fragmentation of innovation, lack of energy flexibility for scaling AI infrastructure, and barriers to self-generation and energy choice.<br>- [Section 5: Constraints of Utility Monopoly Control]:** Monopoly control by some utilities over electricity procurement prevents cost-effective power sourcing, blocks PPAs, and slows the deployment of self-generation and flexibility technologies.<br>- [Section 6: Client Focus Areas and Current Challenges]:** Uncertainty in regulatory processes hinders the pursuit of efficiencies like privately owned substations, SPS/RAS schemes, and onsite renewable energy.<br>- [Section 7: National Policy Framework Recommendations]:** Lack of transparency regarding regulatory barriers, restrictive contracts, and opaque utility policies.<br>- [Section 8: Federal Agency Collaboration]:** None stated for this section.<br>- [Section 9: A National Dialogue on AI Energy Needs]:** Utilities face rate pressures, regulatory scrutiny, and investor concerns, requiring a national forum for dialogue with data consumers.<br>- [Section 10: Energy for AI - Key Themes]:** Innovation is highly regional and non-standardized, economies of scale are essential for scaling AI, and Americans' right to choose their energy provider is constrained by regulations.<br>- [Section 11: Concluding Remarks]:** None stated for this section. | - [Section 1: Introduction/Cover Letter]:** Support work streams dedicated to identifying and providing national support to innovative efforts underway in various states.<br>- [Section 2: Statement of Interest]:** None stated for this section.<br>- [Section 3: Energy for AI - Innovation Discovery Workstream]:** Evaluate and support state-driven energy initiatives focusing on hardware development, software-driven load flexibility, regulatory mechanisms, and grid optimization.<br>- [Section 4: High-Priority Considerations for a National AI Energy Strategy]:** Standardize best practices to unlock scalable, cost-effective energy solutions, promote demand flexibility, and address barriers to self-generation.<br>- [Section 5: Constraints of Utility Monopoly Control]:** None stated for this section.<br>- [Section 6: Client Focus Areas and Current Challenges]:** None stated for this section.<br>- [Section 7: National Policy Framework Recommendations]:** Increase transparency about regulatory barriers, support alternative power procurement structures (private use networks, microgrids), encourage battery storage and demand flexibility, and create a standardized integration process for self-supplied generation.<br>- [Section 8: Federal Agency Collaboration]:** FERC and NERC should collaborate on reliability standards related to SPS/RAS schemes.<br>- [Section 9: A National Dialogue on AI Energy Needs]:** Convene White House-led summits or a national security-led task force to foster collaboration among AI firms, energy innovators, and grid operators.<br>- [Section 10: Energy for AI - Key Themes]:** None stated for this section.<br>- [Section 11: Concluding Remarks]:** None stated for this section. |
| Emory-University-AI-RFI-2025.md | Emory University | Academia | The submitter, represented by Professor Matthew Sag, is focused on the intersection of copyright law and artificial intelligence, specifically advocating for a framework that allows for the continued development of AI technologies while recognizing the rights of copyright holders, and analyzing the global legal landscape surrounding AI training. | - [§1: Introduction] Concerns center around the potential for legal challenges to AI training based on copyright infringement, potentially halting the progress of AI development. The copy-reliant nature of AI is a central point of concern.<br>- [§2: Nonexpressive Use of Copyrighted Works] Concerns revolve around the conceptual difficulty of applying traditional copyright principles to non-expressive uses of copyrighted works, such as reverse engineering and plagiarism detection.<br>- [§2.A: Nonexpressive Use and Generative AI] Concerns focus on the reproducibility of works, the conversion of works into tokens, and the broader legal implications of AI training. Risks related to biases, misinformation, job displacement, and alignment with human values are also raised, but the core concern is the potential for legal restrictions based on copyright. | - [§1: Introduction] Explicitly recommends the U.S. government act legislatively if courts rule against fair use for AI training. Advocates for affirming the importance of broad copyright exceptions.<br>- [§2: Nonexpressive Use of Copyrighted Works] Not stated.<br>- [§2.A: Nonexpressive Use and Generative AI] Not stated. |
| Emmanuel-Edwards-AI-RFI-2025.md | Emmanuel Edwards | Individual | The submitter believes current open-source AI development is a misallocation of resources, infringes on intellectual property rights, and discourages creativity; they advocate for focusing AI efforts on specific, targeted applications. | - Current “AI” sources (like OpenAI) are considered a waste of time, resources, and technology.<br>- Investment in open-source AI discourages creative freedom.<br>- Open-source AI development leads to copyright infringement, trademark violations, and other intellectual property issues.<br>- Publicly available AI is deemed pointless. | - Handicap all open-source AI companies.<br>- Redirect AI efforts and investment into specific fields of AI usage. |
| Emma-Beck-AI-RFI-2025.md | Emma Beck | Individual | The submitter is primarily concerned with the protection of American artists' copyright and the preservation of the American art economy, opposing the use of AI that scrapes existing artwork without permission. | - AI scraping and use of existing copyrighted material without permission.<br>- The perceived devaluation and theft from American artists.<br>- The belief that AI does not create original content but merely transforms existing work.<br>- OpenAI's challenge to copyright law as a threat to the foundation of the American art economy.<br>- The perception that AI is a waste of resources (water, money, time).<br>- Lack of public demand for AI-generated content. | - Uphold existing copyright laws.<br>- Do not push forward any legislation that changes copyright law.<br>- Keep American artistic ideas and creations safe from AI. |
| Emily-Simmons-AI-RFI-2025.md | Emily Simmons | Individual | The submitter's mission or interest is not explicitly stated, but appears to be related to providing input for the NITRD AI Action Plan, potentially as a government employee or contractor. The sender is providing a document "for NITRD official use only". | - None Stated | - None Stated |
| Emily-Sharp-AI-RFI-2025.md | Emily Sharp | Individual | The submitter is interested in responsible AI development and deployment, emphasizing ethical considerations, workforce impacts, and international collaboration, with a focus on proactive government policy. | - [§1: Introduction/Header] None stated for this section.<br>- [§2: Resource Efficiency and Environmental Impact] Potential negative impact on local communities from data center cooling; toxic impacts from methods used.<br>- [§3: Data Privacy] Unauthorized sale or access of individual data input into AI systems by companies or government entities.<br>- [§4: Inclusive Decision-Making] Lack of diverse expert perspectives in shaping AI policies and development.<br>- [§5: Ethical Development] Exploitation of vulnerable populations and unfairness in AI applications.<br>- [§6: Workforce Transition and Education] Human job displacement due to AI advancements.<br>- [§7: International Collaboration] Lack of regulatory alignment and shared risk-based approach to AI regulation internationally.<br>- [§8: Safety and Security] Potential misuse or compromise of AI systems.<br>- [§9: Policy Retention] None stated for this section. | - [§1: Introduction/Header] None stated for this section.<br>- [§2: Resource Efficiency and Environmental Impact] Prioritize resource-efficient systems (water conservation, heat management); encourage research into energy-efficient algorithms and hardware; implement innovative cooling solutions.<br>- [§3: Data Privacy] Establish strict regulations to prevent unauthorized data access and sale; allow controlled data use for training while protecting privacy.<br>- [§4: Inclusive Decision-Making] Ensure diverse expert involvement in policy and development (scientists, creatives, field professionals).<br>- [§5: Ethical Development] Develop comprehensive ethical guidelines to prevent exploitation and ensure fairness; promote transparent and explainable AI systems.<br>- [§6: Workforce Transition and Education] Implement workforce retraining and education programs; proactively support affected workers and involve these sectors in program design.<br>- [§7: International Collaboration] Pursue regulatory alignment with international partners; establish joint R&D projects; engage in global governance framework development (intergovernmental panel).<br>- [§8: Safety and Security] Develop robust safety and security protocols; enhance international collaboration on safety and security measures.<br>- [§9: Policy Retention] Maintain and build upon key elements of Biden-Harris AI Executive Order 14110 (safety/security standards, privacy protections, equity/civil rights, consumer protection, worker support, responsible government use). |
| Emily-Pittman-AI-RFI-2025.md | Emily Pittman | Individual | The submitter is an artist active in the digital art “gig” since 2019/2020 and is primarily concerned about the negative impact of AI on artists' copyright and livelihoods, as well as the environmental consequences of AI development. | - Violation of artists’ copyright without consent.<br>- AI is not considered fair use due to its active copying from artists’ work.<br>- High energy consumption of AI systems (example given: ChatGPT usage compared to water waste or lightbulb power).<br>- Increasing greenhouse gas emissions associated with AI hype (example given: Google's 48% increase in the last 5 years).<br>- Public being "scammed" by AI hype and a product that doesn’t actually exist. | - Do not support AI.<br>- Focus on supporting actual artists instead of AI development. |
| Elmore-AI-RFI-2025.md | Holly Elmore | Individual | Dr. Elmore expresses a strong interest in ensuring the safe and beneficial development of frontier AI, with a focus on US leadership and preventing catastrophic risks through proactive measures and international cooperation. She believes the US must take immediate action to steer AI development and maintain a lead in the field. | - [§1: Introduction and Statement of Urgency] The primary concern is ensuring the “loyalty and reliability” of frontier AI systems and the potential for “out-of-control superhuman AI.”<br>- [§2: Personal Background and Credibility] None stated for this section.<br>- [§3: Potential Risks of Uncontrolled AI Development] Concerns include cybersecurity vulnerabilities, crippling infrastructure, inducing armed conflict with adversaries, and the creation of synthetic superpathogens. The inherent difficulty in anticipating all potential risks is also a major concern.<br>- [§4: Need for US Leadership and an 'AI Deal'] The concern is that without strong US leadership and a coordinated international effort, the US will lose control of the technology and the ability to steer it in a beneficial direction. The risk of other nations making “fatal mistakes” is highlighted.<br>- [§5: Conclusion] None stated for this section. | - [§1: Introduction and Statement of Urgency] The US must “take the reins” and lead the effort to control and steer frontier AI development.<br>- [§2: Personal Background and Credibility] None stated for this section.<br>- [§3: Potential Risks of Uncontrolled AI Development] Mandating better biosecurity practices is suggested as a partial mitigation strategy, though acknowledged as insufficient.<br>- [§4: Need for US Leadership and an 'AI Deal'] Propose an “AI Deal” to limit experimentation by all nations, potentially through limits on training compute, in order to allow the US to maintain its lead and ensure beneficial development. The need for a “great AI Plan” that ensures beneficial AI building is proposed.<br>- [§5: Conclusion] The need for a “comprehensive AI Plan” is reiterated. |
| ElmerConcepcion-AI-RFI-2025.md | Elmer Concepcion | Individual | Elmer Concepcion is a creative artist in the United States with a vested interest in protecting copyright and maintaining the dominance of American cultural production. They express concern that loosening restrictions on AI development will harm creators. | - Removing copyright protections and restrictions on AI development is likely to do more harm than good.<br>- The benefits of AI, particularly Generative AI, are minor at best.<br>- Allowing companies to circumvent laws protecting creators is short-sighted.<br>- Current Generative AI output is of low quality ("woefully cringe").<br>- Empowering harmful behavior through unrestricted AI development. | - None Stated |
| Ella-Bounos-RFI-2025.md | Ella Bounos | Individual | Ella Bounos is a graduating high school senior interested in the impact of generative AI on art and concerned about the potential for copyright infringement and the displacement of artists due to AI technologies. | - AI is "stealing other people's work" – specifically art styles.<br>- Copyright, patent, and trademark infringement applicability to AI creations is unclear.<br>- Artists are unaware their art is being used to train AI models.<br>- Generative AI can potentially put artists out of business.<br>- The value and definition of “real art” in relation to AI-generated art is questioned. | - Implement a copyright infringement system for AI.<br>- Establish a more elevated monitoring system for AI. |
| ElianaSmith-AI-RFI-2025.md | Eliana Smith | Individual | Eliana Smith expresses a strong opposition to the advancement and support of AI, believing it will be detrimental to American workers and society, and expresses distrust in private companies controlling its development. | - AI delivers worse results than human work.<br>- Fixing the problems created by AI requires as much effort as the original work it aims to replace.<br>- AI leads to workers doing more work for less pay.<br>- Recent firings demonstrate the value of well-trained knowledgeable people, implying AI is a poor substitute.<br>- AI is not intelligent but a tool with untested or proven unreliable performance (described as a "black box").<br>- AI destroys jobs for hardworking Americans.<br>- AI facilitates scams and fraud.<br>- AI represents a potential economic bubble.<br>- AI is controlled by private companies lacking public accountability, creating risks of abuse and sabotage.<br>- Lack of trust in private companies to uphold American rights and values. | - None Stated |
